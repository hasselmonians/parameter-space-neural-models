<?xml version="1.0" encoding="UTF-8"?>
<xml><records><record><database name="MyLibrary">MyLibrary</database><source-app name="Zotero">Zotero</source-app><ref-type name="Journal Article">17</ref-type><contributors><authors><author>Loeb, Gerald E.</author></authors></contributors><titles><title>Optimal isn't good enough</title><secondary-title>Biological Cybernetics</secondary-title></titles><periodical><full-title>Biological Cybernetics</full-title><abbr-1>Biol Cybern</abbr-1></periodical><pages>757-765</pages><volume>106</volume><number>11-12</number><issue>11-12</issue><keywords><keyword>Animals</keyword><keyword>Humans</keyword><keyword>Brain</keyword><keyword>Algorithms</keyword><keyword>Feedback, Sensory</keyword><keyword>Learning</keyword><keyword>Motivation</keyword><keyword>Sensation</keyword></keywords><dates><year>2012</year><pub-dates><date>Dec 2012</date></pub-dates></dates><isbn>1432-0770</isbn><electronic-resource-num>10.1007/s00422-012-0514-6</electronic-resource-num><abstract>The notion that biological systems come to embody optimal solutions seems consistent with the competitive drive of evolution. It has been used to interpret many examples of sensorimotor behavior. It is attractive from the viewpoint of control engineers because it solves the redundancy problem by identifying the one optimal motor strategy out of many similarly acceptable possibilities. This perspective examines whether there is sufficient basis to apply the formal engineering tools of optimal control to a reductionist understanding of biological systems. For an experimental biologist, this translates into whether the theory of optimal control generates nontrivial and testable hypotheses that accurately predict novel phenomena, ideally at deeper levels of structure than the observable behavior. The methodology of optimal control is applicable when there is (i) a single, known cost function to be optimized, (ii) an invertible model of the plant, and (iii) simple noise interfering with optimal performance. None of these is likely to be true for biological organisms. Furthermore, their motivation is usually good-enough rather than globally optimal behavior. Even then, the performance of a biological organism is often much farther from optimal than the physical limits of its hardware because the brain is continuously testing the acceptable limits of performance as well as just performing the task. This perspective considers an alternative strategy called "good-enough" control, in which the organism uses trial-and-error learning to acquire a repertoire of sensorimotor behaviors that are known to be useful, but not necessarily optimal. This leads to a diversity of solutions that tends to confer robustness on the individual organism and its evolution. It is also more consistent with the capabilities of higher sensorimotor structures, such as cerebral cortex, which seems to be designed to classify and recall complex sets of information, thereby allowing the organism to learn from experience, rather than to compute new strategies online. Optimal control has been a useful metaphor for understanding some superficial aspects of motor psychophysics. Reductionists who want to understand the underlying neural mechanisms need to move on.</abstract><remote-database-name>PubMed</remote-database-name><language>eng</language><urls><text-urls><url>http://www.ncbi.nlm.nih.gov/pubmed/22895830</url></text-urls></urls></record><record><database name="MyLibrary">MyLibrary</database><source-app name="Zotero">Zotero</source-app><ref-type name="Journal Article">17</ref-type><contributors><authors><author>Golowasch, Jorge</author><author>Goldman, Mark S.</author><author>Abbott, L. F.</author><author>Marder, Eve</author></authors></contributors><titles><title>Failure of averaging in the construction of a conductance-based neuron model</title><secondary-title>Journal of Neurophysiology</secondary-title></titles><periodical><full-title>Journal of Neurophysiology</full-title><abbr-1>J. Neurophysiol.</abbr-1></periodical><pages>1129-1131</pages><volume>87</volume><number>2</number><issue>2</issue><keywords><keyword>Animals</keyword><keyword>Ganglia, Invertebrate</keyword><keyword>Models, Neurological</keyword><keyword>Neurons</keyword><keyword>Action Potentials</keyword><keyword>Brachyura</keyword><keyword>Potassium</keyword></keywords><dates><year>2002</year><pub-dates><date>Feb 2002</date></pub-dates></dates><isbn>0022-3077</isbn><abstract>Parameters for models of biological systems are often obtained by averaging over experimental results from a number of different preparations. To explore the validity of this procedure, we studied the behavior of a conductance-based model neuron with five voltage-dependent conductances. We randomly varied the maximal conductance of each of the active currents in the model and identified sets of maximal conductances that generate bursting neurons that fire a single action potential at the peak of a slow membrane potential depolarization. A model constructed using the means of the maximal conductances of this population is not itself a one-spike burster, but rather fires three action potentials per burst. Averaging fails because the maximal conductances of the population of one-spike bursters lie in a highly concave region of parameter space that does not contain its mean. This demonstrates that averages over multiple samples can fail to characterize a system whose behavior depends on interactions involving a number of highly variable components.</abstract><remote-database-name>PubMed</remote-database-name><language>eng</language><urls><text-urls><url>http://www.ncbi.nlm.nih.gov/pubmed/11826077</url></text-urls></urls></record><record><database name="MyLibrary">MyLibrary</database><source-app name="Zotero">Zotero</source-app><ref-type name="Journal Article">17</ref-type><contributors><authors><author>Voets, Thomas</author><author>Droogmans, Guy</author><author>Wissenbach, Ulrich</author><author>Janssens, Annelies</author><author>Flockerzi, Veit</author><author>Nilius, Bernd</author></authors></contributors><titles><title>The principle of temperature-dependent gating in cold- and heat-sensitive TRP channels</title><secondary-title>Nature</secondary-title></titles><periodical><full-title>Nature</full-title><abbr-1>Nature</abbr-1></periodical><pages>748-754</pages><volume>430</volume><number>7001</number><issue>7001</issue><dates><year>2004</year><pub-dates><date>August 12, 2004</date></pub-dates></dates><isbn>0028-0836</isbn><electronic-resource-num>10.1038/nature02732</electronic-resource-num><abstract>The mammalian sensory system is capable of discriminating thermal stimuli ranging from noxious cold to noxious heat. Principal temperature sensors belong to the TRP cation channel family, but the mechanisms underlying the marked temperature sensitivity of opening and closing (‘gating’) of these channels are unknown. Here we show that temperature sensing is tightly linked to voltage-dependent gating in the cold-sensitive channel TRPM8 and the heat-sensitive channel TRPV1. Both channels are activated upon depolarization, and changes in temperature result in graded shifts of their voltage-dependent activation curves. The chemical agonists menthol (TRPM8) and capsaicin (TRPV1) function as gating modifiers, shifting activation curves towards physiological membrane potentials. Kinetic analysis of gating at different temperatures indicates that temperature sensitivity in TRPM8 and TRPV1 arises from a tenfold difference in the activation energies associated with voltage-dependent opening and closing. Our results suggest a simple unifying principle that explains both cold and heat sensitivity in TRP channels.</abstract><remote-database-name>www.nature.com</remote-database-name><language>en</language><urls><web-urls><url>https://www.nature.com/nature/journal/v430/n7001/full/nature02732.html</url></web-urls><text-urls><url>/home/alec/Zotero/storage/ZT2GCG64/nature02732.html</url></text-urls></urls><access-date>2017-04-26 00:16:43</access-date></record><record><database name="MyLibrary">MyLibrary</database><source-app name="Zotero">Zotero</source-app><ref-type name="Journal Article">17</ref-type><contributors><authors><author>Roffman, Rebecca C.</author><author>Norris, Brian J.</author><author>Calabrese, Ronald L.</author></authors></contributors><titles><title>Animal-to-animal variability of connection strength in the leech heartbeat central pattern generator</title><secondary-title>Journal of Neurophysiology</secondary-title></titles><periodical><full-title>Journal of Neurophysiology</full-title></periodical><pages>1681-1693</pages><volume>107</volume><number>6</number><issue>6</issue><dates><year>2012</year><pub-dates><date>2012/03/15</date></pub-dates></dates><isbn>0022-3077, 1522-1598</isbn><electronic-resource-num>10.1152/jn.00903.2011</electronic-resource-num><abstract>The heartbeat central pattern generator (CPG) in medicinal leeches controls blood flow within a closed circulatory by programming the constrictions of two parallel heart tubes. This circuit reliably produces a stereotyped fictive pattern of activity and has been extensively characterized. Here we determined, as quantitatively as possible, the strength of each inhibitory synapse and electrical junction within the core circuit of the heartbeat CPG. We also examined the animal-to-animal variability in strengths of these connections and, for some, determined the correlations between connections to the same postsynaptic target. The core CPG is composed of seven bilateral pairs of heart interneurons connected via both inhibitory chemical synapses and electrical junctions. Fifteen different connections within the core CPG were measured for strength using extracellular presynaptic recordings and postsynaptic voltage-clamp recordings across a minimum of seven individuals each, and the animal-to-animal variability was characterized. Connection strengths within the core network varied three to more than sevenfold among individuals (depending on the specific connection). The balance between two inputs onto various postsynaptic targets was explored by within-individual comparisons and correlation across individuals. Of the seven comparisons made within the core CPG, three showed a clear correlation of connection strengths, while the other four did not. We conclude that the leech heartbeat CPG can withstand wide variability in connection strengths and still produce stereotyped output. The network appears to preserve the relative strengths of some pairs of inputs, despite the animal-to-animal variability.</abstract><remote-database-name>jn.physiology.org</remote-database-name><language>en</language><urls><web-urls><url>http://jn.physiology.org/content/107/6/1681</url></web-urls><pdf-urls><url>/home/alec/Zotero/storage/IXJC23FR/Roffman et al. - 2012 - Animal-to-animal variability of connection strengt.pdf</url></pdf-urls><text-urls><url>http://www.ncbi.nlm.nih.gov/pubmed/22190622</url><url>/home/alec/Zotero/storage/WVKRKAKW/1681.html</url></text-urls></urls><access-date>2017-04-20 01:23:27</access-date></record><record><database name="MyLibrary">MyLibrary</database><source-app name="Zotero">Zotero</source-app><ref-type name="Journal Article">17</ref-type><contributors><authors><author>Arbas, Edmund A.</author><author>Calabrese, Ronald L.</author></authors></contributors><titles><title>Rate modification in the heartbeat central pattern generator of the medicinal leech</title><secondary-title>Journal of Comparative Physiology A</secondary-title></titles><periodical><full-title>Journal of Comparative Physiology A</full-title><abbr-1>J. Comp. Physiol.</abbr-1></periodical><pages>783-794</pages><volume>155</volume><number>6</number><issue>6</issue><dates><year>1984</year><pub-dates><date>1984/11/01</date></pub-dates></dates><isbn>0340-7594, 1432-1351</isbn><electronic-resource-num>10.1007/BF00611595</electronic-resource-num><abstract>Summary1.Intracellular recordings from neurons in partially dissected leeches and isolated nerve cords were used to study the effects of temperature, sensory stimulation and locomotory activity on the output of the central pattern generator that drives heartbeat. 2.The rate of motor burst production by the heartbeat oscillator changes on gradual heating or cooling with a Q10 averaging around 2.4. Abrupt cooling of isolated nerve cords can induce additional changes in heart rate that are associated with ‘paradoxical’ firing of motor neurons. 3.Heart rate was accelerated when mechanical stimuli were applied to the body wall of partially dissected preparations or when restrained preparations spontaneously initiated movements. For instance, marked acceleration of heart rate occurred when preparations exhibited body movements and motor neuron activity characteristic of swimming in intact animals. Activation of the motor program for swimming in isolated nerve cords led to acceleration of heartbeat oscillator cycling. Accelerations of heart rate associated with swimming are therefore mediated at least partially through central interactions. 4.Individual identified neurons were tested for their influence on the cycle rate of the heartbeat oscillator in isolated nerve cords. Activity of individual mechanosensory neurons was found to accelerate heart rate. Touch mechanosensory neurons were also found to influence the heartbeat oscillator through a rapidly adapting inhibitory pathway. Activity of individual motorneurons generally did not affect heart rate. Some interneurons and neuroeffector cells known to cause motor activation (e.g. swim initiating interneurons, serotonergic Retzius cells) can also produce acceleration of heart rate.</abstract><remote-database-name>link.springer.com</remote-database-name><language>en</language><urls><web-urls><url>https://link.springer.com/article/10.1007/BF00611595</url></web-urls><text-urls><url>/home/alec/Zotero/storage/NAF5N6ID/BF00611595.html</url></text-urls></urls><access-date>2017-04-20 01:22:26</access-date></record><record><database name="MyLibrary">MyLibrary</database><source-app name="Zotero">Zotero</source-app><ref-type name="Journal Article">17</ref-type><contributors><authors><author>Doloc-Mihu, Anca</author><author>Calabrese, Ronald L.</author></authors></contributors><titles><title>Identifying Crucial Parameter Correlations Maintaining Bursting Activity</title><secondary-title>PLoS Computational Biology</secondary-title></titles><periodical><full-title>PLoS Computational Biology</full-title><abbr-1>PLoS Comput Biol</abbr-1></periodical><volume>10</volume><number>6</number><issue>6</issue><dates><year>2014</year><pub-dates><date>2014-6-19</date></pub-dates></dates><isbn>1553-734X</isbn><electronic-resource-num>10.1371/journal.pcbi.1003678</electronic-resource-num><abstract>Recent experimental and computational studies suggest that linearly correlated sets of parameters (intrinsic and synaptic properties of neurons) allow central pattern-generating networks to produce and maintain their rhythmic activity regardless of changing internal and external conditions. To determine the role of correlated conductances in the robust maintenance of functional bursting activity, we used our existing database of half-center oscillator (HCO) model instances of the leech heartbeat CPG. From the database, we identified functional activity groups of burster (isolated neuron) and half-center oscillator model instances and realistic subgroups of each that showed burst characteristics (principally period and spike frequency) similar to the animal. To find linear correlations among the conductance parameters maintaining functional leech bursting activity, we applied Principal Component Analysis (PCA) to each of these four groups. PCA identified a set of three maximal conductances (leak current, &#xD;Leak; a persistent K current, &#xD;K2; and of a persistent Na+ current, &#xD;P) that correlate linearly for the two groups of burster instances but not for the HCO groups. Visualizations of HCO instances in a reduced space suggested that there might be non-linear relationships between these parameters for these instances. Experimental studies have shown that period is a key attribute influenced by modulatory inputs and temperature variations in heart interneurons. Thus, we explored the sensitivity of period to changes in maximal conductances of &#xD;Leak, &#xD;K2, and &#xD;P, and we found that for our realistic bursters the effect of these parameters on period could not be assessed because when varied individually bursting activity was not maintained., Central pattern-generating networks (CPGs) must be remarkably robust, maintaining functional rhythmic activity despite fluctuations in internal and external conditions. Recent experimental evidence suggests that this robustness is achieved by the coordinated regulation of many membrane and synaptic current parameters. Experimental and computational studies showed that linearly correlated sets of such parameters allow CPG neurons to produce and maintain their rhythmic activity. However, the mechanisms that allow multiple parameters to interact, thereby producing and maintaining rhythmic single cell and network activity, are not clear. Here, we use a half-center oscillator (HCO) model that replicates the electrical activity (rhythmic alternating bursting of mutually inhibitory interneurons) of the leech heartbeat CPG to investigate potential relationships between parameters that maintain functional bursting activity in the HCOs and the isolated component neurons (bursters). We found a linearly correlated set of three maximal conductances that maintains functional bursting activity similar to the animal in burster model instances, therefore increasing robustness of bursting activity. In addition, we found that bursting activity was very sensitive to individual variation of these parameters; only correlated changes could maintain the activity type.</abstract><remote-database-name>PubMed Central</remote-database-name><urls><web-urls><url>http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4063674/</url></web-urls><pdf-urls><url>/home/alec/Zotero/storage/249EC44P/Doloc-Mihu and Calabrese - 2014 - Identifying Crucial Parameter Correlations Maintai.pdf</url></pdf-urls><text-urls><url>http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4063674/</url></text-urls></urls><access-date>2017-04-20 01:10:16</access-date></record><record><database name="MyLibrary">MyLibrary</database><source-app name="Zotero">Zotero</source-app><ref-type name="Journal Article">17</ref-type><contributors><authors><author>Caplan, Jonathan S.</author><author>Williams, Alex H.</author><author>Marder, Eve</author></authors></contributors><titles><title>Many Parameter Sets in a Multicompartment Model Oscillator Are Robust to Temperature Perturbations</title><secondary-title>Journal of Neuroscience</secondary-title></titles><periodical><full-title>Journal of Neuroscience</full-title><abbr-1>J. Neurosci.</abbr-1></periodical><pages>4963-4975</pages><volume>34</volume><number>14</number><issue>14</issue><keywords><keyword>stomatogastric ganglion</keyword><keyword>Stomatogastric ganglion</keyword><keyword>conductance-based models</keyword><keyword>electrical coupling</keyword><keyword>Q10</keyword><keyword>temperature</keyword><keyword>voltage-dependent currents</keyword></keywords><dates><year>2014</year><pub-dates><date>2014/04/02</date></pub-dates></dates><isbn>0270-6474, 1529-2401</isbn><electronic-resource-num>10.1523/JNEUROSCI.0280-14.2014</electronic-resource-num><abstract>Neurons in cold-blooded animals remarkably maintain their function over a wide range of temperatures, even though the rates of many cellular processes increase twofold, threefold, or many-fold for each 10°C increase in temperature. Moreover, the kinetics of ion channels, maximal conductances, and Ca2+ buffering each have independent temperature sensitivities, suggesting that the balance of biological parameters can be disturbed by even modest temperature changes. In stomatogastric ganglia of the crab Cancer borealis, the duty cycle of the bursting pacemaker kernel is highly robust between 7 and 23°C (Rinberg et al., 2013). We examined how this might be achieved in a detailed conductance-based model in which exponential temperature sensitivities were given by Q10 parameters. We assessed the temperature robustness of this model across 125,000 random sets of Q10 parameters. To examine how robustness might be achieved across a variable population of animals, we repeated this analysis across six sets of maximal conductance parameters that produced similar activity at 11°C. Many permissible combinations of maximal conductance and Q10 parameters were found over broad regions of parameter space and relatively few correlations among Q10s were observed across successful parameter sets. A significant portion of Q10 sets worked for at least 3 of the 6 maximal conductance sets (∼11.1%). Nonetheless, no Q10 set produced robust function across all six maximal conductance sets, suggesting that maximal conductance parameters critically contribute to temperature robustness. Overall, these results provide insight into principles of temperature robustness in neuronal oscillators.</abstract><remote-database-name>www.jneurosci.org</remote-database-name><language>en</language><urls><web-urls><url>http://www.jneurosci.org/content/34/14/4963</url></web-urls><pdf-urls><url>/home/alec/Zotero/storage/KTV8NXH4/Caplan et al. - 2014 - Many Parameter Sets in a Multicompartment Model Os.pdf</url><url>/home/alec/Zotero/storage/W3QKR4QT/Caplan et al. - 2014 - Many Parameter Sets in a Multicompartment Model Os.pdf</url><url>/home/alec/Zotero/storage/TJTCPPW8/Caplan et al. - 2014 - Many Parameter Sets in a Multicompartment Model Os.pdf</url><url>/home/alec/Zotero/storage/L55LMCSQ/Caplan et al. - 2014 - Many Parameter Sets in a Multicompartment Model Os.pdf</url><url>/home/alec/Zotero/storage/2YR2IVGG/Caplan et al. - 2014 - Many Parameter Sets in a Multicompartment Model Os.pdf</url></pdf-urls><text-urls><url>https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3972722/</url><url>https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3972722/</url><url>http://www.ncbi.nlm.nih.gov/pubmed/24695714</url><url>http://www.ncbi.nlm.nih.gov/pubmed/24695714</url><url>http://www.ncbi.nlm.nih.gov/pubmed/24695714</url><url>/home/alec/Zotero/storage/ZXBRYSQJ/tab-figures-data.html</url><url>/home/alec/Zotero/storage/GMXBRDPE/4963.html</url><url>/home/alec/Zotero/storage/TTJ4DM7C/4963.html</url></text-urls></urls><access-date>2017-04-20 01:10:30</access-date></record><record><database name="MyLibrary">MyLibrary</database><source-app name="Zotero">Zotero</source-app><ref-type name="Journal Article">17</ref-type><contributors><authors><author>Goldman, Mark S.</author><author>Golowasch, Jorge</author><author>Marder, Eve</author><author>Abbott, L. F.</author></authors></contributors><titles><title>Global Structure, Robustness, and Modulation of Neuronal Models</title><secondary-title>Journal of Neuroscience</secondary-title></titles><periodical><full-title>Journal of Neuroscience</full-title><abbr-1>J. Neurosci.</abbr-1></periodical><pages>5229-5238</pages><volume>21</volume><number>14</number><issue>14</issue><keywords><keyword>Animals</keyword><keyword>Ganglia, Invertebrate</keyword><keyword>Models, Neurological</keyword><keyword>Nerve Net</keyword><keyword>Neurons</keyword><keyword>Synaptic Transmission</keyword><keyword>Acetylcholine</keyword><keyword>Glutamic Acid</keyword><keyword>Membrane Potentials</keyword><keyword>Motor Neurons</keyword><keyword>Pylorus</keyword><keyword>stomatogastric ganglion</keyword><keyword>Action Potentials</keyword><keyword>Interneurons</keyword><keyword>Synapses</keyword><keyword>Computer Simulation</keyword><keyword>Brain</keyword><keyword>Potassium Channels</keyword><keyword>Neural Inhibition</keyword><keyword>Periodicity</keyword><keyword>Algorithms</keyword><keyword>Brachyura</keyword><keyword>Potassium</keyword><keyword>Biophysics</keyword><keyword>Patch-Clamp Techniques</keyword><keyword>Biological Clocks</keyword><keyword>Biophysical Phenomena</keyword><keyword>bursting neuron</keyword><keyword>Cell Communication</keyword><keyword>Central Pattern Generator</keyword><keyword>Central Pattern Generators</keyword><keyword>conductance-based model</keyword><keyword>dynamic clamp</keyword><keyword>Electric Stimulation</keyword><keyword>Electrophysiological Phenomena</keyword><keyword>Gap Junctions</keyword><keyword>Gene Expression Regulation</keyword><keyword>Heart</keyword><keyword>Heart Rate</keyword><keyword>Hirudo medicinalis</keyword><keyword>Homeostatic Regulation of Excitability</keyword><keyword>In Vitro Techniques</keyword><keyword>Kinetics</keyword><keyword>Molecular Sequence Data</keyword><keyword>Nervous System</keyword><keyword>Neural Conduction</keyword><keyword>Neural Pathways</keyword><keyword>neuromodulator</keyword><keyword>parameter space</keyword><keyword>RNA, Messenger</keyword><keyword>Sensory Gating</keyword><keyword>Shal Potassium Channels</keyword><keyword>Species Specificity</keyword><keyword>Statistics as Topic</keyword><keyword>Stomatogastric Nervous System</keyword><keyword>Stomatognathic System</keyword><keyword>Tongue</keyword></keywords><dates><year>2001</year><pub-dates><date>2001/07/15</date></pub-dates></dates><isbn>0270-6474, 1529-2401</isbn><electronic-resource-num>10.1523/JNEUROSCI.21-14-05229.2001</electronic-resource-num><abstract>The electrical characteristics of many neurons are remarkably robust in the face of changing internal and external conditions. At the same time, neurons can be highly sensitive to neuromodulators. We find correlates of this dual robustness and sensitivity in a global analysis of the structure of a conductance-based model neuron. We vary the maximal conductance parameters of the model neuron and, for each set of parameters tested, characterize the activity pattern generated by the cell as silent, tonically firing, or bursting. Within the parameter space of the five maximal conductances of the model, we find directions, representing concerted changes in multiple conductances, along which the basic pattern of neural activity does not change. In other directions, relatively small concurrent changes in a few conductances can induce transitions between these activity patterns. The global structure of the conductance-space maps implies that neuromodulators that alter a sensitive set of conductances will have powerful, and possibly state-dependent, effects. Other modulators that may have no direct impact on the activity of the neuron may nevertheless change the effects of such direct modulators via this state dependence. Some of the results and predictions arising from the model studies are replicated and verified in recordings of stomatogastric ganglion neurons using the dynamic clamp.</abstract><remote-database-name>www.jneurosci.org</remote-database-name><language>en</language><urls><web-urls><url>http://www.jneurosci.org/content/21/14/5229</url></web-urls><pdf-urls><url>/home/alec/Zotero/storage/K2JUEXFG/Turrigiano et al. - 1995 - Selective regulation of current densities underlie.pdf</url><url>/home/alec/Zotero/storage/AERMLPKQ/Schulz et al. - 2007 - Quantitative expression profiling of identified ne.pdf</url><url>/home/alec/Zotero/storage/CG8PFQUC/Goldman et al. - 2001 - Global Structure, Robustness, and Modulation of Ne.pdf</url><url>/home/alec/Zotero/storage/HBPVBTQM/Turrigiano et al. - 1995 - Selective regulation of current densities underlie.pdf</url><url>/home/alec/Zotero/storage/PFQP2LBD/Hamood et al. - 2015 - QUANTITATIVE REEVALUATION OF THE EFFECTS OF SHORT-.pdf</url><url>/home/alec/Zotero/storage/WIYG4USV/Schulz et al. - 2007 - Quantitative expression profiling of identified ne.pdf</url><url>/home/alec/Zotero/storage/8XFNTXVN/Goldman et al. - 2001 - Global Structure, Robustness, and Modulation of Ne.pdf</url><url>/home/alec/Zotero/storage/9C5SE3ZS/Goldman et al. - 2001 - Global Structure, Robustness, and Modulation of Ne.pdf</url><url>/home/alec/Zotero/storage/4DB9PJ2D/Goldman et al. - 2001 - Global Structure, Robustness, and Modulation of Ne.pdf</url><url>/home/alec/Zotero/storage/ZAUHCEXA/Hamood et al. - 2015 - Quantitative Reevaluation of the Effects of Short-.pdf</url><url>/home/alec/Zotero/storage/GUE9JGV3/Marder and Bucher - 2001 - Central pattern generators and the control of rhyt.pdf</url><url>/home/alec/Zotero/storage/G88A6KZB/Marder and Bucher - 2001 - Central pattern generators and the control of rhyt.pdf</url></pdf-urls><text-urls><url>https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4408878/</url><url>http://www.ncbi.nlm.nih.gov/pubmed/21724619</url><url>http://www.ncbi.nlm.nih.gov/pubmed/23473315</url><url>http://www.ncbi.nlm.nih.gov/pubmed/19838180</url><url>http://www.ncbi.nlm.nih.gov/pubmed/7538565</url><url>http://www.ncbi.nlm.nih.gov/pubmed/25460072</url><url>http://www.ncbi.nlm.nih.gov/pubmed/21724619</url><url>http://www.ncbi.nlm.nih.gov/pubmed/23473315</url><url>http://www.ncbi.nlm.nih.gov/pubmed/19838180</url><url>http://www.ncbi.nlm.nih.gov/pubmed/16444270</url><url>http://www.ncbi.nlm.nih.gov/pubmed/7538565</url><url>http://www.ncbi.nlm.nih.gov/pubmed/17652510</url><url>http://www.ncbi.nlm.nih.gov/pubmed/17652510</url><url>http://www.ncbi.nlm.nih.gov/pubmed/11438598</url><url>http://www.ncbi.nlm.nih.gov/pubmed/25460072</url><url>http://www.ncbi.nlm.nih.gov/pubmed/16444270</url><url>http://www.ncbi.nlm.nih.gov/pubmed/11438598</url><url>http://www.ncbi.nlm.nih.gov/pubmed/11438598</url><url>http://www.ncbi.nlm.nih.gov/pubmed/11438598</url><url>/home/alec/Zotero/storage/9GMESYLH/S0960982201005814.html</url><url>/home/alec/Zotero/storage/44SYAH2J/S0960982201005814.html</url><url>/home/alec/Zotero/storage/RSFCWPP5/3640.html</url><url>/home/alec/Zotero/storage/UQJDVKL8/ENEURO.0058-14.html</url><url>/home/alec/Zotero/storage/58C4C3TN/13187.html</url><url>/home/alec/Zotero/storage/7CJ4LIRY/5229.html</url><url>/home/alec/Zotero/storage/TIV6BFUA/3640.html</url><url>/home/alec/Zotero/storage/MRN9KPV8/13187.html</url><url>/home/alec/Zotero/storage/HBZHT5MW/5229.html</url><url>/home/alec/Zotero/storage/V42S8SA9/5229.html</url><url>/home/alec/Zotero/storage/PHBRXSZ8/5229.html</url></text-urls></urls><access-date>2018-07-11 20:36:00</access-date></record><record><database name="MyLibrary">MyLibrary</database><source-app name="Zotero">Zotero</source-app><ref-type name="Journal Article">17</ref-type><contributors><authors><author>O'Leary, Timothy</author><author>Williams, Alex H.</author><author>Caplan, Jonathan S.</author><author>Marder, Eve</author></authors></contributors><titles><title>Correlations in ion channel expression emerge from homeostatic tuning rules</title><secondary-title>Proceedings of the National Academy of Sciences of the United States of America</secondary-title></titles><periodical><full-title>Proceedings of the National Academy of Sciences of the United States of America</full-title><abbr-1>Proc. Natl. Acad. Sci. U.S.A.</abbr-1></periodical><pages>E2645-2654</pages><volume>110</volume><number>28</number><issue>28</issue><keywords><keyword>Neurons</keyword><keyword>Action Potentials</keyword><keyword>Homeostasis</keyword><keyword>Ion Channels</keyword><keyword>computational models</keyword><keyword>control theory</keyword><keyword>Models, Biological</keyword><keyword>neuronal excitability</keyword><keyword>robustness</keyword></keywords><dates><year>2013</year><pub-dates><date>Jul 09, 2013</date></pub-dates></dates><isbn>1091-6490</isbn><electronic-resource-num>10.1073/pnas.1309966110</electronic-resource-num><abstract>Experimental observations reveal that the expression levels of different ion channels vary across neurons of a defined type, even when these neurons exhibit stereotyped electrical properties. However, there are robust correlations between different ion channel expression levels, although the mechanisms that determine these correlations are unknown. Using generic model neurons, we show that correlated conductance expression can emerge from simple homeostatic control mechanisms that couple expression rates of individual conductances to cellular readouts of activity. The correlations depend on the relative rates of expression of different conductances. Thus, variability is consistent with homeostatic regulation and the structure of this variability reveals quantitative relations between regulation dynamics of different conductances. Furthermore, we show that homeostatic regulation is remarkably insensitive to the details that couple the regulation of a given conductance to overall neuronal activity because of degeneracy in the function of multiple conductances and can be robust to "antihomeostatic" regulation of a subset of conductances expressed in a cell.</abstract><remote-database-name>PubMed</remote-database-name><language>eng</language><urls><pdf-urls><url>/home/alec/Zotero/storage/72NQE8AD/O'Leary et al. - 2013 - Correlations in ion channel expression emerge from.pdf</url></pdf-urls><text-urls><url>http://www.ncbi.nlm.nih.gov/pubmed/23798391</url><url>http://www.ncbi.nlm.nih.gov/pubmed/23798391</url><url>/home/alec/Zotero/storage/TRXCKR6W/E2645.html</url></text-urls></urls></record><record><database name="MyLibrary">MyLibrary</database><source-app name="Zotero">Zotero</source-app><ref-type name="Journal Article">17</ref-type><contributors><authors><author>Prinz, Astrid A.</author><author>Billimoria, Cyrus P.</author><author>Marder, Eve</author></authors></contributors><titles><title>Alternative to hand-tuning conductance-based models: construction and analysis of databases of model neurons</title><secondary-title>Journal of Neurophysiology</secondary-title><short-title>Alternative to hand-tuning conductance-based models</short-title></titles><periodical><full-title>Journal of Neurophysiology</full-title><abbr-1>J. Neurophysiol.</abbr-1></periodical><pages>3998-4015</pages><volume>90</volume><number>6</number><issue>6</issue><keywords><keyword>Animals</keyword><keyword>Models, Neurological</keyword><keyword>Neurons</keyword><keyword>Membrane Potentials</keyword><keyword>Nephropidae</keyword><keyword>Computer Simulation</keyword><keyword>Electrophysiology</keyword><keyword>Algorithms</keyword><keyword>Electric Stimulation</keyword><keyword>Databases, Factual</keyword></keywords><dates><year>2003</year><pub-dates><date>Dec 2003</date></pub-dates></dates><isbn>0022-3077</isbn><electronic-resource-num>10.1152/jn.00641.2003</electronic-resource-num><abstract>Conventionally, the parameters of neuronal models are hand-tuned using trial-and-error searches to produce a desired behavior. Here, we present an alternative approach. We have generated a database of about 1.7 million single-compartment model neurons by independently varying 8 maximal membrane conductances based on measurements from lobster stomatogastric neurons. We classified the spontaneous electrical activity of each model neuron and its responsiveness to inputs during runtime with an adaptive algorithm and saved a reduced version of each neuron's activity pattern. Our analysis of the distribution of different activity types (silent, spiking, bursting, irregular) in the 8-dimensional conductance space indicates that the coarse grid of conductance values we chose is sufficient to capture the salient features of the distribution. The database can be searched for different combinations of neuron properties such as activity type, spike or burst frequency, resting potential, frequency-current relation, and phase-response curve. We demonstrate how the database can be screened for models that reproduce the behavior of a specific biological neuron and show that the contents of the database can give insight into the way a neuron's membrane conductances determine its activity pattern and response properties. Similar databases can be constructed to explore parameter spaces in multicompartmental models or small networks, or to examine the effects of changes in the voltage dependence of currents. In all cases, database searches can provide insight into how neuronal and network properties depend on the values of the parameters in the models.</abstract><remote-database-name>PubMed</remote-database-name><language>eng</language><urls><text-urls><url>http://www.ncbi.nlm.nih.gov/pubmed/12944532</url><url>http://www.ncbi.nlm.nih.gov/pubmed/12944532</url></text-urls></urls></record><record><database name="MyLibrary">MyLibrary</database><source-app name="Zotero">Zotero</source-app><ref-type name="Journal Article">17</ref-type><contributors><authors><author>O'Dowd, D. K.</author><author>Aldrich, R. W.</author></authors></contributors><titles><title>Voltage-clamp analysis of sodium channels in wild-type and mutant Drosophila neurons</title><secondary-title>The Journal of Neuroscience: The Official Journal of the Society for Neuroscience</secondary-title></titles><periodical><full-title>The Journal of Neuroscience: The Official Journal of the Society for Neuroscience</full-title><abbr-1>J. Neurosci.</abbr-1></periodical><pages>3633-3643</pages><volume>8</volume><number>10</number><issue>10</issue><keywords><keyword>Animals</keyword><keyword>Neurons</keyword><keyword>Electrophysiology</keyword><keyword>Sodium Channels</keyword><keyword>In Vitro Techniques</keyword><keyword>Kinetics</keyword><keyword>Drosophila</keyword><keyword>Genotype</keyword><keyword>Mutation</keyword></keywords><dates><year>1988</year><pub-dates><date>Oct 1988</date></pub-dates></dates><isbn>0270-6474</isbn><abstract>In this study we describe a preparation in which we examined directly, using tight-seal whole-cell recording, sodium currents from embryonic Drosophila neurons maintained in culture. Sodium currents were expressed in approximately 65% of the neurons prepared from wild-type Drosophila embryos when examined at room temperature, 24 hr after plating. While current density was low, other features of the sodium current in wild-type neurons, including the voltage sensitivity, steady-state inactivation, macroscopic time course, and TTX sensitivity were similar to those found in other excitable cells. Physiological and biochemical evidence has led to the suggestion that mutations in the nap, seizure, and tip-E loci of Drosophila may affect voltage-dependent sodium channels. There was no significant difference in the percentage of neurons expressing sodium currents in cultures prepared from embryos with mutations at the nap, sei or tip-E loci compared with wild-type cultures. Sodium currents recorded from napts appeared similar in all of the properties examined to those in wild-type cells. However, neuronal sodium current density was 40-60% lower in cultures prepared from both tip-E and seits1 embryos. The voltage dependence and gating properties of these sodium channels, as well as the TTX sensitivity, appear similar to wild type. These results indicate that both the tip-E and sei loci are important in regulation of sodium current density in embryonic neurons.</abstract><remote-database-name>PubMed</remote-database-name><language>eng</language><urls><text-urls><url>http://www.ncbi.nlm.nih.gov/pubmed/2848103</url></text-urls></urls></record><record><database name="MyLibrary">MyLibrary</database><source-app name="Zotero">Zotero</source-app><ref-type name="Journal Article">17</ref-type><contributors><authors><author>Hodgkin, A. L.</author><author>Huxley, A. F.</author></authors></contributors><titles><title>A quantitative description of membrane current and its application to conduction and excitation in nerve</title><secondary-title>The Journal of Physiology</secondary-title></titles><periodical><full-title>The Journal of Physiology</full-title><abbr-1>J Physiol</abbr-1></periodical><pages>500-544</pages><volume>117</volume><number>4</number><issue>4</issue><dates><year>1952</year><pub-dates><date>1952-08-28</date></pub-dates></dates><isbn>0022-3751</isbn><remote-database-name>PubMed Central</remote-database-name><urls><web-urls><url>https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1392413/</url></web-urls><pdf-urls><url>/home/alec/Zotero/storage/3A2PJ9VN/Hodgkin and Huxley - 1952 - A quantitative description of membrane current and.pdf</url><url>/home/alec/Zotero/storage/9WXGVCQJ/Hodgkin and Huxley - 1952 - A quantitative description of membrane current and.pdf</url><url>/home/alec/Zotero/storage/VY5H4B4V/Hodgkin and Huxley - 1952 - A quantitative description of membrane current and.pdf</url></pdf-urls><text-urls><url>http://www.ncbi.nlm.nih.gov/pmc/articles/PMC1392413/</url><url>https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1392413/</url><url>https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1392413/</url></text-urls></urls><access-date>2018-06-13 17:38:01</access-date></record><record><database name="MyLibrary">MyLibrary</database><source-app name="Zotero">Zotero</source-app><ref-type name="Journal Article">17</ref-type><contributors><authors><author>Brette, Romain</author><author>Rudolph, Michelle</author><author>Carnevale, Ted</author><author>Hines, Michael</author><author>Beeman, David</author><author>Bower, James M.</author><author>Diesmann, Markus</author><author>Morrison, Abigail</author><author>Goodman, Philip H.</author><author>Harris, Frederick C.</author><author>Zirpe, Milind</author><author>Natschläger, Thomas</author><author>Pecevski, Dejan</author><author>Ermentrout, Bard</author><author>Djurfeldt, Mikael</author><author>Lansner, Anders</author><author>Rochel, Olivier</author><author>Vieville, Thierry</author><author>Muller, Eilif</author><author>Davison, Andrew P.</author><author>Boustani, Sami El</author><author>Destexhe, Alain</author></authors></contributors><titles><title>Simulation of networks of spiking neurons: A review of tools and strategies</title><secondary-title>Journal of Computational Neuroscience</secondary-title><short-title>Simulation of networks of spiking neurons</short-title></titles><periodical><full-title>Journal of Computational Neuroscience</full-title><abbr-1>J Comput Neurosci</abbr-1></periodical><pages>349-398</pages><volume>23</volume><number>3</number><issue>3</issue><dates><year>2007</year><pub-dates><date>2007/12/01</date></pub-dates></dates><isbn>1573-6873</isbn><electronic-resource-num>10.1007/s10827-007-0038-6</electronic-resource-num><abstract>We review different aspects of the simulation of spiking neural networks. We start by reviewing the different types of simulation strategies and algorithms that are currently implemented. We next review the precision of those simulation strategies, in particular in cases where plasticity depends on the exact timing of the spikes. We overview different simulators and simulation environments presently available (restricted to those freely available, open source and documented). For each simulation tool, its advantages and pitfalls are reviewed, with an aim to allow the reader to identify which simulator is appropriate for a given task. Finally, we provide a series of benchmark simulations of different types of networks of spiking neurons, including Hodgkin–Huxley type, integrate-and-fire models, interacting with current-based or conductance-based synapses, using clock-driven or event-driven integration strategies. The same set of models are implemented on the different simulators, and the codes are made available. The ultimate goal of this review is to provide a resource to facilitate identifying the appropriate integration strategy and simulation tool to use for a given modeling problem related to spiking neural networks.</abstract><remote-database-name>link.springer.com</remote-database-name><language>en</language><urls><web-urls><url>https://link.springer.com/article/10.1007/s10827-007-0038-6</url></web-urls><pdf-urls><url>/home/alec/Zotero/storage/F3D3SS3V/Brette et al. - 2007 - Simulation of networks of spiking neurons A revie.pdf</url><url>/home/alec/Zotero/storage/8CPUZUXZ/Brette et al. - 2007 - Simulation of networks of spiking neurons A revie.pdf</url><url>/home/alec/Zotero/storage/AUC7XKTF/Brette et al. - 2007 - Simulation of networks of spiking neurons A revie.pdf</url><url>/home/alec/Zotero/storage/ZL3EWGTM/Brette et al. - 2007 - Simulation of networks of spiking neurons A revie.pdf</url></pdf-urls><text-urls><url>/home/alec/Zotero/storage/N9Y64LAF/s10827-007-0038-6.html</url><url>/home/alec/Zotero/storage/IKAQAUT8/10.html</url><url>/home/alec/Zotero/storage/DH539GHI/s10827-007-0038-6.html</url><url>/home/alec/Zotero/storage/2QNMHF99/s10827-007-0038-6.html</url></text-urls></urls><access-date>2018-06-05 00:26:24</access-date></record><record><database name="MyLibrary">MyLibrary</database><source-app name="Zotero">Zotero</source-app><ref-type name="Journal Article">17</ref-type><contributors><authors><author>Marder, Eve</author><author>Haddad, Sara A.</author><author>Goeritz, Marie L.</author><author>Rosenbaum, Philipp</author><author>Kispersky, Tilman</author></authors></contributors><titles><title>How can motor systems retain performance over a wide temperature range? Lessons from the crustacean stomatogastric nervous system</title><secondary-title>Journal of Comparative Physiology. A, Neuroethology, Sensory, Neural, and Behavioral Physiology</secondary-title><short-title>How can motor systems retain performance over a wide temperature range?</short-title></titles><periodical><full-title>Journal of Comparative Physiology. A, Neuroethology, Sensory, Neural, and Behavioral Physiology</full-title><abbr-1>J Comp Physiol A Neuroethol Sens Neural Behav Physiol</abbr-1></periodical><pages>851-856</pages><volume>201</volume><number>9</number><issue>9</issue><dates><year>2015</year><pub-dates><date>2015</date></pub-dates></dates><isbn>0340-7594</isbn><electronic-resource-num>10.1007/s00359-014-0975-2</electronic-resource-num><abstract>Marine invertebrates, such as lobsters and crabs, deal with a widely and wildly fluctuating temperature environment. Here, we describe the effects of changing temperature on the motor patterns generated by the stomatogastric nervous system of the crab, Cancer borealis. Over a broad range of “permissive” temperatures, the pyloric rhythm increases in frequency but maintains its characteristic phase relationships. Nonetheless, at more extreme high temperatures, the normal triphasic pyloric rhythm breaks down, or “crashes”. We present both experimental and computational approaches to understanding the stability of both single neurons and networks to temperature perturbations, and discuss data that shows that the “crash” temperatures themselves may be environmentally regulated. These approaches provide insight into how the nervous system can be stable to a global perturbation, such as temperature, in spite of the fact that all biological processes are temperature dependent.</abstract><remote-database-name>PubMed Central</remote-database-name><urls><web-urls><url>http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4552768/</url></web-urls><pdf-urls><url>/home/alec/Zotero/storage/FEL7E5FN/Marder et al. - 2015 - How can motor systems retain performance over a wi.pdf</url><url>/home/alec/Zotero/storage/C966V5EI/Marder et al. - 2015 - How can motor systems retain performance over a wi.pdf</url></pdf-urls><text-urls><url>http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4552768/</url><url>/home/alec/Zotero/storage/H68GBMFQ/s00359-014-0975-2.html</url></text-urls></urls><access-date>2017-04-25 23:21:53</access-date></record><record><database name="MyLibrary">MyLibrary</database><source-app name="Zotero">Zotero</source-app><ref-type name="Journal Article">17</ref-type><contributors><authors><author>Soofi, Wafa</author><author>Goeritz, Marie L.</author><author>Kispersky, Tilman J.</author><author>Prinz, Astrid A.</author><author>Marder, Eve</author><author>Stein, Wolfgang</author></authors></contributors><titles><title>Phase maintenance in a rhythmic motor pattern during temperature changes in vivo</title><secondary-title>Journal of Neurophysiology</secondary-title></titles><periodical><full-title>Journal of Neurophysiology</full-title><abbr-1>J Neurophysiol</abbr-1></periodical><pages>2603-2613</pages><volume>111</volume><number>12</number><issue>12</issue><dates><year>2014</year><pub-dates><date>2014-6-15</date></pub-dates></dates><isbn>0022-3077</isbn><electronic-resource-num>10.1152/jn.00906.2013</electronic-resource-num><abstract>Central-pattern-generating neural circuits function reliably throughout an animal's life, despite constant molecular turnover and environmental perturbations. Fluctuations in temperature pose a problem to the nervous systems of poikilotherms because their body temperature follows the ambient temperature, thus affecting the temperature-dependent dynamics of various subcellular components that constitute neuronal circuits. In the crustacean stomatogastric nervous system, the pyloric circuit produces a triphasic rhythm comprising the output of the pyloric dilator, lateral pyloric, and pyloric constrictor neurons. In vitro, the phase relationships of these neurons are maintained over a fourfold change in pyloric frequency as temperature increases from 7°C to 23°C. To determine whether these temperature effects are also found in intact crabs, in the presence of sensory feedback and neuromodulator-rich environments, we measured the temperature dependence of the pyloric frequency and phases in vivo by implanting extracellular electrodes into Cancer borealis and Cancer pagurus and shifting tank water temperature from 11°C to 26°C. Pyloric frequency in the intact crab increased significantly with temperature (Q10 = 2–2.5), while pyloric phases were generally conserved. For a subset of the C. borealis experiments, animals were subsequently dissected and the stomatogastric ganglion subjected to a similar temperature ramp in vitro. We found that the maximal frequency attained at high temperatures in vivo is lower than it is under in vitro conditions. Our results demonstrate that, over a wide temperature range, the phases of the pyloric rhythm in vivo are generally preserved, but that the frequency range is more restricted than it is in vitro.</abstract><remote-database-name>PubMed Central</remote-database-name><urls><web-urls><url>https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4044431/</url></web-urls><pdf-urls><url>/home/alec/Zotero/storage/7EZ8X689/Soofi et al. - 2014 - Phase maintenance in a rhythmic motor pattern duri.pdf</url></pdf-urls><text-urls><url>https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4044431/</url><url>/home/alec/Zotero/storage/MSI2EXSZ/jn.00906.html</url></text-urls></urls><access-date>2017-12-05 02:32:34</access-date></record><record><database name="MyLibrary">MyLibrary</database><source-app name="Zotero">Zotero</source-app><ref-type name="Journal Article">17</ref-type><contributors><authors><author>Hamood, Albert W.</author><author>Marder, Eve</author></authors></contributors><titles><title>Animal-to-animal variability in neuromodulation and circuit function</title><secondary-title>Cold Spring Harbor symposia on quantitative biology</secondary-title></titles><periodical><full-title>Cold Spring Harbor symposia on quantitative biology</full-title><abbr-1>Cold Spring Harb Symp Quant Biol</abbr-1></periodical><pages>21-28</pages><volume>79</volume><dates><year>2014</year><pub-dates><date>2014</date></pub-dates></dates><isbn>0091-7451</isbn><electronic-resource-num>10.1101/sqb.2014.79.024828</electronic-resource-num><abstract>Each animal alive in the world is different from all other individuals, while sharing most attributes of form and function with others of the same species. Still other attributes are shared within a phylum, and still others are common to most eukaryotic organisms. All animals have mechanisms that modulate the strength of their synapses or alter the intrinsic excitability of component neurons. What animal-to-animal variability in behavior arises from differences in neuronal structure, ion channel expression, or connectivity, and what variability arises from neuromodulation of brain states? Conversely, can robust behavior be maintained despite variability in circuit components by the action of neuromodulatory inputs? These are fundamental issues relevant to all nervous systems that have been illuminated by many years of study of the small, rhythmic motor circuits found in the crustacean stomatogastric nervous system.</abstract><remote-database-name>PubMed Central</remote-database-name><urls><web-urls><url>https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4610821/</url></web-urls><pdf-urls><url>/home/alec/Zotero/storage/KJKRXW4X/Hamood and Marder - 2014 - Animal-to-Animal Variability in Neuromodulation an.pdf</url><url>/home/alec/Zotero/storage/3BZYNUSR/Hamood and Marder - 2014 - Animal-to-animal variability in neuromodulation an.pdf</url></pdf-urls><text-urls><url>https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4610821/</url><url>http://www.ncbi.nlm.nih.gov/pubmed/25876630</url><url>/home/alec/Zotero/storage/G5SGSB57/21.html</url></text-urls></urls><access-date>2017-11-16 00:36:06</access-date></record><record><database name="MyLibrary">MyLibrary</database><source-app name="Zotero">Zotero</source-app><ref-type name="Journal Article">17</ref-type><contributors><authors><author>Rinberg, Anatoly</author><author>Taylor, Adam L.</author><author>Marder, Eve</author></authors></contributors><titles><title>The Effects of Temperature on the Stability of a Neuronal Oscillator</title><secondary-title>PLoS Computational Biology</secondary-title></titles><periodical><full-title>PLoS Computational Biology</full-title><abbr-1>PLoS Comput Biol</abbr-1></periodical><volume>9</volume><number>1</number><issue>1</issue><keywords><keyword>Neurons</keyword><keyword>Membrane potential</keyword><keyword>Neural networks</keyword><keyword>Action potentials</keyword><keyword>Bifurcation theory</keyword><keyword>Crabs</keyword><keyword>Pacemakers</keyword><keyword>Q10 temperature coefficient</keyword></keywords><dates><year>2013</year><pub-dates><date>2013-1-10</date></pub-dates></dates><isbn>1553-734X</isbn><electronic-resource-num>10.1371/journal.pcbi.1002857</electronic-resource-num><abstract>The crab Cancer borealis undergoes large daily fluctuations in environmental temperature (8–24°C) and must maintain appropriate neural function in the face of this perturbation. In the pyloric circuit of the crab stomatogastric ganglion, we pharmacologically isolated the pacemaker kernel (the AB and PD neurons) and characterized its behavior in response to temperature ramps from 7°C to 31°C. For moderate temperatures, the pacemaker displayed a frequency-temperature curve statistically indistinguishable from that of the intact circuit, and like the intact circuit maintained a constant duty cycle. At high temperatures (above 23°C), a variety of different behaviors were seen: in some preparations the pacemaker increased in frequency, in some it slowed, and in many preparations the pacemaker stopped oscillating (“crashed”). Furthermore, these crashes seemed to fall into two qualitatively different classes. Additionally, the animal-to-animal variability in frequency increased at high temperatures. We used a series of Morris-Lecar mathematical models to gain insight into these phenomena. The biophysical components of the final model have temperature sensitivities similar to those found in nature, and can crash via two qualitatively different mechanisms that resemble those observed experimentally. The crash type is determined by the precise parameters of the model at the reference temperature, 11°C, which could explain why some preparations seem to crash in one way and some in another. Furthermore, even models with very similar behavior at the reference temperature diverge greatly at high temperatures, resembling the experimental observations., The nervous systems of cold-blooded animals must maintain essential function despite fluctuations in environmental temperature. We studied the pyloric rhythm of the crab, Cancer borealis. The pyloric rhythm is important for the animal's feeding behavior, and previous work has shown that relative timing, or phase, of the neurons in the pyloric circuit is temperature invariant over a range of physiologically realistic temperatures (7 to 23°C), although the frequency of the rhythm increases. At higher temperatures, the rhythm often becomes disorganized or stops. In this paper we present experimental work on the isolated pacemaker of the pyloric rhythm together with a computational model that explores the loss of stability of the pacemaker as a function of temperature. Experimentally, we found that the isolated pacemaker responds to temperature similarly to the intact network, and maintains constant duty cycle over large temperature ranges. At high temperatures, about half of the pacemakers stop oscillating, reminiscent of certain mathematical bifurcations. We varied the temperature dependence and conductance densities of a simple model oscillator, and characterized its bifurcations as a function of temperature. We found that particular temperature-dependent relationships must be maintained to provide robust temperature performance of oscillators with variable underlying conductances.</abstract><remote-database-name>PubMed Central</remote-database-name><urls><web-urls><url>http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3542102/</url></web-urls><pdf-urls><url>/home/alec/Zotero/storage/Q8MBB8I7/Rinberg et al. - 2013 - The Effects of Temperature on the Stability of a N.pdf</url><url>/home/alec/Zotero/storage/8JBQ3ZMU/Rinberg et al. - 2013 - The Effects of Temperature on the Stability of a N.pdf</url></pdf-urls><text-urls><url>http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3542102/</url><url>/home/alec/Zotero/storage/MS3QK5CK/article.html</url></text-urls></urls><access-date>2017-04-25 23:33:45</access-date></record><record><database name="MyLibrary">MyLibrary</database><source-app name="Zotero">Zotero</source-app><ref-type name="Journal Article">17</ref-type><contributors><authors><author>Tang, Lamont S.</author><author>Taylor, Adam L.</author><author>Rinberg, Anatoly</author><author>Marder, Eve</author></authors></contributors><titles><title>Robustness of a rhythmic circuit to short- and long-term temperature changes</title><secondary-title>The Journal of Neuroscience: The Official Journal of the Society for Neuroscience</secondary-title></titles><periodical><full-title>The Journal of Neuroscience: The Official Journal of the Society for Neuroscience</full-title><abbr-1>J. Neurosci.</abbr-1></periodical><pages>10075-10085</pages><volume>32</volume><number>29</number><issue>29</issue><keywords><keyword>Animals</keyword><keyword>Ganglia, Invertebrate</keyword><keyword>Neurons</keyword><keyword>Pylorus</keyword><keyword>Periodicity</keyword><keyword>Brachyura</keyword><keyword>Acclimatization</keyword><keyword>Environment</keyword><keyword>Temperature</keyword></keywords><dates><year>2012</year><pub-dates><date>Jul 18, 2012</date></pub-dates></dates><isbn>1529-2401</isbn><electronic-resource-num>10.1523/JNEUROSCI.1443-12.2012</electronic-resource-num><abstract>Recent computational and experimental work has shown that similar network performance can result from variable sets of synaptic and intrinsic properties. Because temperature is a global perturbation that differentially influences every biological process within the nervous system, one might therefore expect that individual animals would respond differently to temperature. Nonetheless, the phase relationships of the pyloric rhythm of the stomatogastric ganglion (STG) of the crab, Cancer borealis, are remarkably invariant between 7 and 23°C (Tang et al., 2010). Here, we report that, when isolated STG preparations were exposed to more extreme temperature ranges, their networks became nonrhythmic, or "crashed", in a reversible fashion. Animals were acclimated for at least 3 weeks at 7, 11, or 19°C. When networks from the acclimated animals were perturbed by acute physiologically relevant temperature ramps (11-23°C), the network frequency and phase relationships were independent of the acclimation group. At high acute temperatures (&gt;23°C), circuits from the cold-acclimated animals produced less-regular pyloric rhythms than those from warm-acclimated animals. At high acute temperatures, phase relationships between pyloric neurons were more variable from animal to animal than at moderate acute temperatures, suggesting that individual differences across animals in intrinsic circuit parameters are revealed at high temperatures. This shows that individual and variable neuronal circuits can behave similarly in normal conditions, but their behavior may diverge when confronted with extreme external perturbations.</abstract><remote-database-name>PubMed</remote-database-name><language>eng</language><urls><pdf-urls><url>/home/alec/Zotero/storage/MSWMWFR7/Tang et al. - 2012 - Robustness of a Rhythmic Circuit to Short- and Lon.pdf</url></pdf-urls><text-urls><url>http://www.ncbi.nlm.nih.gov/pubmed/22815521</url><url>http://www.ncbi.nlm.nih.gov/pubmed/22815521</url><url>/home/alec/Zotero/storage/R6J9PYWY/10075.html</url></text-urls></urls></record><record><database name="MyLibrary">MyLibrary</database><source-app name="Zotero">Zotero</source-app><ref-type name="Journal Article">17</ref-type><contributors><authors><author>Tang, Lamont S.</author><author>Goeritz, Marie L.</author><author>Caplan, Jonathan S.</author><author>Taylor, Adam L.</author><author>Fisek, Mehmet</author><author>Marder, Eve</author></authors></contributors><titles><title>Precise Temperature Compensation of Phase in a Rhythmic Motor Pattern</title><secondary-title>PLOS Biology</secondary-title></titles><periodical><full-title>PLOS Biology</full-title><abbr-1>PLOS Biology</abbr-1></periodical><pages>e1000469</pages><volume>8</volume><number>8</number><issue>8</issue><keywords><keyword>Neurons</keyword><keyword>Membrane potential</keyword><keyword>Pacemakers</keyword><keyword>Q10 temperature coefficient</keyword><keyword>Ion channels</keyword><keyword>Motor neurons</keyword><keyword>Nervous system</keyword><keyword>Neural pathways</keyword></keywords><dates><year>2010</year><pub-dates><date>Aug 31, 2010</date></pub-dates></dates><isbn>1545-7885</isbn><electronic-resource-num>10.1371/journal.pbio.1000469</electronic-resource-num><abstract>Computational modeling and experimentation in a model system for network dynamics reveal how network phase relationships are temperature-compensated in terms of their underlying synaptic and intrinsic membrane currents.</abstract><remote-database-name>PLoS Journals</remote-database-name><urls><web-urls><url>http://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.1000469</url></web-urls><pdf-urls><url>/home/alec/Zotero/storage/TVVK8GHZ/Tang et al. - 2010 - Precise Temperature Compensation of Phase in a Rhy.pdf</url><url>/home/alec/Zotero/storage/YFPRFPSH/Tang et al. - 2010 - Precise Temperature Compensation of Phase in a Rhy.pdf</url></pdf-urls><text-urls><url>/home/alec/Zotero/storage/JJFMSQ7J/article.html</url><url>/home/alec/Zotero/storage/P856V3YJ/article.html</url></text-urls></urls><access-date>2017-11-15 22:38:17</access-date></record><record><database name="MyLibrary">MyLibrary</database><source-app name="Zotero">Zotero</source-app><ref-type name="Journal Article">17</ref-type><contributors><authors><author>Williams, Alex H.</author><author>Kwiatkowski, Molly A.</author><author>Mortimer, Adam L.</author><author>Marder, Eve</author><author>Zeeman, Mary Lou</author><author>Dickinson, Patsy S.</author></authors></contributors><titles><title>Animal-to-animal variability in the phasing of the crustacean cardiac motor pattern: an experimental and computational analysis</title><secondary-title>Journal of Neurophysiology</secondary-title><short-title>Animal-to-animal variability in the phasing of the crustacean cardiac motor pattern</short-title></titles><periodical><full-title>Journal of Neurophysiology</full-title><abbr-1>Journal of Neurophysiology</abbr-1></periodical><pages>2451-2465</pages><volume>109</volume><number>10</number><issue>10</issue><dates><year>2013</year><pub-dates><date>February 27, 2013</date></pub-dates></dates><isbn>0022-3077</isbn><electronic-resource-num>10.1152/jn.01010.2012</electronic-resource-num><abstract>The cardiac ganglion (CG) of Homarus americanus is a central pattern generator that consists of two oscillatory groups of neurons: “small cells” (SCs) and “large cells” (LCs). We have shown that SCs and LCs begin their bursts nearly simultaneously but end their bursts at variable phases. This variability contrasts with many other central pattern generator systems in which phase is well maintained. To determine both the consequences of this variability and how CG phasing is controlled, we modeled the CG as a pair of Morris-Lecar oscillators coupled by electrical and excitatory synapses and constructed a database of 15,000 simulated networks using random parameter sets. These simulations, like our experimental results, displayed variable phase relationships, with the bursts beginning together but ending at variable phases. The model suggests that the variable phasing of the pattern has important implications for the functional role of the excitatory synapses. In networks in which the two oscillators had similar duty cycles, the excitatory coupling functioned to increase cycle frequency. In networks with disparate duty cycles, it functioned to decrease network frequency. Overall, we suggest that the phasing of the CG may vary without compromising appropriate motor output and that this variability may critically determine how the network behaves in response to manipulations.</abstract><remote-database-name>physiology.org (Atypon)</remote-database-name><urls><web-urls><url>http://www.physiology.org/doi/abs/10.1152/jn.01010.2012</url></web-urls><pdf-urls><url>/home/alec/Zotero/storage/69PBXRSU/Williams et al. - 2013 - Animal-to-animal variability in the phasing of the.pdf</url></pdf-urls><text-urls><url>/home/alec/Zotero/storage/2RBXWYB2/jn.01010.html</url></text-urls></urls><access-date>2018-02-14 23:50:34</access-date></record><record><database name="MyLibrary">MyLibrary</database><source-app name="Zotero">Zotero</source-app><ref-type name="Journal Article">17</ref-type><contributors><authors><author>Marder, Eve</author><author>Goeritz, Marie L</author><author>Otopalik, Adriane G</author></authors></contributors><titles><title>Robust circuit rhythms in small circuits arise from variable circuit components and mechanisms</title><secondary-title>Current Opinion in Neurobiology</secondary-title></titles><periodical><full-title>Current Opinion in Neurobiology</full-title><abbr-1>Current Opinion in Neurobiology</abbr-1></periodical><pages>156-163</pages><volume>31</volume><dates><year>2015</year><pub-dates><date>April 1, 2015</date></pub-dates></dates><isbn>0959-4388</isbn><electronic-resource-num>10.1016/j.conb.2014.10.012</electronic-resource-num><abstract>Small central pattern generating circuits found in invertebrates have significant advantages for the study of the circuit mechanisms that generate brain rhythms. Experimental and computational studies of small oscillatory circuits reveal that similar rhythms can arise from disparate mechanisms. Animal-to-animal variation in the properties of single neurons and synapses may underly robust circuit performance, and can be revealed by perturbations. Neuromodulation can produce altered circuit performance but also ensure reliable circuit function.</abstract><remote-database-name>ScienceDirect</remote-database-name><urls><web-urls><url>http://www.sciencedirect.com/science/article/pii/S0959438814002128</url></web-urls><text-urls><url>/home/alec/Zotero/storage/IASLHP8B/S0959438814002128.html</url></text-urls></urls><access-date>2018-02-14 23:48:54</access-date></record><record><database name="MyLibrary">MyLibrary</database><source-app name="Zotero">Zotero</source-app><ref-type name="Journal Article">17</ref-type><contributors><authors><author>Marder, Eve</author><author>O'Leary, Timothy</author><author>Shruti, Sonal</author></authors></contributors><titles><title>Neuromodulation of Circuits with Variable Parameters: Single Neurons and Small Circuits Reveal Principles of State-Dependent and Robust Neuromodulation</title><secondary-title>Annual Review of Neuroscience</secondary-title><short-title>Neuromodulation of Circuits with Variable Parameters</short-title></titles><periodical><full-title>Annual Review of Neuroscience</full-title></periodical><pages>329-346</pages><volume>37</volume><number>1</number><issue>1</issue><dates><year>2014</year><pub-dates><date>2014</date></pub-dates></dates><electronic-resource-num>10.1146/annurev-neuro-071013-013958</electronic-resource-num><abstract>Neuromodulation underlies many behavioral states and has been extensively studied in small circuits. This has allowed the systematic exploration of how neuromodulatory substances and the neurons that release them can influence circuit function. The physiological state of a network and its level of activity can have profound effects on how the modulators act, a phenomenon known as state dependence. We provide insights from experiments and computational work that show how state dependence can arise and the consequences it can have for cellular and circuit function. These observations pose a general unsolved question that is relevant to all nervous systems: How is robust modulation achieved in spite of animal-to-animal variability and degenerate, nonlinear mechanisms for the production of neuronal and network activity?</abstract><remote-database-name>Annual Reviews</remote-database-name><urls><web-urls><url>https://doi.org/10.1146/annurev-neuro-071013-013958</url></web-urls><pdf-urls><url>/home/alec/Zotero/storage/5J6KZ6UL/Marder et al. - 2014 - Neuromodulation of Circuits with Variable Paramete.pdf</url></pdf-urls></urls><access-date>2018-02-14 23:48:13</access-date></record><record><database name="MyLibrary">MyLibrary</database><source-app name="Zotero">Zotero</source-app><ref-type name="Journal Article">17</ref-type><contributors><authors><author>Roemschied, Frederic A</author><author>Eberhard, Monika JB</author><author>Schleimer, Jan-Hendrik</author><author>Ronacher, Bernhard</author><author>Schreiber, Susanne</author></authors></contributors><titles><title>Cell-intrinsic mechanisms of temperature compensation in a grasshopper sensory receptor neuron</title><secondary-title>eLife</secondary-title></titles><periodical><full-title>eLife</full-title><abbr-1>eLife</abbr-1></periodical><volume>3</volume><dates><year>2014</year><pub-dates><date>2014-5-08</date></pub-dates></dates><isbn>2050-084X</isbn><electronic-resource-num>10.7554/eLife.02078</electronic-resource-num><abstract>Changes in temperature affect biochemical reaction rates and, consequently, neural processing. The nervous systems of poikilothermic animals must have evolved mechanisms enabling them to retain their functionality under varying temperatures. Auditory receptor neurons of grasshoppers respond to sound in a surprisingly temperature-compensated manner: firing rates depend moderately on temperature, with average Q10 values around 1.5. Analysis of conductance-based neuron models reveals that temperature compensation of spike generation can be achieved solely relying on cell-intrinsic processes and despite a strong dependence of ion conductances on temperature. Remarkably, this type of temperature compensation need not come at an additional metabolic cost of spike generation. Firing rate-based information transfer is likely to increase with temperature and we derive predictions for an optimal temperature dependence of the tympanal transduction process fostering temperature compensation. The example of auditory receptor neurons demonstrates how neurons may exploit single-cell mechanisms to cope with multiple constraints in parallel., DOI:&#xD;http://dx.doi.org/10.7554/eLife.02078.001, Warm-blooded animals—including mammals and birds—expend large amounts of energy in keeping their body temperature constant regardless of how hot or cold their environment is. By contrast, the body temperature of cold-blooded animals—including amphibians, reptiles, and insects—follows that of their surroundings. Cold-blooded animals must therefore have evolved a means to cope with the effects of changes in temperature, but exactly how they do this is not clear., Now, Roemschied et al. have obtained new insights into this process by studying the nerve cells in grasshoppers that allow them to hear sounds. The auditory system of the grasshopper comprises sensory receptor neurons that are located on the abdomen of the insect. Sound waves move the tympanal membrane, which causes ion channels within the cell membranes to open. This enables the neurons to produce an electrical signal known as a spike., Recordings from grasshoppers revealed that changing the outside temperature by up to 10°C affected the rate at which the neurons produced spikes by only about half the amount expected. Given that these neurons do not receive inputs from any other cells, this ability to withstand changes in temperature must be intrinsic to the neurons themselves. Consistent with this, computational modeling showed that while the activity of individual ion channels did indeed vary with changes in temperature, these changes in ion channel activity had little overall effect on the rate at which a neuron produced spikes., Whereas it has previously been assumed that compensation for changes in temperature occurs at the level of networks of neurons, the work of Roemschied et al. reveals that such compensation can occur in individual cells, and that it need not require a lot of energy to be expended., DOI:&#xD;http://dx.doi.org/10.7554/eLife.02078.002</abstract><remote-database-name>PubMed Central</remote-database-name><urls><web-urls><url>https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4012639/</url></web-urls><pdf-urls><url>/home/alec/Zotero/storage/EWEF7WU7/Roemschied et al. - 2014 - Cell-intrinsic mechanisms of temperature compensat.pdf</url></pdf-urls><text-urls><url>https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4012639/</url></text-urls></urls><access-date>2017-12-19 18:19:45</access-date></record><record><database name="MyLibrary">MyLibrary</database><source-app name="Zotero">Zotero</source-app><ref-type name="Journal Article">17</ref-type><contributors><authors><author>Tononi, Giulio</author><author>Sporns, Olaf</author><author>Edelman, Gerald M.</author></authors></contributors><titles><title>Measures of degeneracy and redundancy in biological networks</title><secondary-title>Proceedings of the National Academy of Sciences</secondary-title></titles><periodical><full-title>Proceedings of the National Academy of Sciences</full-title><abbr-1>PNAS</abbr-1></periodical><pages>3257-3262</pages><volume>96</volume><number>6</number><issue>6</issue><dates><year>1999</year><pub-dates><date>1999</date></pub-dates></dates><isbn>0027-8424, 1091-6490</isbn><electronic-resource-num>10.1073/pnas.96.6.3257</electronic-resource-num><abstract>Degeneracy, the ability of elements that are structurally different to perform the same function, is a prominent property of many biological systems ranging from genes to neural networks to evolution itself. Because structurally different elements may produce different outputs in different contexts, degeneracy should be distinguished from redundancy, which occurs when the same function is performed by identical elements. However, because of ambiguities in the distinction between structure and function and because of the lack of a theoretical treatment, these two notions often are conflated. By using information theoretical concepts, we develop here functional measures of the degeneracy and redundancy of a system with respect to a set of outputs. These measures help to distinguish the concept of degeneracy from that of redundancy and make it operationally useful. Through computer simulations of neural systems differing in connectivity, we show that degeneracy is low both for systems in which each element affects the output independently and for redundant systems in which many elements can affect the output in a similar way but do not have independent effects. By contrast, degeneracy is high for systems in which many different elements can affect the output in a similar way and at the same time can have independent effects. We demonstrate that networks that have been selected for degeneracy have high values of complexity, a measure of the average mutual information between the subsets of a system. These measures promise to be useful in characterizing and understanding the functional robustness and adaptability of biological networks.</abstract><remote-database-name>www.pnas.org</remote-database-name><language>en</language><urls><web-urls><url>http://www.pnas.org/content/96/6/3257</url></web-urls><pdf-urls><url>/home/alec/Zotero/storage/BNMF5VTP/Tononi et al. - 1999 - Measures of degeneracy and redundancy in biologica.pdf</url></pdf-urls><text-urls><url>http://www.ncbi.nlm.nih.gov/pubmed/10077671</url><url>/home/alec/Zotero/storage/IDRJPC45/3257.html</url></text-urls></urls><access-date>2017-11-15 17:23:03</access-date></record><record><database name="MyLibrary">MyLibrary</database><source-app name="Zotero">Zotero</source-app><ref-type name="Journal Article">17</ref-type><contributors><authors><author>Whitacre, James</author><author>Bender, Axel</author></authors></contributors><titles><title>Degeneracy: A design principle for achieving robustness and evolvability</title><secondary-title>Journal of Theoretical Biology</secondary-title><short-title>Degeneracy</short-title></titles><periodical><full-title>Journal of Theoretical Biology</full-title><abbr-1>Journal of Theoretical Biology</abbr-1></periodical><pages>143-153</pages><volume>263</volume><number>1</number><issue>1</issue><keywords><keyword>Distributed robustness</keyword><keyword>Evolution</keyword><keyword>Fitness landscapes</keyword><keyword>Neutral networks</keyword><keyword>Redundancy</keyword></keywords><dates><year>2010</year><pub-dates><date>March 7, 2010</date></pub-dates></dates><isbn>0022-5193</isbn><electronic-resource-num>10.1016/j.jtbi.2009.11.008</electronic-resource-num><abstract>Robustness, the insensitivity of some of a biological system's functionalities to a set of distinct conditions, is intimately linked to fitness. Recent studies suggest that it may also play a vital role in enabling the evolution of species. Increasing robustness, so is proposed, can lead to the emergence of evolvability if evolution proceeds over a neutral network that extends far throughout the fitness landscape. Here, we show that the design principles used to achieve robustness dramatically influence whether robustness leads to evolvability. In simulation experiments, we find that purely redundant systems have remarkably low evolvability while degenerate, i.e. partially redundant, systems tend to be orders of magnitude more evolvable. Surprisingly, the magnitude of observed variation in evolvability can neither be explained by differences in the size nor the topology of the neutral networks. This suggests that degeneracy, a ubiquitous characteristic in biological systems, may be an important enabler of natural evolution. More generally, our study provides valuable new clues about the origin of innovations in complex adaptive systems.</abstract><remote-database-name>ScienceDirect</remote-database-name><urls><web-urls><url>http://www.sciencedirect.com/science/article/pii/S0022519309005347</url></web-urls><pdf-urls><url>/home/alec/Zotero/storage/GAVP5BMQ/Whitacre and Bender - 2010 - Degeneracy A design principle for achieving robus.pdf</url></pdf-urls><text-urls><url>/home/alec/Zotero/storage/BYN2S8NT/S0022519309005347.html</url></text-urls></urls><access-date>2017-12-05 02:29:26</access-date></record><record><database name="MyLibrary">MyLibrary</database><source-app name="Zotero">Zotero</source-app><ref-type name="Journal Article">17</ref-type><contributors><authors><author>Kitano, Hiroaki</author></authors></contributors><titles><title>Biological robustness</title><secondary-title>Nature Reviews Genetics</secondary-title></titles><periodical><full-title>Nature Reviews Genetics</full-title></periodical><pages>826</pages><volume>5</volume><number>11</number><issue>11</issue><dates><year>2004</year><pub-dates><date>2004/11</date></pub-dates></dates><isbn>1471-0064</isbn><electronic-resource-num>10.1038/nrg1471</electronic-resource-num><abstract><style face="normal">&#xD;&#xD;Robustness is a ubiquitous feature of biological systems. It ensures that specific functions of the system are maintained despite external and internal perturbations. System control, alternative (or fail-safe) mechanisms, modularity and decoupling are the underlying mechanisms that produce robustness.&#xD;&#xD;Robustness facilitates the evolvability of complex dynamic systems. Evolution, given enough time, might select a robust trait that is tolerant against environmental perturbations. This interlinks the properties of robustness and evolvability. Robustness is ubiquitous in biological systems that have evolved.&#xD;&#xD;There are specific architectural requirements for robust and evolvable systems — genetic buffering, robust modules and bow-tie architecture. These architectural requirements are the basis for the system's robustness against environmental perturbations, but congruent with genetic perturbations; they facilitate generation of a flexible phenotype.&#xD;&#xD;Systems that are robust involve intrinsic trade-offs. Enhanced robustness against certain perturbations has to be balanced by extreme fragility elsewhere. This robust yet fragile nature, predicted by the highly optimized tolerance (HOT) theory, is a fundamental property of the system that has been optimally designed or has evolved to cope with perturbations. There are also other trade-offs in the system's performance and resource demands.&#xD;&#xD;Diseases can be thought of in terms of the exposed fragility of robust yet fragile systems. The design of effective countermeasures requires proper understanding of a system's behavioural and failure patterns. </style><style face="italic">Diabetes mellitus</style><style face="normal">, cancer and HIV infection represent the typical failure of such a system that requires systematic countermeasures to control robustness of an epidemic state. Countermeasures include systematic intervention to control a system's dynamics, attack fragility or introduce decoys to re-establish control.&#xD;&#xD;Developing a theory of biological robustness with a solid mathematical foundation that can realistically represent biological systems is a difficult challenge. Research into non-linear dynamics, control theory and non-equilibrium theory is urgently required, but it has to be careful to capture the essential structural complexity and heterogeneity of biological systems.&#xD;&#xD;&lt;/ul&gt;</style></abstract><remote-database-name>www.nature.com</remote-database-name><language>En</language><urls><web-urls><url>https://www.nature.com/articles/nrg1471</url></web-urls><pdf-urls><url>/home/alec/Zotero/storage/QT97EEBT/Kitano - 2004 - Biological robustness.pdf</url></pdf-urls><text-urls><url>/home/alec/Zotero/storage/HV49WRXW/nrg1471.html</url></text-urls></urls><access-date>2017-12-05 02:29:16</access-date></record><record><database name="MyLibrary">MyLibrary</database><source-app name="Zotero">Zotero</source-app><ref-type name="Journal Article">17</ref-type><contributors><authors><author>Stelling, Jörg</author><author>Sauer, Uwe</author><author>Szallasi, Zoltan</author><author>Doyle, Francis J.</author><author>Doyle, John</author></authors></contributors><titles><title>Robustness of Cellular Functions</title><secondary-title>Cell</secondary-title></titles><periodical><full-title>Cell</full-title><abbr-1>Cell</abbr-1></periodical><pages>675-685</pages><volume>118</volume><number>6</number><issue>6</issue><dates><year>2004</year><pub-dates><date>September 17, 2004</date></pub-dates></dates><isbn>0092-8674</isbn><electronic-resource-num>10.1016/j.cell.2004.09.008</electronic-resource-num><abstract>Robustness, the ability to maintain performance in the face of perturbations and uncertainty, is a long-recognized key property of living systems. Owing to intimate links to cellular complexity, however, its molecular and cellular basis has only recently begun to be understood. Theoretical approaches to complex engineered systems can provide guidelines for investigating cellular robustness because biology and engineering employ a common set of basic mechanisms in different combinations. Robustness may be a key to understanding cellular complexity, elucidating design principles, and fostering closer interactions between experimentation and theory.</abstract><remote-database-name>ScienceDirect</remote-database-name><urls><web-urls><url>http://www.sciencedirect.com/science/article/pii/S0092867404008402</url></web-urls><pdf-urls><url>/home/alec/Zotero/storage/3MPQ7FVW/Stelling et al. - 2004 - Robustness of Cellular Functions.pdf</url></pdf-urls><text-urls><url>/home/alec/Zotero/storage/BBCVWDBJ/S0092867404008402.html</url></text-urls></urls><access-date>2017-12-05 02:29:10</access-date></record><record><database name="MyLibrary">MyLibrary</database><source-app name="Zotero">Zotero</source-app><ref-type name="Journal Article">17</ref-type><contributors><authors><author>Prinz, Astrid A.</author><author>Bucher, Dirk</author><author>Marder, Eve</author></authors></contributors><titles><title>Similar network activity from disparate circuit parameters</title><secondary-title>Nature Neuroscience</secondary-title></titles><periodical><full-title>Nature Neuroscience</full-title><abbr-1>Nat. Neurosci.</abbr-1></periodical><pages>1345-1352</pages><volume>7</volume><number>12</number><issue>12</issue><keywords><keyword>Animals</keyword><keyword>Crustacea</keyword><keyword>Nerve Net</keyword><keyword>Neurons</keyword><keyword>Action Potentials</keyword><keyword>Synapses</keyword></keywords><dates><year>2004</year><pub-dates><date>Dec 2004</date></pub-dates></dates><isbn>1097-6256</isbn><electronic-resource-num>10.1038/nn1352</electronic-resource-num><abstract>It is often assumed that cellular and synaptic properties need to be regulated to specific values to allow a neuronal network to function properly. To determine how tightly neuronal properties and synaptic strengths need to be tuned to produce a given network output, we simulated more than 20 million versions of a three-cell model of the pyloric network of the crustacean stomatogastric ganglion using different combinations of synapse strengths and neuron properties. We found that virtually indistinguishable network activity can arise from widely disparate sets of underlying mechanisms, suggesting that there could be considerable animal-to-animal variability in many of the parameters that control network activity, and that many different combinations of synaptic strengths and intrinsic membrane properties can be consistent with appropriate network performance.</abstract><remote-database-name>PubMed</remote-database-name><language>eng</language><urls><pdf-urls><url>/home/alec/Zotero/storage/3D6QHNAW/Prinz et al. - 2004 - Similar network activity from disparate circuit pa.pdf</url></pdf-urls><text-urls><url>http://www.ncbi.nlm.nih.gov/pubmed/15558066</url><url>/home/alec/Zotero/storage/NNCABTBJ/nn1352.html</url><url>/home/alec/Zotero/storage/TJNVS6RE/nn1352.html</url></text-urls></urls></record><record><database name="MyLibrary">MyLibrary</database><source-app name="Zotero">Zotero</source-app><ref-type name="Journal Article">17</ref-type><contributors><authors><author>Bucher, Dirk</author><author>Prinz, Astrid A.</author><author>Marder, Eve</author></authors></contributors><titles><title>Animal-to-animal variability in motor pattern production in adults and during growth</title><secondary-title>The Journal of Neuroscience: The Official Journal of the Society for Neuroscience</secondary-title></titles><periodical><full-title>The Journal of Neuroscience: The Official Journal of the Society for Neuroscience</full-title><abbr-1>J. Neurosci.</abbr-1></periodical><pages>1611-1619</pages><volume>25</volume><number>7</number><issue>7</issue><keywords><keyword>Animals</keyword><keyword>Ganglia, Invertebrate</keyword><keyword>Nerve Net</keyword><keyword>Membrane Potentials</keyword><keyword>Motor Neurons</keyword><keyword>Nephropidae</keyword><keyword>Pylorus</keyword><keyword>Motor Activity</keyword><keyword>Individuality</keyword><keyword>Reaction Time</keyword></keywords><dates><year>2005</year><pub-dates><date>Feb 16, 2005</date></pub-dates></dates><isbn>1529-2401</isbn><electronic-resource-num>10.1523/JNEUROSCI.3679-04.2005</electronic-resource-num><abstract>Which features of network output are well preserved during growth of the nervous system and across different preparations of the same size? To address this issue, we characterized the pyloric rhythms generated by the stomatogastric nervous systems of 99 adult and 12 juvenile lobsters (Homarus americanus). Anatomical studies of single pyloric network neurons and of the whole stomatogastric ganglion (STG) showed that the STG and its neurons grow considerably from juvenile to adult. Despite these changes in size, intracellularly recorded membrane potential waveforms of pyloric network neurons and the phase relationships in the pyloric rhythm were very similar between juvenile and adult preparations. Across adult preparations, the cycle period and number of spikes per burst were not tightly maintained, but the mean phase relationships were independent of the period of the rhythm and relatively tightly maintained across preparations. We interpret this as evidence for homeostatic regulation of network activity.</abstract><remote-database-name>PubMed</remote-database-name><language>eng</language><urls><text-urls><url>http://www.ncbi.nlm.nih.gov/pubmed/15716396</url></text-urls></urls></record><record><database name="MyLibrary">MyLibrary</database><source-app name="Zotero">Zotero</source-app><ref-type name="Journal Article">17</ref-type><contributors><authors><author>Haddad, Sara A.</author><author>Marder, Eve</author></authors></contributors><titles><title>Circuit robustness to temperature perturbation is altered by neuromodulators</title><secondary-title>bioRxiv</secondary-title></titles><periodical><full-title>bioRxiv</full-title></periodical><pages>178764</pages><dates><year>2017</year><pub-dates><date>2017-08-21</date></pub-dates></dates><electronic-resource-num>10.1101/178764</electronic-resource-num><abstract>In the ocean, the crab, Cancer borealis, is subject to daily and seasonal temperature changes. Previous work, done in the presence of descending modulatory inputs, had shown that the pyloric rhythm of the crab increases in frequency as temperature increases, but maintains its characteristic phase relationships until it 'crashes' at extreme high temperatures. To study the interaction between neuromodulators and temperature perturbations, we studied the effects of temperature on preparations from which the descending modulatory inputs were removed. Under these conditions the pyloric rhythm was destabilized. We then studied the effects of temperature on preparations in the presence of oxotremorine, proctolin, and serotonin. Oxotremorine and proctolin enhanced the robustness of the pyloric rhythm, while serotonin made the rhythm less robust. These experiments reveal considerable animal-to-animal diversity in their crash stability, consistent with the interpretation that cryptic differences in many cell and network parameters are revealed by extreme perturbations.</abstract><remote-database-name>www.biorxiv.org</remote-database-name><language>en</language><urls><web-urls><url>https://www.biorxiv.org/content/early/2017/08/21/178764</url></web-urls><pdf-urls><url>/home/alec/Zotero/storage/AZYIP2V4/Haddad and Marder - 2017 - Circuit robustness to temperature perturbation is .pdf</url></pdf-urls><text-urls><url>/home/alec/Zotero/storage/GYQTSDP9/178764.html</url></text-urls></urls><access-date>2017-11-15 22:37:30</access-date></record><record><database name="MyLibrary">MyLibrary</database><source-app name="Zotero">Zotero</source-app><ref-type name="Journal Article">17</ref-type><contributors><authors><author>Gorur-Shandilya, Srinivas</author><author>Hoyland, Alec</author><author>Marder, Eve</author></authors></contributors><titles><title>Xolotl: An Intuitive and Approachable Neuron and Network Simulator for Research and Teaching</title><secondary-title>Frontiers in Neuroinformatics</secondary-title><short-title>Xolotl</short-title></titles><periodical><full-title>Frontiers in Neuroinformatics</full-title><abbr-1>Front. Neuroinform.</abbr-1></periodical><volume>12</volume><keywords><keyword>Software</keyword><keyword>code:C++</keyword><keyword>code:Matlab.</keyword><keyword>Conductance-based</keyword><keyword>Hodgkin-Huxley</keyword></keywords><dates><year>2018</year><pub-dates><date>2018</date></pub-dates></dates><isbn>1662-5196</isbn><electronic-resource-num>10.3389/fninf.2018.00087</electronic-resource-num><abstract>Conductance-based models of neurons are used extensively in computational neuroscience. Working with these models can be challenging due to their high dimensionality and large number of parameters. Here, we present a neuron and network simulator built on a novel automatic type system that binds object-oriented code written in C++ to objects in MATLAB. Our approach builds on the tradition of uniting the speed of languages like C++ with the ease-of-use and feature-set of scientific programming languages like MATLAB. Xolotl allows for the creation and manipulation of hierarchical models with components that are named and searchable, permitting intuitive high-level programmatic control over all parts of the model. The simulator's architecture allows for the interactive manipulation of any parameter in any model, and for visualizing the effects of changing that parameter immediately. Xolotl is fully featured with hundreds of ion channel models from the electrophysiological literature, and can be extended to include arbitrary conductances, synapses, and mechanisms. Several core features like bookmarking of parameters and automatic hashing of source code facilitate reproducible and auditable research. Its ease of use and rich visualization capabilities make it an attractive option in teaching environments. Finally, xolotl is written in a modular fashion, includes detailed tutorials and worked examples, and is freely available at https://github.com/sg-s/xolotl, enabling seamless integration into the workflows of other researchers.</abstract><remote-database-name>Frontiers</remote-database-name><language>English</language><urls><web-urls><url>https://www.frontiersin.org/articles/10.3389/fninf.2018.00087/full</url></web-urls><pdf-urls><url>/home/alec/Zotero/storage/T9CMVVQP/Gorur-Shandilya et al. - 2018 - Xolotl An Intuitive and Approachable Neuron and N.pdf</url><url>/home/alec/Zotero/storage/JSC246Z6/Gorur-Shandilya et al. - 2018 - Xolotl An Intuitive and Approachable Neuron and N.pdf</url></pdf-urls></urls><access-date>2019-05-04 18:59:20</access-date></record><record><database name="MyLibrary">MyLibrary</database><source-app name="Zotero">Zotero</source-app><ref-type name="Journal Article">17</ref-type><contributors><authors><author>Piatkevich, Kiryl D.</author><author>Bensussen, Seth</author><author>Tseng, Hua-an</author><author>Shroff, Sanaya N.</author><author>Lopez-Huerta, Violeta Gisselle</author><author>Park, Demian</author><author>Jung, Erica E.</author><author>Shemesh, Or A.</author><author>Straub, Christoph</author><author>Gritton, Howard J.</author><author>Romano, Michael F.</author><author>Costa, Emma</author><author>Sabatini, Bernardo L.</author><author>Fu, Zhanyan</author><author>Boyden, Edward S.</author><author>Han, Xue</author></authors></contributors><titles><title>Population imaging of neural activity in awake behaving mice in multiple brain regions</title><secondary-title>bioRxiv</secondary-title></titles><periodical><full-title>bioRxiv</full-title></periodical><pages>616094</pages><dates><year>2019</year><pub-dates><date>2019-04-23</date></pub-dates></dates><electronic-resource-num>10.1101/616094</electronic-resource-num><abstract>Abstract&#xD;&#xD;A longstanding goal in neuroscience has been to image membrane voltage, with high temporal precision and sensitivity, in awake behaving mammals. Here, we report a genetically encoded voltage indicator, SomArchon, which exhibits millisecond response times and compatibility with optogenetic control, and which increases the sensitivity, signal-to-noise ratio, and number of neurons observable, by manyfold over previous reagents. SomArchon only requires conventional one-photon microscopy to achieve these high performance characteristics. These improvements enable population analysis of neural activity, both at the subthreshold and spiking levels, in multiple brain regions – cortex, hippocampus, and striatum – of awake behaving mice. Using SomArchon, we detect both positive and negative responses of striatal neurons during movement, highlighting the power of voltage imaging to reveal bidirectional modulation. We also examine how the intracellular subthreshold theta oscillations of hippocampal neurons govern spike output, finding that nearby cells can exhibit highly correlated subthreshold activities, even as they generate highly divergent spiking patterns.</abstract><remote-database-name>www.biorxiv.org</remote-database-name><language>en</language><urls><web-urls><url>https://www.biorxiv.org/content/10.1101/616094v1</url></web-urls><pdf-urls><url>/home/alec/Zotero/storage/IATU5KCA/Piatkevich et al. - 2019 - Population imaging of neural activity in awake beh.pdf</url></pdf-urls><text-urls><url>/home/alec/Zotero/storage/PE9NSV28/616094v1.html</url></text-urls></urls><access-date>2019-06-12 16:00:36</access-date></record><record><database name="MyLibrary">MyLibrary</database><source-app name="Zotero">Zotero</source-app><ref-type name="Journal Article">17</ref-type><contributors><authors><author>Mau, William</author><author>Sullivan, David W.</author><author>Kinsky, Nathaniel R.</author><author>Hasselmo, Michael E.</author><author>Howard, Marc W.</author><author>Eichenbaum, Howard</author></authors></contributors><titles><title>The Same Hippocampal CA1 Population Simultaneously Codes Temporal Information over Multiple Timescales</title><secondary-title>Current Biology</secondary-title></titles><periodical><full-title>Current Biology</full-title><abbr-1>Current Biology</abbr-1></periodical><pages>1499-1508.e4</pages><volume>28</volume><number>10</number><issue>10</issue><keywords><keyword>CA1</keyword><keyword>calcium imaging</keyword><keyword>hippocampus</keyword><keyword>longitudinal recording</keyword><keyword>temporal encoding</keyword></keywords><dates><year>2018</year><pub-dates><date>May 21, 2018</date></pub-dates></dates><isbn>0960-9822</isbn><electronic-resource-num>10.1016/j.cub.2018.03.051</electronic-resource-num><abstract>Summary&#xD;It has long been hypothesized that a primary function of the hippocampus is to discover and exploit temporal relationships between events. Previously, it has been reported that sequences of “time cells” in the hippocampus extend for tens of seconds. Other studies have shown that neuronal firing in the hippocampus fluctuates over hours and days. Both of these mechanisms could enable temporal encoding of events over very different timescales. However, thus far, these two classes of phenomena have never been observed simultaneously, which is necessary to ascribe broad-range temporal coding to the hippocampus. Using in vivo calcium imaging in unrestrained mice, we observed sequences of hippocampal neurons that bridged a 10 s delay. Similar sequences were observed over multiple days, but the set of neurons participating in those sequences changed gradually. Thus, the same population of neurons that encodes temporal information over seconds can also be used to distinguish periods of time over much longer timescales. These results unify two previously separate paradigms of temporal processing in the hippocampus that support episodic memory.</abstract><remote-database-name>ScienceDirect</remote-database-name><urls><web-urls><url>http://www.sciencedirect.com/science/article/pii/S0960982218303804</url></web-urls><pdf-urls><url>/home/alec/Zotero/storage/RFZIQL5I/Mau et al. - 2018 - The Same Hippocampal CA1 Population Simultaneously.pdf</url></pdf-urls><text-urls><url>/home/alec/Zotero/storage/FT6BGM39/S0960982218303804.html</url></text-urls></urls><access-date>2019-06-12 16:09:55</access-date></record><record><database name="MyLibrary">MyLibrary</database><source-app name="Zotero">Zotero</source-app><ref-type name="Journal Article">17</ref-type><contributors><authors><author>Adam, Yoav</author><author>Kim, Jeong J.</author><author>Lou, Shan</author><author>Zhao, Yongxin</author><author>Xie, Michael E.</author><author>Brinks, Daan</author><author>Wu, Hao</author><author>Mostajo-Radji, Mohammed A.</author><author>Kheifets, Simon</author><author>Parot, Vicente</author><author>Chettih, Selmaan</author><author>Williams, Katherine J.</author><author>Gmeiner, Benjamin</author><author>Farhi, Samouil L.</author><author>Madisen, Linda</author><author>Buchanan, E. Kelly</author><author>Kinsella, Ian</author><author>Zhou, Ding</author><author>Paninski, Liam</author><author>Harvey, Christopher D.</author><author>Zeng, Hongkui</author><author>Arlotta, Paola</author><author>Campbell, Robert E.</author><author>Cohen, Adam E.</author></authors></contributors><titles><title>Voltage imaging and optogenetics reveal behaviour-dependent changes in hippocampal dynamics</title><secondary-title>Nature</secondary-title></titles><periodical><full-title>Nature</full-title></periodical><pages>413</pages><volume>569</volume><number>7756</number><issue>7756</issue><dates><year>2019</year><pub-dates><date>2019/05</date></pub-dates></dates><isbn>1476-4687</isbn><electronic-resource-num>10.1038/s41586-019-1166-7</electronic-resource-num><abstract>A combination of improved near-infrared voltage indicators, high-speed microscopes and targeted gene expression schemes enabled simultaneous in vivo optogenetic control and&amp;nbsp;recording of voltage dynamics in multiple neurons in the hippocampus of behaving mice.</abstract><remote-database-name>www.nature.com</remote-database-name><language>En</language><urls><web-urls><url>https://www.nature.com/articles/s41586-019-1166-7</url></web-urls><pdf-urls><url>/home/alec/Zotero/storage/RPKBID35/Adam et al. - 2019 - Voltage imaging and optogenetics reveal behaviour-.pdf</url></pdf-urls><text-urls><url>/home/alec/Zotero/storage/CMSYFVRJ/s41586-019-1166-7.html</url></text-urls></urls><access-date>2019-06-12 16:47:09</access-date></record><record><database name="MyLibrary">MyLibrary</database><source-app name="Zotero">Zotero</source-app><ref-type name="Conference Proceedings">10</ref-type><contributors><authors><author>Kennedy, J.</author><author>Eberhart, R.</author></authors></contributors><titles><title>Particle swarm optimization</title><secondary-title>Proceedings of ICNN'95 - International Conference on Neural Networks</secondary-title></titles><periodical><full-title>Proceedings of ICNN'95 - International Conference on Neural Networks</full-title></periodical><pages>1942-1948 vol.4</pages><volume>4</volume><keywords><keyword>Humans</keyword><keyword>Testing</keyword><keyword>simulation</keyword><keyword>artificial intelligence</keyword><keyword>artificial life</keyword><keyword>Artificial neural networks</keyword><keyword>Birds</keyword><keyword>Educational institutions</keyword><keyword>evolution</keyword><keyword>genetic algorithms</keyword><keyword>Genetic algorithms</keyword><keyword>Marine animals</keyword><keyword>multidimensional search</keyword><keyword>neural nets</keyword><keyword>neural network</keyword><keyword>nonlinear functions</keyword><keyword>optimization</keyword><keyword>Optimization methods</keyword><keyword>particle swarm</keyword><keyword>Particle swarm optimization</keyword><keyword>Performance evaluation</keyword><keyword>search problems</keyword><keyword>social metaphor</keyword></keywords><dates><year>1995</year><pub-dates><date>November 1995</date></pub-dates></dates><electronic-resource-num>10.1109/ICNN.1995.488968</electronic-resource-num><abstract>A concept for the optimization of nonlinear functions using particle swarm methodology is introduced. The evolution of several paradigms is outlined, and an implementation of one of the paradigms is discussed. Benchmark testing of the paradigm is described, and applications, including nonlinear function optimization and neural network training, are proposed. The relationships between particle swarm optimization and both artificial life and genetic algorithms are described.</abstract><remote-database-name>IEEE Xplore</remote-database-name><urls><text-urls><url>/home/alec/Zotero/storage/87XU3B47/488968.html</url></text-urls></urls><custom3>Proceedings of ICNN'95 - International Conference on Neural Networks</custom3></record><record><database name="MyLibrary">MyLibrary</database><source-app name="Zotero">Zotero</source-app><ref-type name="Book">6</ref-type><contributors><authors><author>Mitchell, Melanie</author></authors></contributors><titles><title>An introduction to genetic algorithms</title><secondary-title>Complex adaptive systems</secondary-title></titles><pages>209</pages><edition>7. print</edition><dates><year>2001</year><pub-dates><date>2001</date></pub-dates></dates><pub-location>Cambridge, Mass.</pub-location><isbn>978-0-262-63185-3 978-0-262-13316-6</isbn><remote-database-name>Gemeinsamer Bibliotheksverbund ISBN</remote-database-name></record><record><database name="MyLibrary">MyLibrary</database><source-app name="Zotero">Zotero</source-app><ref-type name="Journal Article">17</ref-type><contributors><authors><author>Hooke, Robert</author><author>Jeeves, T. A.</author></authors></contributors><titles><title>“ Direct Search” Solution of Numerical and Statistical Problems</title><secondary-title>J. ACM</secondary-title></titles><periodical><full-title>J. ACM</full-title></periodical><pages>212–229</pages><volume>8</volume><number>2</number><issue>2</issue><dates><year>1961</year><pub-dates><date>April 1961</date></pub-dates></dates><isbn>0004-5411</isbn><electronic-resource-num>10.1145/321062.321069</electronic-resource-num><remote-database-name>ACM Digital Library</remote-database-name><urls><web-urls><url>http://doi.acm.org/10.1145/321062.321069</url></web-urls></urls><access-date>2019-07-21 17:25:13</access-date></record><record><database name="MyLibrary">MyLibrary</database><source-app name="Zotero">Zotero</source-app><ref-type name="Thesis">32</ref-type><contributors><authors><author>Hoyland, Alec</author></authors></contributors><titles><title>Differential Responses to Neuromodulation in Model Neurons of the Crustacean Stomatogastric Ganglion</title></titles><dates><year>2018</year><pub-dates><date>2018</date></pub-dates></dates><publisher>Brandeis University</publisher><abstract>Neuronal networks must produce stable circuit output for sustained periods of time &#xD;despite environmental perturbation. In addition, they must be sensitive to key &#xD;endogenous signaling to produce differing output. The STG manages these &#xD;competing objectives while remaining degenerate to ion channel density. &#xD;Neuromodulators can produce a diverse set of network states using the same cellular &#xD;and synaptic morphology. In particular to the STG, the dense, tangled neuropil and &#xD;gradations in reversal potential render neurons isopotential with respect to the &#xD;somata. Neuromodulators, then, play the role of maintaining and switching network &#xD;activity. For stable and responsive biological activity, degenerate networks must still &#xD;be robust to environmental perturbation and responsive to intentional modulation. &#xD;In this thesis, I describe red pigment-concentrating hormone (RPCH) acting as a &#xD;neuromodulator on a computational model of a rhythmic motor circuit.</abstract><work-type>Thesis</work-type><remote-database-name>bir.brandeis.edu</remote-database-name><language>eng</language><urls><web-urls><url>http://bir.brandeis.edu/handle/10192/35686</url></web-urls><pdf-urls><url>/home/alec/Zotero/storage/PHP64TQ7/Hoyland - 2018 - Differential Responses to Neuromodulation in Model.pdf</url><url>/home/alec/Zotero/storage/SKGLS35W/Hoyland - 2018 - Differential Responses to Neuromodulation in Model.pdf</url></pdf-urls><text-urls><url>/home/alec/Zotero/storage/THX69LQK/35686.html</url><url>/home/alec/Zotero/storage/XBFNU6QM/35686.html</url></text-urls></urls><access-date>2019-08-14 15:09:10</access-date><misc2>Thesis</misc2></record><record><database name="MyLibrary">MyLibrary</database><source-app name="Zotero">Zotero</source-app><ref-type name="Journal Article">17</ref-type><contributors><authors><author>Traub, Roger D.</author><author>Contreras, Diego</author><author>Cunningham, Mark O.</author><author>Murray, Hilary</author><author>LeBeau, Fiona E. N.</author><author>Roopun, Anita</author><author>Bibbig, Andrea</author><author>Wilent, W. Bryan</author><author>Higley, Michael J.</author><author>Whittington, Miles A.</author></authors></contributors><titles><title>Single-column thalamocortical network model exhibiting gamma oscillations, sleep spindles, and epileptogenic bursts</title><secondary-title>Journal of Neurophysiology</secondary-title></titles><periodical><full-title>Journal of Neurophysiology</full-title><abbr-1>J. Neurophysiol.</abbr-1></periodical><pages>2194-2232</pages><volume>93</volume><number>4</number><issue>4</issue><keywords><keyword>Animals</keyword><keyword>Models, Neurological</keyword><keyword>Nerve Net</keyword><keyword>Action Potentials</keyword><keyword>Cerebral Cortex</keyword><keyword>Rats</keyword><keyword>Biological Clocks</keyword><keyword>Male</keyword><keyword>Rats, Wistar</keyword><keyword>Rats, Sprague-Dawley</keyword><keyword>Thalamus</keyword><keyword>Epilepsy</keyword><keyword>Sleep</keyword></keywords><dates><year>2005</year><pub-dates><date>Apr 2005</date></pub-dates></dates><isbn>0022-3077</isbn><electronic-resource-num>10.1152/jn.00983.2004</electronic-resource-num><abstract>To better understand population phenomena in thalamocortical neuronal ensembles, we have constructed a preliminary network model with 3,560 multicompartment neurons (containing soma, branching dendrites, and a portion of axon). Types of neurons included superficial pyramids (with regular spiking [RS] and fast rhythmic bursting [FRB] firing behaviors); RS spiny stellates; fast spiking (FS) interneurons, with basket-type and axoaxonic types of connectivity, and located in superficial and deep cortical layers; low threshold spiking (LTS) interneurons, which contacted principal cell dendrites; deep pyramids, which could have RS or intrinsic bursting (IB) firing behaviors, and endowed either with nontufted apical dendrites or with long tufted apical dendrites; thalamocortical relay (TCR) cells; and nucleus reticularis (nRT) cells. To the extent possible, both electrophysiology and synaptic connectivity were based on published data, although many arbitrary choices were necessary. In addition to synaptic connectivity (by AMPA/kainate, NMDA, and GABA(A) receptors), we also included electrical coupling between dendrites of interneurons, nRT cells, and TCR cells, and--in various combinations--electrical coupling between the proximal axons of certain cortical principal neurons. Our network model replicates several observed population phenomena, including 1) persistent gamma oscillations; 2) thalamocortical sleep spindles; 3) series of synchronized population bursts, resembling electrographic seizures; 4) isolated double population bursts with superimposed very fast oscillations (&gt;100 Hz, "VFO"); 5) spike-wave, polyspike-wave, and fast runs (about 10 Hz). We show that epileptiform bursts, including double and multiple bursts, containing VFO occur in rat auditory cortex in vitro, in the presence of kainate, when both GABA(A) and GABA(B) receptors are blocked. Electrical coupling between axons appears necessary (as reported previously) for persistent gamma and additionally plays a role in the detailed shaping of epileptogenic events. The degree of recurrent synaptic excitation between spiny stellate cells, and their tendency to fire throughout multiple bursts, also appears critical in shaping epileptogenic events.</abstract><remote-database-name>PubMed</remote-database-name><language>eng</language><urls><pdf-urls><url>/home/alec/Zotero/storage/XZSC7YL3/Traub et al. - 2005 - Single-column thalamocortical network model exhibi.pdf</url></pdf-urls><text-urls><url>http://www.ncbi.nlm.nih.gov/pubmed/15525801</url></text-urls></urls></record><record><database name="MyLibrary">MyLibrary</database><source-app name="Zotero">Zotero</source-app><ref-type name="Journal Article">17</ref-type><contributors><authors><author>Traub, Roger D.</author><author>Buhl, Eberhard H.</author><author>Gloveli, Tengis</author><author>Whittington, Miles A.</author></authors></contributors><titles><title>Fast rhythmic bursting can be induced in layer 2/3 cortical neurons by enhancing persistent Na+ conductance or by blocking BK channels</title><secondary-title>Journal of Neurophysiology</secondary-title></titles><periodical><full-title>Journal of Neurophysiology</full-title><abbr-1>J. Neurophysiol.</abbr-1></periodical><pages>909-921</pages><volume>89</volume><number>2</number><issue>2</issue><keywords><keyword>Animals</keyword><keyword>Models, Neurological</keyword><keyword>Action Potentials</keyword><keyword>Axons</keyword><keyword>Cerebral Cortex</keyword><keyword>Periodicity</keyword><keyword>Rats</keyword><keyword>Sodium Channels</keyword><keyword>Calcium</keyword><keyword>Organ Culture Techniques</keyword><keyword>Male</keyword><keyword>Pyramidal Cells</keyword><keyword>Rats, Wistar</keyword><keyword>Sodium</keyword><keyword>Large-Conductance Calcium-Activated Potassium Channels</keyword><keyword>Nitric Oxide Donors</keyword><keyword>Penicillamine</keyword><keyword>Potassium Channels, Calcium-Activated</keyword></keywords><dates><year>2003</year><pub-dates><date>Feb 2003</date></pub-dates></dates><isbn>0022-3077</isbn><electronic-resource-num>10.1152/jn.00573.2002</electronic-resource-num><abstract>Fast rhythmic bursting (or "chattering") is a firing pattern exhibited by selected neocortical neurons in cats in vivo and in slices of adult ferret and cat brain. Fast rhythmic bursting (FRB) has been recorded in certain superficial and deep principal neurons and in aspiny presumed local circuit neurons; it can be evoked by depolarizing currents or by sensory stimulation and has been proposed to depend on a persistent g(Na) that causes spike depolarizing afterpotentials. We constructed a multicompartment 11-conductance model of a layer 2/3 pyramidal neuron, containing apical dendritic calcium-mediated electrogenesis; the model can switch between rhythmic spiking (RS) and FRB modes of firing, with various parameter changes. FRB in this model is favored by enhancing persistent g(Na) and also by measures that reduce [Ca(2+)](i) or that reduce the conductance of g(K(C)) (a fast voltage- and Ca(2+)-dependent conductance). Axonal excitability plays a critical role in generating fast bursts in the model. In vitro experiments in rat layer 2/3 neurons confirmed (as shown previously by others) that RS firing could be switched to fast rhythmic bursting, either by buffering [Ca(2+)](i) or by enhancing persistent g(Na). In addition, our experiments confirmed the model prediction that reducing g(KC) (with iberiotoxin) would favor FRB. During the bursts, fast prepotentials (spikelets) could occur that did not originate in apical dendrites and that appear to derive from the axon. We suggest that modulator-induced regulation of [Ca(2+)] dynamics or of BK channel conductance, for example via protein kinase A, could play a role in determining the firing pattern of neocortical neurons; specifically, such modulation could play a role in regulating whether neurons respond to strong stimulation with fast rhythmic bursts.</abstract><remote-database-name>PubMed</remote-database-name><language>eng</language><urls><pdf-urls><url>/home/alec/Zotero/storage/UBC35TY6/Traub et al. - 2003 - Fast rhythmic bursting can be induced in layer 23.pdf</url></pdf-urls><text-urls><url>http://www.ncbi.nlm.nih.gov/pubmed/12574468</url></text-urls></urls></record><record><database name="MyLibrary">MyLibrary</database><source-app name="Zotero">Zotero</source-app><ref-type name="Journal Article">17</ref-type><contributors><authors><author>Polsky, Alon</author><author>Mel, Bartlett W.</author><author>Schiller, Jackie</author></authors></contributors><titles><title>Computational subunits in thin dendrites of pyramidal cells</title><secondary-title>Nature Neuroscience</secondary-title></titles><periodical><full-title>Nature Neuroscience</full-title><abbr-1>Nat. Neurosci.</abbr-1></periodical><pages>621-627</pages><volume>7</volume><number>6</number><issue>6</issue><keywords><keyword>Animals</keyword><keyword>Nerve Net</keyword><keyword>Dendrites</keyword><keyword>Rats</keyword><keyword>Pyramidal Cells</keyword><keyword>Rats, Wistar</keyword><keyword>Excitatory Postsynaptic Potentials</keyword><keyword>Neocortex</keyword></keywords><dates><year>2004</year><pub-dates><date>Jun 2004</date></pub-dates></dates><isbn>1097-6256</isbn><electronic-resource-num>10.1038/nn1253</electronic-resource-num><abstract>The thin basal and oblique dendrites of cortical pyramidal neurons receive most of the synaptic inputs from other cells, but their integrative properties remain uncertain. Previous studies have most often reported global linear or sublinear summation. An alternative view, supported by biophysical modeling studies, holds that thin dendrites provide a layer of independent computational 'subunits' that sigmoidally modulate their inputs prior to global summation. To distinguish these possibilities, we combined confocal imaging and dual-site focal synaptic stimulation of identified thin dendrites in rat neocortical pyramidal neurons. We found that nearby inputs on the same branch summed sigmoidally, whereas widely separated inputs or inputs to different branches summed linearly. This strong spatial compartmentalization effect is incompatible with a global summation rule and provides the first experimental support for a two-layer 'neural network' model of pyramidal neuron thin-branch integration. Our findings could have important implications for the computing and memory-related functions of cortical tissue.</abstract><remote-database-name>PubMed</remote-database-name><language>eng</language><urls><text-urls><url>http://www.ncbi.nlm.nih.gov/pubmed/15156147</url></text-urls></urls></record><record><database name="MyLibrary">MyLibrary</database><source-app name="Zotero">Zotero</source-app><ref-type name="Journal Article">17</ref-type><contributors><authors><author>Poirazi, Panayiota</author><author>Brannon, Terrence</author><author>Mel, Bartlett W.</author></authors></contributors><titles><title>Pyramidal neuron as two-layer neural network</title><secondary-title>Neuron</secondary-title></titles><periodical><full-title>Neuron</full-title><abbr-1>Neuron</abbr-1></periodical><pages>989-999</pages><volume>37</volume><number>6</number><issue>6</issue><keywords><keyword>Nerve Net</keyword><keyword>Synapses</keyword><keyword>Dendrites</keyword><keyword>Potassium Channels</keyword><keyword>Electrophysiology</keyword><keyword>Mathematics</keyword><keyword>Biophysics</keyword><keyword>Electric Conductivity</keyword><keyword>Sodium Channels</keyword><keyword>Biophysical Phenomena</keyword><keyword>Models, Biological</keyword><keyword>Calcium</keyword><keyword>Pyramidal Cells</keyword></keywords><dates><year>2003</year><pub-dates><date>Mar 27, 2003</date></pub-dates></dates><isbn>0896-6273</isbn><electronic-resource-num>10.1016/s0896-6273(03)00149-1</electronic-resource-num><abstract>The pyramidal neuron is the principal cell type in the mammalian forebrain, but its function remains poorly understood. Using a detailed compartmental model of a hippocampal CA1 pyramidal cell, we recorded responses to complex stimuli consisting of dozens of high-frequency activated synapses distributed throughout the apical dendrites. We found the cell's firing rate could be predicted by a simple formula that maps the physical components of the cell onto those of an abstract two-layer "neural network." In the first layer, synaptic inputs drive independent sigmoidal subunits corresponding to the cell's several dozen long, thin terminal dendrites. The subunit outputs are then summed within the main trunk and cell body prior to final thresholding. We conclude that insofar as the neural code is mediated by average firing rate, a two-layer neural network may provide a useful abstraction for the computing function of the individual pyramidal neuron.</abstract><remote-database-name>PubMed</remote-database-name><language>eng</language><urls><text-urls><url>http://www.ncbi.nlm.nih.gov/pubmed/12670427</url></text-urls></urls></record><record><database name="MyLibrary">MyLibrary</database><source-app name="Zotero">Zotero</source-app><ref-type name="Journal Article">17</ref-type><contributors><authors><author>Izhikevich, E. M.</author></authors></contributors><titles><title>Simple model of spiking neurons</title><secondary-title>IEEE transactions on neural networks</secondary-title></titles><periodical><full-title>IEEE transactions on neural networks</full-title><abbr-1>IEEE Trans Neural Netw</abbr-1></periodical><pages>1569-1572</pages><volume>14</volume><number>6</number><issue>6</issue><dates><year>2003</year><pub-dates><date>2003</date></pub-dates></dates><isbn>1045-9227</isbn><electronic-resource-num>10.1109/TNN.2003.820440</electronic-resource-num><abstract>A model is presented that reproduces spiking and bursting behavior of known types of cortical neurons. The model combines the biologically plausibility of Hodgkin-Huxley-type dynamics and the computational efficiency of integrate-and-fire neurons. Using this model, one can simulate tens of thousands of spiking cortical neurons in real time (1 ms resolution) using a desktop PC.</abstract><remote-database-name>PubMed</remote-database-name><language>eng</language><urls><pdf-urls><url>/home/alec/Zotero/storage/XGXJFU7B/Izhikevich - 2003 - Simple model of spiking neurons.pdf</url></pdf-urls><text-urls><url>http://www.ncbi.nlm.nih.gov/pubmed/18244602</url></text-urls></urls></record><record><database name="MyLibrary">MyLibrary</database><source-app name="Zotero">Zotero</source-app><ref-type name="Journal Article">17</ref-type><contributors><authors><author>Rumelhart, David E.</author><author>Hinton, Geoffrey E.</author><author>Williams, Ronald J.</author></authors></contributors><titles><title>Learning representations by back-propagating errors</title><secondary-title>Nature</secondary-title></titles><periodical><full-title>Nature</full-title><abbr-1>Nature</abbr-1></periodical><pages>533-536</pages><volume>323</volume><number>6088</number><issue>6088</issue><dates><year>1986</year><pub-dates><date>1986/10</date></pub-dates></dates><isbn>1476-4687</isbn><electronic-resource-num>10.1038/323533a0</electronic-resource-num><abstract>We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector. As a result of the weight adjustments, internal ‘hidden’ units which are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. The ability to create useful new features distinguishes back-propagation from earlier, simpler methods such as the perceptron-convergence procedure1.</abstract><remote-database-name>www.nature.com</remote-database-name><language>en</language><urls><web-urls><url>https://www.nature.com/articles/323533a0</url></web-urls><pdf-urls><url>/home/alec/Zotero/storage/H2PP9874/Rumelhart et al. - 1986 - Learning representations by back-propagating error.pdf</url></pdf-urls><text-urls><url>/home/alec/Zotero/storage/288I54UV/323533a0.html</url></text-urls></urls><access-date>2019-11-25 16:12:03</access-date></record><record><database name="MyLibrary">MyLibrary</database><source-app name="Zotero">Zotero</source-app><ref-type name="Journal Article">17</ref-type><contributors><authors><author>Destexhe, A.</author><author>Bal, T.</author><author>McCormick, D. A.</author><author>Sejnowski, T. J.</author></authors></contributors><titles><title>Ionic mechanisms underlying synchronized oscillations and propagating waves in a model of ferret thalamic slices</title><secondary-title>Journal of Neurophysiology</secondary-title></titles><periodical><full-title>Journal of Neurophysiology</full-title><abbr-1>J. Neurophysiol.</abbr-1></periodical><pages>2049-2070</pages><volume>76</volume><number>3</number><issue>3</issue><keywords><keyword>Animals</keyword><keyword>Models, Neurological</keyword><keyword>Membrane Potentials</keyword><keyword>Neural Networks (Computer)</keyword><keyword>Synapses</keyword><keyword>Axons</keyword><keyword>Calcium Channels</keyword><keyword>Potassium Channels</keyword><keyword>Electrophysiology</keyword><keyword>Ion Channels</keyword><keyword>In Vitro Techniques</keyword><keyword>Kinetics</keyword><keyword>Calcium</keyword><keyword>Thalamus</keyword><keyword>Bicuculline</keyword><keyword>Ferrets</keyword><keyword>GABA Antagonists</keyword><keyword>Recruitment, Neurophysiological</keyword><keyword>Refractory Period, Electrophysiological</keyword><keyword>Reticular Formation</keyword><keyword>Up-Regulation</keyword></keywords><dates><year>1996</year><pub-dates><date>Sep 1996</date></pub-dates></dates><isbn>0022-3077</isbn><electronic-resource-num>10.1152/jn.1996.76.3.2049</electronic-resource-num><abstract>1. A network model of thalamocortical (TC) and thalamic reticular (RE) neurons was developed based on electrophysiological measurements in ferret thalamic slices. Single-compartment TC and RE cells included voltage- and calcium-sensitive currents described by Hodgkin-Huxley type of kinetics. Synaptic currents were modeled by kinetic models of alpha-amino-3-hydroxy-5-methyl-4-isoxazolepropionic acid (AMPA), gamma-aminobutyric acid-A (GABAA) and GABAB receptors. 2. The model reproduced successfully the characteristics of spindle and slow bicuculline-induced oscillations observed in vitro. The characteristics of these two types of oscillations depended on both the intrinsic properties of TC and RE cells and their pattern of interconnectivity. 3. The oscillations were organized by the reciprocal recruitment between TC and RE cells, due to their manual connectivity and bursting properties. TC cells elicited AMPA-mediated excitatory postsynaptic potentials (EPSPs) in RE cells, whereas RE cells elicited a mixture of GABAA and GABAB inhibitory postsynaptic potentials (IPSPs) in TC cells. Because of the presence of a T current, sufficiently strong EPSPs could elicit a burst in RE cells, and TC cells could generate a rebound burst following GABAergic IPSPs. Under these conditions, interaction between the TC and RE cells produced sustained oscillations. 4. In the absence of spontaneous oscillation in any cell, the TC-RE network remained quiescent. Spindle oscillations with a frequency of 9-11 Hz could be initiated by stimulation of either TC or RE neurons. A few spontaneously oscillating TC neurons recruited the entire network model into a "waxing-and waning" oscillation. These "initiator" cells could be an extremely small proportion of TC cells. 5. In intracellular recordings, TC cells display a reduced ability for burst firing after a sequence of bursts. The "waning" phase of spindles was reproduced in the network model by assuming an activity-dependent upregulation of Ih operating via a calcium-binding protein in TC cells, as shown previously in a two-cell model. 6. Following the global suppression of GABAA inhibition, the disinhibited RE cells produced prolonged burst discharges that elicited strong GABAB-mediated currents in TC cells. The enhancement of slow IPSPs in TC cells was also due to cooperativity in the activation of GABAB-mediated current. These slow IPSPs recruited TC and RE cells into slower waxing-and-waning oscillations (3-4 HZ) that were even more highly synchronized. 7. Local axonal arborization of the TC to RE and RE to TC projections allowed oscillations to propagate through the network. An oscillation starting at a single focus induced a propagating wavefront as more cells were recruited progressively. The waning of the oscillation also propagated due to upregulation of Ih in TC cells, leading to waves of spindle activity as observed in experiments. 8. The spatiotemporal properties of propagating waves in the model were highly dependent on the intrinsic properties of TC cells. The spatial pattern of spiking activity was markedly different for spindles compared with bicuculline-induced oscillations and depended on the rebound burst behavior of TC cells. The upregulation of Ih produced a refractory period so that colliding spindle waves merged into a single oscillation and extinguished. Finally, reducing the Ih conductance led to sustained oscillations. 9. Two key properties of cells in the thalamic network may account for the initiation, propagation, and termination of spindle oscillations, the activity-dependent upregulation of Ih in TC cells, and the localized axonal projections between TC and RE cells. In addition, the model predicts that a nonlinear stimulus dependency of GABAB responses accounts for the genesis of prolonged synchronized discharges following block of GABAA receptors.</abstract><remote-database-name>PubMed</remote-database-name><language>eng</language><urls><pdf-urls><url>/home/alec/Zotero/storage/7KGW4I9I/Destexhe et al. - 1996 - Ionic mechanisms underlying synchronized oscillati.pdf</url><url>/home/alec/Zotero/storage/HIIGUUDP/Destexhe et al. - 1996 - Ionic mechanisms underlying synchronized oscillati.pdf</url></pdf-urls><text-urls><url>http://www.ncbi.nlm.nih.gov/pubmed/8890314</url><url>http://www.ncbi.nlm.nih.gov/pubmed/8890314</url></text-urls></urls></record><record><database name="MyLibrary">MyLibrary</database><source-app name="Zotero">Zotero</source-app><ref-type name="Journal Article">17</ref-type><contributors><authors><author>Destexhe, A.</author><author>Contreras, D.</author><author>Sejnowski, T. J.</author><author>Steriade, M.</author></authors></contributors><titles><title>A model of spindle rhythmicity in the isolated thalamic reticular nucleus</title><secondary-title>Journal of Neurophysiology</secondary-title></titles><periodical><full-title>Journal of Neurophysiology</full-title><abbr-1>J. Neurophysiol.</abbr-1></periodical><pages>803-818</pages><volume>72</volume><number>2</number><issue>2</issue><keywords><keyword>Animals</keyword><keyword>Nerve Net</keyword><keyword>Synaptic Transmission</keyword><keyword>Membrane Potentials</keyword><keyword>Neural Networks (Computer)</keyword><keyword>Synapses</keyword><keyword>Calcium Channels</keyword><keyword>Cerebral Cortex</keyword><keyword>Potassium Channels</keyword><keyword>Neural Inhibition</keyword><keyword>Ion Channels</keyword><keyword>Cats</keyword><keyword>Electroencephalography</keyword><keyword>Sleep Stages</keyword><keyword>Afferent Pathways</keyword><keyword>gamma-Aminobutyric Acid</keyword><keyword>Receptors, GABA-B</keyword><keyword>Thalamic Nuclei</keyword></keywords><dates><year>1994</year><pub-dates><date>Aug 1994</date></pub-dates></dates><isbn>0022-3077</isbn><electronic-resource-num>10.1152/jn.1994.72.2.803</electronic-resource-num><abstract>1. The oscillatory properties of the isolated reticular (RE) thalamus were modeled with the use of compartmental models of RE cells. Hodgkin-Huxley type kinetic models of ionic channels were derived from voltage- and current-clamp data from RE cells. Interactions between interconnected RE cells were simulated with the use of a kinetic model of gamma-aminobutyric acid (GABA) inhibitory synapses. 2. The intrinsic bursting properties of RE cells in the model were due to the presence of a low-threshold Ca2+ current and two Ca(2+)-activated currents. The properties of these model RE cells were compared with RE neurons recorded intracellularly in vivo in cats. 3. Model RE cells densely interconnected with GABAA synapses produced synchronous oscillations at a frequency close to that of spindles (7-14 Hz). Networks of RE neurons organized in a two-dimensional array with only proximal connectivity also exhibited synchronized oscillations in the spindle range. In addition, the proximally connected network showed periods of high and low synchronicity, giving rise to waxing and waning oscillations in the population of RE cells. 4. The spatiotemporal behavior of the network was investigated during waxing and waning oscillations. The waxing and waning emerged as an alternation between periods of desynchronized and synchronized activity, corresponding to periods of irregular and coherent spatial activity. During synchronized periods, the network displayed propagating coherent waves of synchronous activity that had a tendency to form spirals. 5. Networks of model RE neurons fully connected through GABAB synapses exhibited perfectly synchronous oscillations at lower frequencies (0.5-1 Hz), but two-dimensional networks with proximal GABAB connectivity failed to synchronize. 6. These simulations demonstrate that networks of model neurons that include the main intrinsic currents found in RE cells can generate waxing and waning oscillatory activity similar to the spindle rhythmicity observed in the isolated RE nucleus in vivo. The model reveals the interplay between the intrinsic rhythmic properties of RE cells and the fast synaptic interactions in organizing synchronized rhythmicity.</abstract><remote-database-name>PubMed</remote-database-name><language>eng</language><urls><pdf-urls><url>/home/alec/Zotero/storage/CNBQW5VE/Destexhe et al. - 1994 - A model of spindle rhythmicity in the isolated tha.pdf</url><url>/home/alec/Zotero/storage/MWREW6E3/Destexhe et al. - 1994 - A model of spindle rhythmicity in the isolated tha.pdf</url></pdf-urls><text-urls><url>http://www.ncbi.nlm.nih.gov/pubmed/7527077</url><url>http://www.ncbi.nlm.nih.gov/pubmed/7527077</url></text-urls></urls></record><record><database name="MyLibrary">MyLibrary</database><source-app name="Zotero">Zotero</source-app><ref-type name="Journal Article">17</ref-type><contributors><authors><author>Hasselmo, M. E.</author><author>Schnell, E.</author><author>Barkai, E.</author></authors></contributors><titles><title>Dynamics of learning and recall at excitatory recurrent synapses and cholinergic modulation in rat hippocampal region CA3</title><secondary-title>The Journal of Neuroscience: The Official Journal of the Society for Neuroscience</secondary-title></titles><periodical><full-title>The Journal of Neuroscience: The Official Journal of the Society for Neuroscience</full-title><abbr-1>J. Neurosci.</abbr-1></periodical><pages>5249-5262</pages><volume>15</volume><number>7 Pt 2</number><issue>7 Pt 2</issue><keywords><keyword>Animals</keyword><keyword>Models, Neurological</keyword><keyword>Synaptic Transmission</keyword><keyword>Synapses</keyword><keyword>Learning</keyword><keyword>Rats</keyword><keyword>Rats, Sprague-Dawley</keyword><keyword>Hippocampus</keyword><keyword>Mental Recall</keyword><keyword>Parasympathetic Nervous System</keyword></keywords><dates><year>1995</year><pub-dates><date>Jul 1995</date></pub-dates></dates><isbn>0270-6474</isbn><abstract>Hippocampal region CA3 contains strong recurrent excitation mediated by synapses of the longitudinal association fibers. These recurrent excitatory connections may play a dominant role in determining the information processing characteristics of this region. However, they result in feedback dynamics that may cause both runaway excitatory activity and runaway synaptic modification. Previous models of recurrent excitation have prevented unbounded activity using biologically unrealistic techniques. Here, the activation of feedback inhibition is shown to prevent unbounded activity, allowing stable activity states during recall and learning. In the model, cholinergic suppression of synaptic transmission at excitatory feedback synapses is shown to determine the extent to which activity depends upon new features of the afferent input versus components of previously stored representations. Experimental work in brain slice preparations of region CA3 demonstrates the cholinergic suppression of synaptic transmission in stratum radiatum, which contains synapses of the longitudinal association fibers.</abstract><remote-database-name>PubMed</remote-database-name><language>eng</language><urls><text-urls><url>http://www.ncbi.nlm.nih.gov/pubmed/7623149</url></text-urls></urls></record><record><database name="MyLibrary">MyLibrary</database><source-app name="Zotero">Zotero</source-app><ref-type name="Journal Article">17</ref-type><contributors><authors><author>Izhikevich, Eugene M.</author></authors></contributors><titles><title>Which model to use for cortical spiking neurons?</title><secondary-title>IEEE transactions on neural networks</secondary-title></titles><periodical><full-title>IEEE transactions on neural networks</full-title><abbr-1>IEEE Trans Neural Netw</abbr-1></periodical><pages>1063-1070</pages><volume>15</volume><number>5</number><issue>5</issue><keywords><keyword>Animals</keyword><keyword>Models, Neurological</keyword><keyword>Nerve Net</keyword><keyword>Neurons</keyword><keyword>Synaptic Transmission</keyword><keyword>Action Potentials</keyword><keyword>Humans</keyword><keyword>Synapses</keyword><keyword>Cerebral Cortex</keyword><keyword>Neural Pathways</keyword><keyword>Reaction Time</keyword><keyword>Nonlinear Dynamics</keyword></keywords><dates><year>2004</year><pub-dates><date>Sep 2004</date></pub-dates></dates><isbn>1045-9227</isbn><electronic-resource-num>10.1109/TNN.2004.832719</electronic-resource-num><abstract>We discuss the biological plausibility and computational efficiency of some of the most useful models of spiking and bursting neurons. We compare their applicability to large-scale simulations of cortical neural networks.</abstract><remote-database-name>PubMed</remote-database-name><language>eng</language><urls><text-urls><url>http://www.ncbi.nlm.nih.gov/pubmed/15484883</url></text-urls></urls></record><record><database name="MyLibrary">MyLibrary</database><source-app name="Zotero">Zotero</source-app><ref-type name="Journal Article">17</ref-type><contributors><authors><author>Yuan, Ye</author><author>Liu, Jian</author><author>Zhao, Peng</author><author>Xing, Fu</author><author>Huo, Hong</author><author>Fang, Tao</author></authors></contributors><titles><title>Structural Insights Into the Dynamic Evolution of Neuronal Networks as Synaptic Density Decreases</title><secondary-title>Frontiers in Neuroscience</secondary-title></titles><periodical><full-title>Frontiers in Neuroscience</full-title><abbr-1>Front Neurosci</abbr-1></periodical><pages>892</pages><volume>13</volume><keywords><keyword>evolving network model</keyword><keyword>network connectivity</keyword><keyword>network efficiency</keyword><keyword>scale-free network</keyword><keyword>synaptic density</keyword></keywords><dates><year>2019</year><pub-dates><date>2019</date></pub-dates></dates><isbn>1662-4548</isbn><electronic-resource-num>10.3389/fnins.2019.00892</electronic-resource-num><abstract>The human brain is thought to be an extremely complex but efficient computing engine, processing vast amounts of information from a changing world. The decline in the synaptic density of neuronal networks is one of the most important characteristics of brain development, which is closely related to synaptic pruning, synaptic growth, synaptic plasticity, and energy metabolism. However, because of technical limitations in observing large-scale neuronal networks dynamically connected through synapses, how neuronal networks are organized and evolve as their synaptic density declines remains unclear. Here, by establishing a biologically reasonable neuronal network model, we show that despite a decline in the synaptic density, the connectivity, and efficiency of neuronal networks can be improved. Importantly, by analyzing the degree distribution, we also find that both the scale-free characteristic of neuronal networks and the emergence of hub neurons rely on the spatial distance between neurons. These findings may promote our understanding of neuronal networks in the brain and have guiding significance for the design of neuronal network models.</abstract><remote-database-name>PubMed</remote-database-name><language>eng</language><urls><pdf-urls><url>/home/alec/Zotero/storage/2MXJLZDS/Yuan et al. - 2019 - Structural Insights Into the Dynamic Evolution of .pdf</url></pdf-urls><text-urls><url>http://www.ncbi.nlm.nih.gov/pubmed/31507365</url></text-urls></urls></record><record><database name="MyLibrary">MyLibrary</database><source-app name="Zotero">Zotero</source-app><ref-type name="Journal Article">17</ref-type><contributors><authors><author>Poirazi, Panayiota</author><author>Brannon, Terrence</author><author>Mel, Bartlett W.</author></authors></contributors><titles><title>Arithmetic of Subthreshold Synaptic Summation in a Model CA1 Pyramidal Cell</title><secondary-title>Neuron</secondary-title></titles><periodical><full-title>Neuron</full-title><abbr-1>Neuron</abbr-1></periodical><pages>977-987</pages><volume>37</volume><number>6</number><issue>6</issue><dates><year>2003</year><pub-dates><date>2003-03-27</date></pub-dates></dates><isbn>0896-6273</isbn><electronic-resource-num>10.1016/S0896-6273(03)00148-X</electronic-resource-num><remote-database-name>www.cell.com</remote-database-name><language>English</language><urls><web-urls><url>https://www.cell.com/neuron/abstract/S0896-6273(03)00148-X</url></web-urls><pdf-urls><url>/home/alec/Zotero/storage/CIS982G2/Poirazi et al. - 2003 - Arithmetic of Subthreshold Synaptic Summation in a.pdf</url></pdf-urls><text-urls><url>http://www.ncbi.nlm.nih.gov/pubmed/12670426</url><url>/home/alec/Zotero/storage/YIIAC7GN/S0896-6273(03)00148-X.html</url></text-urls></urls><access-date>2019-11-25 17:32:48</access-date></record><record><database name="MyLibrary">MyLibrary</database><source-app name="Zotero">Zotero</source-app><ref-type name="Journal Article">17</ref-type><contributors><authors><author>Sadeh, Sadra</author><author>Silver, R. Angus</author><author>Mrsic-Flogel, Thomas D.</author><author>Muir, Dylan Richard</author></authors></contributors><titles><title>Assessing the Role of Inhibition in Stabilizing Neocortical Networks Requires Large-Scale Perturbation of the Inhibitory Population</title><secondary-title>The Journal of Neuroscience: The Official Journal of the Society for Neuroscience</secondary-title></titles><periodical><full-title>The Journal of Neuroscience: The Official Journal of the Society for Neuroscience</full-title><abbr-1>J. Neurosci.</abbr-1></periodical><pages>12050-12067</pages><volume>37</volume><number>49</number><issue>49</issue><keywords><keyword>Animals</keyword><keyword>Nerve Net</keyword><keyword>Action Potentials</keyword><keyword>Humans</keyword><keyword>Neural Inhibition</keyword><keyword>Neocortex</keyword><keyword>computational model</keyword><keyword>cortical computation</keyword><keyword>inhibitory stabilization</keyword><keyword>optogenetics</keyword><keyword>recurrent excitation</keyword></keywords><dates><year>2017</year><pub-dates><date>12 06, 2017</date></pub-dates></dates><isbn>1529-2401</isbn><electronic-resource-num>10.1523/JNEUROSCI.0963-17.2017</electronic-resource-num><abstract>Neurons within cortical microcircuits are interconnected with recurrent excitatory synaptic connections that are thought to amplify signals (Douglas and Martin, 2007), form selective subnetworks (Ko et al., 2011), and aid feature discrimination. Strong inhibition (Haider et al., 2013) counterbalances excitation, enabling sensory features to be sharpened and represented by sparse codes (Willmore et al., 2011). This balance between excitation and inhibition makes it difficult to assess the strength, or gain, of recurrent excitatory connections within cortical networks, which is key to understanding their operational regime and the computations that they perform. Networks that combine an unstable high-gain excitatory population with stabilizing inhibitory feedback are known as inhibition-stabilized networks (ISNs) (Tsodyks et al., 1997). Theoretical studies using reduced network models predict that ISNs produce paradoxical responses to perturbation, but experimental perturbations failed to find evidence for ISNs in cortex (Atallah et al., 2012). Here, we reexamined this question by investigating how cortical network models consisting of many neurons behave after perturbations and found that results obtained from reduced network models fail to predict responses to perturbations in more realistic networks. Our models predict that a large proportion of the inhibitory network must be perturbed to reliably detect an ISN regime robustly in cortex. We propose that wide-field optogenetic suppression of inhibition under promoters targeting a large fraction of inhibitory neurons may provide a perturbation of sufficient strength to reveal the operating regime of cortex. Our results suggest that detailed computational models of optogenetic perturbations are necessary to interpret the results of experimental paradigms.SIGNIFICANCE STATEMENT Many useful computational mechanisms proposed for cortex require local excitatory recurrence to be very strong, such that local inhibitory feedback is necessary to avoid epileptiform runaway activity (an "inhibition-stabilized network" or "ISN" regime). However, recent experimental results suggest that this regime may not exist in cortex. We simulated activity perturbations in cortical networks of increasing realism and found that, to detect ISN-like properties in cortex, large proportions of the inhibitory population must be perturbed. Current experimental methods for inhibitory perturbation are unlikely to satisfy this requirement, implying that existing experimental observations are inconclusive about the computational regime of cortex. Our results suggest that new experimental designs targeting a majority of inhibitory neurons may be able to resolve this question.</abstract><remote-database-name>PubMed</remote-database-name><language>eng</language><urls><pdf-urls><url>/home/alec/Zotero/storage/9925C2BK/Sadeh et al. - 2017 - Assessing the Role of Inhibition in Stabilizing Ne.pdf</url></pdf-urls><text-urls><url>http://www.ncbi.nlm.nih.gov/pubmed/29074575</url></text-urls></urls></record><record><database name="MyLibrary">MyLibrary</database><source-app name="Zotero">Zotero</source-app><ref-type name="Journal Article">17</ref-type><contributors><authors><author>Potjans, Tobias C.</author><author>Diesmann, Markus</author></authors></contributors><titles><title>The cell-type specific cortical microcircuit: relating structure and activity in a full-scale spiking network model</title><secondary-title>Cerebral Cortex (New York, N.Y.: 1991)</secondary-title><short-title>The cell-type specific cortical microcircuit</short-title></titles><periodical><full-title>Cerebral Cortex (New York, N.Y.: 1991)</full-title><abbr-1>Cereb. Cortex</abbr-1></periodical><pages>785-806</pages><volume>24</volume><number>3</number><issue>3</issue><keywords><keyword>Models, Neurological</keyword><keyword>Nerve Net</keyword><keyword>Neurons</keyword><keyword>Neural Networks (Computer)</keyword><keyword>Action Potentials</keyword><keyword>Humans</keyword><keyword>Computer Simulation</keyword><keyword>Cerebral Cortex</keyword><keyword>Neural Inhibition</keyword><keyword>Neural Pathways</keyword><keyword>large-scale models</keyword><keyword>connectivity maps</keyword><keyword>cortical microcircuit</keyword><keyword>layered network</keyword><keyword>specificity of connections</keyword></keywords><dates><year>2014</year><pub-dates><date>Mar 2014</date></pub-dates></dates><isbn>1460-2199</isbn><electronic-resource-num>10.1093/cercor/bhs358</electronic-resource-num><abstract>In the past decade, the cell-type specific connectivity and activity of local cortical networks have been characterized experimentally to some detail. In parallel, modeling has been established as a tool to relate network structure to activity dynamics. While available comprehensive connectivity maps ( Thomson, West, et al. 2002; Binzegger et al. 2004) have been used in various computational studies, prominent features of the simulated activity such as the spontaneous firing rates do not match the experimental findings. Here, we analyze the properties of these maps to compile an integrated connectivity map, which additionally incorporates insights on the specific selection of target types. Based on this integrated map, we build a full-scale spiking network model of the local cortical microcircuit. The simulated spontaneous activity is asynchronous irregular and cell-type specific firing rates are in agreement with in vivo recordings in awake animals, including the low rate of layer 2/3 excitatory cells. The interplay of excitation and inhibition captures the flow of activity through cortical layers after transient thalamic stimulation. In conclusion, the integration of a large body of the available connectivity data enables us to expose the dynamical consequences of the cortical microcircuitry.</abstract><remote-database-name>PubMed</remote-database-name><language>eng</language><urls><pdf-urls><url>/home/alec/Zotero/storage/VF7TV46T/Potjans and Diesmann - 2014 - The cell-type specific cortical microcircuit rela.pdf</url></pdf-urls><text-urls><url>http://www.ncbi.nlm.nih.gov/pubmed/23203991</url></text-urls></urls></record><record><database name="MyLibrary">MyLibrary</database><source-app name="Zotero">Zotero</source-app><ref-type name="Journal Article">17</ref-type><contributors><authors><author>Nadim, F.</author><author>Manor, Y.</author><author>Nusbaum, M. P.</author><author>Marder, E.</author></authors></contributors><titles><title>Frequency regulation of a slow rhythm by a fast periodic input</title><secondary-title>The Journal of Neuroscience: The Official Journal of the Society for Neuroscience</secondary-title></titles><periodical><full-title>The Journal of Neuroscience: The Official Journal of the Society for Neuroscience</full-title><abbr-1>J. Neurosci.</abbr-1></periodical><pages>5053-5067</pages><volume>18</volume><number>13</number><issue>13</issue><keywords><keyword>Animals</keyword><keyword>Ganglia, Invertebrate</keyword><keyword>Models, Neurological</keyword><keyword>Neurons</keyword><keyword>Pylorus</keyword><keyword>Action Potentials</keyword><keyword>Electrophysiology</keyword><keyword>Periodicity</keyword><keyword>Brachyura</keyword><keyword>Nervous System Physiological Phenomena</keyword><keyword>Time Factors</keyword></keywords><dates><year>1998</year><pub-dates><date>Jul 01, 1998</date></pub-dates></dates><isbn>0270-6474</isbn><abstract>Many nervous systems contain rhythmically active subnetworks that interact despite oscillating at widely different frequencies. The stomatogastric nervous system of the crab Cancer borealis produces a rapid pyloric rhythm and a considerably slower gastric mill rhythm. We construct and analyze a conductance-based compartmental model to explore the activation of the gastric mill rhythm by the modulatory commissural neuron 1 (MCN1). This model demonstrates that the period of the MCN1-activated gastric mill rhythm, which was thought to be determined entirely by the interaction of neurons in the gastric mill network, can be strongly influenced by inhibitory synaptic input from the pacemaker neuron of the fast pyloric rhythm, the anterior burster (AB) neuron. Surprisingly, the change of the gastric mill period produced by the pyloric input to the gastric mill system can be many times larger than the period of the pyloric rhythm itself. This model illustrates several mechanisms by which a fast oscillatory neuron may control the frequency of a much slower oscillatory network. These findings suggest that it is possible to modify the slow rhythm either by direct modulation or indirectly by modulating the faster rhythm.</abstract><remote-database-name>PubMed</remote-database-name><language>eng</language><urls><text-urls><url>http://www.ncbi.nlm.nih.gov/pubmed/9634571</url></text-urls></urls></record><record><database name="MyLibrary">MyLibrary</database><source-app name="Zotero">Zotero</source-app><ref-type name="Journal Article">17</ref-type><contributors><authors><author>Lytton, William W.</author><author>Contreras, Diego</author><author>Destexhe, Alain</author><author>Steriade, Mircea</author></authors></contributors><titles><title>Dynamic Interactions Determine Partial Thalamic Quiescence in a Computer Network Model of Spike-and-Wave Seizures</title><secondary-title>Journal of Neurophysiology</secondary-title></titles><periodical><full-title>Journal of Neurophysiology</full-title><abbr-1>Journal of Neurophysiology</abbr-1></periodical><pages>1679-1696</pages><volume>77</volume><number>4</number><issue>4</issue><dates><year>1997</year><pub-dates><date>April 1, 1997</date></pub-dates></dates><isbn>0022-3077</isbn><electronic-resource-num>10.1152/jn.1997.77.4.1679</electronic-resource-num><abstract>Lytton, William W., Diego Contreras, Alain Destexhe, and Mircea Steriade. Dynamic interactions determine partial thalamic quiescence in a computer network model of spike-and-wave seizures. J. Neurophysiol. 77: 1679–1696, 1997. In vivo intracellular recording from cat thalamus and cortex was performed during spontaneous spike-wave seizures characterized by synchronously firing cortical neurons correlated with the electroencephalogram. During these seizures, thalamic reticular (RE) neurons discharged with long spike bursts riding on a depolarization, whereas thalamocortical (TC) neurons were either entrained into the seizures (40%) or were quiescent (60%). During quiescence, TC neurons showed phasic inhibitory postsynaptic potentials (IPSPs) that coincided with paroxysmal depolarizing shifts in the simultaneously recorded cortical neuron. Computer simulations of a reciprocally connected TC-RE pair showed two major modes of TC-RE interaction. In one mode, a mutual oscillation involved direct TC neuron excitation of the RE neuron leading to a burst that fed back an IPSP into the TC neuron, producing a low-threshold spike. In the other, quiescent mode, the TC neuron was subject to stronger coalescing IPSPs. Simulated cortical stimulation could trigger a transition between the two modes. This transition could go in either direction and was dependent on the precise timing of the input. The transition did not always follow the stimulation immediately. A larger, multicolumnar simulation was set up to assess the role of the TC-RE pair in the context of extensive divergence and convergence. The amount of TC neuron spiking generally correlated with the strength of total inhibitory input, but large variations in the amount of spiking could be seen. Evidence for mutual oscillation could be demonstrated by comparing TC neuron firing with that in reciprocally connected RE neurons. An additional mechanism for TC neuron quiescence was assessed with the use of a cooperative model of γ-aminobutyric acid-B (GABAB)-mediated responses. With this model, RE neurons receiving repeated strong excitatory input produced TC neuron quiescence due to burst-duration-associated augmentation of GABAB current. We predict the existence of spatial inhomogeneity in apparently generalized spike-wave seizures, involving a center-surround pattern. In the center, intense cortical and RE neuron activity would be associated with TC neuron quiescence. In the surround, less intense hyperpolarization of TC neurons would allow low-threshold spikes to occur. This surround, an “epileptic penumbra,” would be the forefront of the expanding epileptic wave during the process of initial seizure generalization. Therapeutically, we would then predict that agents that reduce TC neuron activity would have a greater effect on seizure onset than on ongoing spike-wave seizures or other thalamic oscillations.</abstract><remote-database-name>physiology.org (Atypon)</remote-database-name><urls><web-urls><url>https://www.physiology.org/doi/full/10.1152/jn.1997.77.4.1679</url></web-urls><pdf-urls><url>/home/alec/Zotero/storage/CYCZUVJB/Lytton et al. - 1997 - Dynamic Interactions Determine Partial Thalamic Qu.pdf</url></pdf-urls><text-urls><url>/home/alec/Zotero/storage/DNEUZ7MQ/jn.1997.77.4.html</url></text-urls></urls><access-date>2019-11-26 18:00:42</access-date></record><record><database name="MyLibrary">MyLibrary</database><source-app name="Zotero">Zotero</source-app><ref-type name="Journal Article">17</ref-type><contributors><authors><author>Hill, Andrew A. V.</author><author>Masino, Mark A.</author><author>Calabrese, Ronald L.</author></authors></contributors><titles><title>Model of intersegmental coordination in the leech heartbeat neuronal network</title><secondary-title>Journal of Neurophysiology</secondary-title></titles><periodical><full-title>Journal of Neurophysiology</full-title><abbr-1>J. Neurophysiol.</abbr-1></periodical><pages>1586-1602</pages><volume>87</volume><number>3</number><issue>3</issue><keywords><keyword>Animals</keyword><keyword>Ganglia, Invertebrate</keyword><keyword>Neural Networks (Computer)</keyword><keyword>Action Potentials</keyword><keyword>Interneurons</keyword><keyword>Neural Inhibition</keyword><keyword>Periodicity</keyword><keyword>Heart</keyword><keyword>Heart Rate</keyword><keyword>Nervous System</keyword><keyword>Leeches</keyword></keywords><dates><year>2002</year><pub-dates><date>Mar 2002</date></pub-dates></dates><isbn>0022-3077</isbn><electronic-resource-num>10.1152/jn.00337.2001</electronic-resource-num><abstract>We have created a computational model of the timing network that paces the heartbeat of the medicinal leech, Hirudo medicinalis. The rhythmic activity of this network originates from two segmental oscillators located in the third and fourth midbody ganglia. In the intact nerve cord, these segmental oscillators are mutually entrained to the same cycle period. Although experiments have shown that the segmental oscillators are coupled by inhibitory coordinating interneurons, the underlying mechanisms of intersegmental coordination have not yet been elucidated. To help understand this coordination, we have created a simple computational model with two variants: symmetric and asymmetric. In the symmetric model, neurons within each segmental oscillator called oscillator interneurons, inhibit the coordinating interneurons. In contrast, in the asymmetric model only the oscillator interneurons of one segmental oscillator inhibit the coordinating interneurons. In the symmetric model, when two segmental oscillators with different inherent periods are coupled, the faster one leads in phase, and the period of the coupled system is equal to the period of the faster oscillator. This behavior arises because, during each oscillation cycle, the oscillator interneurons of the faster segmental oscillator begin to burst before those of the slower oscillator, thereby terminating spike activity in the coordinating interneurons. Thus there is a brief period of time in each cycle when the oscillator interneurons of the slower segmental oscillator are relieved of inhibition from the coordinating interneurons. This "removal of synaptic inhibition" allows, within certain limits, the slower segmental oscillator to be sped to the period of the faster one. Thus the symmetric model demonstrates a plausible biophysical mechanism by which one segmental oscillator can entrain the other. In general the asymmetric model, in which only one segmental oscillator has the ability to inhibit the coordinating interneurons, behaves similarly, except only one segmental oscillator can control the period of the system. In addition, we simulated physiological experiments in which a "driving" stimulus, consisting of alternating positive and negative current steps, was used to control a single oscillator interneuron and thereby entrain the activity of the entire timing network.</abstract><remote-database-name>PubMed</remote-database-name><language>eng</language><urls><pdf-urls><url>/home/alec/Zotero/storage/RSKRQQHZ/Hill et al. - 2002 - Model of intersegmental coordination in the leech .pdf</url></pdf-urls><text-urls><url>http://www.ncbi.nlm.nih.gov/pubmed/11877528</url></text-urls></urls></record><record><database name="MyLibrary">MyLibrary</database><source-app name="Zotero">Zotero</source-app><ref-type name="Journal Article">17</ref-type><contributors><authors><author>Bartos, Marlene</author><author>Vida, Imre</author><author>Frotscher, Michael</author><author>Meyer, Axel</author><author>Monyer, Hannah</author><author>Geiger, Jörg R. P.</author><author>Jonas, Peter</author></authors></contributors><titles><title>Fast synaptic inhibition promotes synchronized gamma oscillations in hippocampal interneuron networks</title><secondary-title>Proceedings of the National Academy of Sciences</secondary-title></titles><periodical><full-title>Proceedings of the National Academy of Sciences</full-title><abbr-1>PNAS</abbr-1></periodical><pages>13222-13227</pages><volume>99</volume><number>20</number><issue>20</issue><dates><year>2002</year><pub-dates><date>2002/10/01</date></pub-dates></dates><isbn>0027-8424, 1091-6490</isbn><electronic-resource-num>10.1073/pnas.192233099</electronic-resource-num><abstract>Networks of GABAergic interneurons are of critical importance for the generation of gamma frequency oscillations in the brain. To examine the underlying synaptic mechanisms, we made paired recordings from “basket cells” (BCs) in different subfields of hippocampal slices, using transgenic mice that express enhanced green fluorescent protein (EGFP) under the control of the parvalbumin promoter. Unitary inhibitory postsynaptic currents (IPSCs) showed large amplitude and fast time course with mean amplitude-weighted decay time constants of 2.5, 1.2, and 1.8 ms in the dentate gyrus, and the cornu ammonis area 3 (CA3) and 1 (CA1), respectively (33–34°C). The decay of unitary IPSCs at BC–BC synapses was significantly faster than that at BC–principal cell synapses, indicating target cell-specific differences in IPSC kinetics. In addition, electrical coupling was found in a subset of BC–BC pairs. To examine whether an interneuron network with fast inhibitory synapses can act as a gamma frequency oscillator, we developed an interneuron network model based on experimentally determined properties. In comparison to previous interneuron network models, our model was able to generate oscillatory activity with higher coherence over a broad range of frequencies (20–110 Hz). In this model, high coherence and flexibility in frequency control emerge from the combination of synaptic properties, network structure, and electrical coupling.</abstract><remote-database-name>www.pnas.org</remote-database-name><language>en</language><urls><web-urls><url>https://www.pnas.org/content/99/20/13222</url></web-urls><pdf-urls><url>/home/alec/Zotero/storage/GIBAMX28/Bartos et al. - 2002 - Fast synaptic inhibition promotes synchronized gam.pdf</url></pdf-urls><text-urls><url>http://www.ncbi.nlm.nih.gov/pubmed/12235359</url><url>/home/alec/Zotero/storage/CZW2IQ7X/13222.html</url></text-urls></urls><access-date>2019-11-26 19:54:19</access-date></record><record><database name="MyLibrary">MyLibrary</database><source-app name="Zotero">Zotero</source-app><ref-type name="Journal Article">17</ref-type><contributors><authors><author>Wang, Xiao-Jing</author><author>Buzsáki, György</author></authors></contributors><titles><title>Gamma Oscillation by Synaptic Inhibition in a Hippocampal Interneuronal Network Model</title><secondary-title>Journal of Neuroscience</secondary-title></titles><periodical><full-title>Journal of Neuroscience</full-title><abbr-1>J. Neurosci.</abbr-1></periodical><pages>6402-6413</pages><volume>16</volume><number>20</number><issue>20</issue><keywords><keyword>interneurons</keyword><keyword>hippocampus</keyword><keyword>computer model</keyword><keyword>GABAA</keyword><keyword>gamma rhythm</keyword><keyword>synchronization</keyword></keywords><dates><year>1996</year><pub-dates><date>1996/10/15</date></pub-dates></dates><isbn>0270-6474, 1529-2401</isbn><electronic-resource-num>10.1523/JNEUROSCI.16-20-06402.1996</electronic-resource-num><abstract>Fast neuronal oscillations (gamma, 20–80 Hz) have been observed in the neocortex and hippocampus during behavioral arousal. Using computer simulations, we investigated the hypothesis that such rhythmic activity can emerge in a random network of interconnected GABAergic fast-spiking interneurons. Specific conditions for the population synchronization, on properties of single cells and the circuit, were identified. These include the following: (1) that the amplitude of spike afterhyperpolarization be above the GABAA synaptic reversal potential; (2) that the ratio between the synaptic decay time constant and the oscillation period be sufficiently large; (3) that the effects of heterogeneities be modest because of a steep frequency–current relationship of fast-spiking neurons. Furthermore, using a population coherence measure, based on coincident firings of neural pairs, it is demonstrated that large-scale network synchronization requires a critical (minimal) average number of synaptic contacts per cell, which is not sensitive to the network size.&#xD;By changing the GABAA synaptic maximal conductance, synaptic decay time constant, or the mean external excitatory drive to the network, the neuronal firing frequencies were gradually and monotonically varied. By contrast, the network synchronization was found to be high only within a frequency band coinciding with the gamma (20–80 Hz) range. We conclude that the GABAA synaptic transmission provides a suitable mechanism for synchronized gamma oscillations in a sparsely connected network of fast-spiking interneurons. In turn, the interneuronal network can presumably maintain subthreshold oscillations in principal cell populations and serve to synchronize discharges of spatially distributed neurons.</abstract><remote-database-name>www.jneurosci.org</remote-database-name><language>en</language><urls><web-urls><url>https://www.jneurosci.org/content/16/20/6402</url></web-urls><pdf-urls><url>/home/alec/Zotero/storage/Q7BHA8QQ/Wang and Buzsáki - 1996 - Gamma Oscillation by Synaptic Inhibition in a Hipp.pdf</url></pdf-urls><text-urls><url>http://www.ncbi.nlm.nih.gov/pubmed/8815919</url><url>/home/alec/Zotero/storage/IFIMQHV3/6402.html</url></text-urls></urls><access-date>2019-11-26 20:00:34</access-date></record><record><database name="MyLibrary">MyLibrary</database><source-app name="Zotero">Zotero</source-app><ref-type name="Journal Article">17</ref-type><contributors><authors><author>Schmidhuber, Juergen</author></authors></contributors><titles><title>Deep Learning in Neural Networks: An Overview</title><secondary-title>Neural Networks</secondary-title><short-title>Deep Learning in Neural Networks</short-title></titles><periodical><full-title>Neural Networks</full-title><abbr-1>Neural Networks</abbr-1></periodical><pages>85-117</pages><volume>61</volume><keywords><keyword>Computer Science - Machine Learning</keyword><keyword>Computer Science - Neural and Evolutionary Computing</keyword></keywords><dates><year>2015</year><pub-dates><date>01/2015</date></pub-dates></dates><isbn>08936080</isbn><electronic-resource-num>10.1016/j.neunet.2014.09.003</electronic-resource-num><abstract>In recent years, deep artificial neural networks (including recurrent ones) have won numerous contests in pattern recognition and machine learning. This historical survey compactly summarises relevant work, much of it from the previous millennium. Shallow and deep learners are distinguished by the depth of their credit assignment paths, which are chains of possibly learnable, causal links between actions and effects. I review deep supervised learning (also recapitulating the history of backpropagation), unsupervised learning, reinforcement learning &amp; evolutionary computation, and indirect search for short programs encoding deep and large networks.</abstract><remote-database-name>arXiv.org</remote-database-name><urls><web-urls><url>http://arxiv.org/abs/1404.7828</url></web-urls><pdf-urls><url>/home/alec/Zotero/storage/UA32GKS8/Schmidhuber - 2015 - Deep Learning in Neural Networks An Overview.pdf</url><url>/home/alec/Zotero/storage/KQ2C9WDK/Schmidhuber - 2015 - Deep Learning in Neural Networks An Overview.pdf</url></pdf-urls><text-urls><url>/home/alec/Zotero/storage/5U4Y8M4I/1404.html</url><url>/home/alec/Zotero/storage/3W3HP6UV/1404.html</url></text-urls></urls><access-date>2019-11-26 21:39:30</access-date></record><record><database name="MyLibrary">MyLibrary</database><source-app name="Zotero">Zotero</source-app><ref-type name="Journal Article">17</ref-type><contributors><authors><author>Krizhevsky, Alex</author><author>Sutskever, Ilya</author><author>Hinton, Geoffrey E.</author></authors></contributors><titles><title>ImageNet classification with deep convolutional neural networks</title><secondary-title>Communications of the ACM</secondary-title></titles><periodical><full-title>Communications of the ACM</full-title></periodical><pages>84-90</pages><volume>60</volume><number>6</number><issue>6</issue><dates><year>2017</year><pub-dates><date>05/24/2017</date></pub-dates></dates><isbn>0001-0782</isbn><electronic-resource-num>10.1145/3065386</electronic-resource-num><remote-database-name>dl.acm.org</remote-database-name><urls><web-urls><url>http://dl.acm.org/citation.cfm?id=3098997.3065386</url></web-urls><pdf-urls><url>/home/alec/Zotero/storage/FMEMB5JU/Krizhevsky et al. - 2017 - ImageNet classification with deep convolutional ne.pdf</url></pdf-urls></urls><access-date>2019-11-26 21:55:45</access-date></record><record><database name="MyLibrary">MyLibrary</database><source-app name="Zotero">Zotero</source-app><ref-type name="Journal Article">17</ref-type><contributors><authors><author>Simonyan, Karen</author><author>Zisserman, Andrew</author></authors></contributors><titles><title>Very Deep Convolutional Networks for Large-Scale Image Recognition</title><secondary-title>arXiv:1409.1556 [cs]</secondary-title></titles><periodical><full-title>arXiv:1409.1556 [cs]</full-title></periodical><keywords><keyword>Computer Science - Computer Vision and Pattern Recognition</keyword></keywords><dates><year>2015</year><pub-dates><date>2015-04-10</date></pub-dates></dates><abstract>In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.</abstract><remote-database-name>arXiv.org</remote-database-name><urls><web-urls><url>http://arxiv.org/abs/1409.1556</url></web-urls><pdf-urls><url>/home/alec/Zotero/storage/62P8ZU4S/Simonyan and Zisserman - 2015 - Very Deep Convolutional Networks for Large-Scale I.pdf</url></pdf-urls><text-urls><url>/home/alec/Zotero/storage/9HN3GK9P/1409.html</url></text-urls></urls><access-date>2019-11-26 22:03:36</access-date></record><record><database name="MyLibrary">MyLibrary</database><source-app name="Zotero">Zotero</source-app><ref-type name="Journal Article">17</ref-type><contributors><authors><author>He, Kaiming</author><author>Zhang, Xiangyu</author><author>Ren, Shaoqing</author><author>Sun, Jian</author></authors></contributors><titles><title>Deep Residual Learning for Image Recognition</title><secondary-title>arXiv:1512.03385 [cs]</secondary-title></titles><periodical><full-title>arXiv:1512.03385 [cs]</full-title></periodical><keywords><keyword>Computer Science - Computer Vision and Pattern Recognition</keyword></keywords><dates><year>2015</year><pub-dates><date>2015-12-10</date></pub-dates></dates><abstract>Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC &amp; COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.</abstract><remote-database-name>arXiv.org</remote-database-name><urls><web-urls><url>http://arxiv.org/abs/1512.03385</url></web-urls><pdf-urls><url>/home/alec/Zotero/storage/E6SDIDLX/He et al. - 2015 - Deep Residual Learning for Image Recognition.pdf</url></pdf-urls><text-urls><url>/home/alec/Zotero/storage/Y846DGIZ/1512.html</url></text-urls></urls><access-date>2019-11-26 22:12:29</access-date></record><record><database name="MyLibrary">MyLibrary</database><source-app name="Zotero">Zotero</source-app><ref-type name="Journal Article">17</ref-type><contributors><authors><author>Canziani, Alfredo</author><author>Paszke, Adam</author><author>Culurciello, Eugenio</author></authors></contributors><titles><title>An Analysis of Deep Neural Network Models for Practical Applications</title><secondary-title>arXiv:1605.07678 [cs]</secondary-title></titles><periodical><full-title>arXiv:1605.07678 [cs]</full-title></periodical><keywords><keyword>Computer Science - Computer Vision and Pattern Recognition</keyword></keywords><dates><year>2017</year><pub-dates><date>2017-04-14</date></pub-dates></dates><abstract>Since the emergence of Deep Neural Networks (DNNs) as a prominent technique in the field of computer vision, the ImageNet classification challenge has played a major role in advancing the state-of-the-art. While accuracy figures have steadily increased, the resource utilisation of winning models has not been properly taken into account. In this work, we present a comprehensive analysis of important metrics in practical applications: accuracy, memory footprint, parameters, operations count, inference time and power consumption. Key findings are: (1) power consumption is independent of batch size and architecture; (2) accuracy and inference time are in a hyperbolic relationship; (3) energy constraint is an upper bound on the maximum achievable accuracy and model complexity; (4) the number of operations is a reliable estimate of the inference time. We believe our analysis provides a compelling set of information that helps design and engineer efficient DNNs.</abstract><remote-database-name>arXiv.org</remote-database-name><urls><web-urls><url>http://arxiv.org/abs/1605.07678</url></web-urls><pdf-urls><url>/home/alec/Zotero/storage/52AYWG4U/Canziani et al. - 2017 - An Analysis of Deep Neural Network Models for Prac.pdf</url></pdf-urls><text-urls><url>/home/alec/Zotero/storage/A92L96YD/1605.html</url></text-urls></urls><access-date>2019-11-26 22:39:12</access-date></record><record><database name="MyLibrary">MyLibrary</database><source-app name="Zotero">Zotero</source-app><ref-type name="Journal Article">17</ref-type><contributors><authors><author>Alvarez, Jose M.</author><author>Salzmann, Mathieu</author></authors></contributors><titles><title>Learning the Number of Neurons in Deep Networks</title><secondary-title>arXiv:1611.06321 [cs]</secondary-title></titles><periodical><full-title>arXiv:1611.06321 [cs]</full-title></periodical><keywords><keyword>Computer Science - Machine Learning</keyword><keyword>Computer Science - Computer Vision and Pattern Recognition</keyword><keyword>Computer Science - Neural and Evolutionary Computing</keyword></keywords><dates><year>2018</year><pub-dates><date>2018-10-11</date></pub-dates></dates><abstract>Nowadays, the number of layers and of neurons in each layer of a deep network are typically set manually. While very deep and wide networks have proven effective in general, they come at a high memory and computation cost, thus making them impractical for constrained platforms. These networks, however, are known to have many redundant parameters, and could thus, in principle, be replaced by more compact architectures. In this paper, we introduce an approach to automatically determining the number of neurons in each layer of a deep network during learning. To this end, we propose to make use of structured sparsity during learning. More precisely, we use a group sparsity regularizer on the parameters of the network, where each group is defined to act on a single neuron. Starting from an overcomplete network, we show that our approach can reduce the number of parameters by up to 80\% while retaining or even improving the network accuracy.</abstract><remote-database-name>arXiv.org</remote-database-name><urls><web-urls><url>http://arxiv.org/abs/1611.06321</url></web-urls><pdf-urls><url>/home/alec/Zotero/storage/UPFEL75V/Alvarez and Salzmann - 2018 - Learning the Number of Neurons in Deep Networks.pdf</url></pdf-urls><text-urls><url>/home/alec/Zotero/storage/VFWP9Y5G/1611.html</url></text-urls></urls><access-date>2019-11-27 00:03:47</access-date></record><record><database name="MyLibrary">MyLibrary</database><source-app name="Zotero">Zotero</source-app><ref-type name="Journal Article">17</ref-type><contributors><authors><author>Markram, Henry</author><author>Muller, Eilif</author><author>Ramaswamy, Srikanth</author><author>Reimann, Michael W.</author><author>Abdellah, Marwan</author><author>Sanchez, Carlos Aguado</author><author>Ailamaki, Anastasia</author><author>Alonso-Nanclares, Lidia</author><author>Antille, Nicolas</author><author>Arsever, Selim</author><author>Kahou, Guy Antoine Atenekeng</author><author>Berger, Thomas K.</author><author>Bilgili, Ahmet</author><author>Buncic, Nenad</author><author>Chalimourda, Athanassia</author><author>Chindemi, Giuseppe</author><author>Courcol, Jean-Denis</author><author>Delalondre, Fabien</author><author>Delattre, Vincent</author><author>Druckmann, Shaul</author><author>Dumusc, Raphael</author><author>Dynes, James</author><author>Eilemann, Stefan</author><author>Gal, Eyal</author><author>Gevaert, Michael Emiel</author><author>Ghobril, Jean-Pierre</author><author>Gidon, Albert</author><author>Graham, Joe W.</author><author>Gupta, Anirudh</author><author>Haenel, Valentin</author><author>Hay, Etay</author><author>Heinis, Thomas</author><author>Hernando, Juan B.</author><author>Hines, Michael</author><author>Kanari, Lida</author><author>Keller, Daniel</author><author>Kenyon, John</author><author>Khazen, Georges</author><author>Kim, Yihwa</author><author>King, James G.</author><author>Kisvarday, Zoltan</author><author>Kumbhar, Pramod</author><author>Lasserre, Sébastien</author><author>Le Bé, Jean-Vincent</author><author>Magalhães, Bruno R. C.</author><author>Merchán-Pérez, Angel</author><author>Meystre, Julie</author><author>Morrice, Benjamin Roy</author><author>Muller, Jeffrey</author><author>Muñoz-Céspedes, Alberto</author><author>Muralidhar, Shruti</author><author>Muthurasa, Keerthan</author><author>Nachbaur, Daniel</author><author>Newton, Taylor H.</author><author>Nolte, Max</author><author>Ovcharenko, Aleksandr</author><author>Palacios, Juan</author><author>Pastor, Luis</author><author>Perin, Rodrigo</author><author>Ranjan, Rajnish</author><author>Riachi, Imad</author><author>Rodríguez, José-Rodrigo</author><author>Riquelme, Juan Luis</author><author>Rössert, Christian</author><author>Sfyrakis, Konstantinos</author><author>Shi, Ying</author><author>Shillcock, Julian C.</author><author>Silberberg, Gilad</author><author>Silva, Ricardo</author><author>Tauheed, Farhan</author><author>Telefont, Martin</author><author>Toledo-Rodriguez, Maria</author><author>Tränkler, Thomas</author><author>Van Geit, Werner</author><author>Díaz, Jafet Villafranca</author><author>Walker, Richard</author><author>Wang, Yun</author><author>Zaninetta, Stefano M.</author><author>DeFelipe, Javier</author><author>Hill, Sean L.</author><author>Segev, Idan</author><author>Schürmann, Felix</author></authors></contributors><titles><title>Reconstruction and Simulation of Neocortical Microcircuitry</title><secondary-title>Cell</secondary-title></titles><periodical><full-title>Cell</full-title><abbr-1>Cell</abbr-1></periodical><pages>456-492</pages><volume>163</volume><number>2</number><issue>2</issue><dates><year>2015</year><pub-dates><date>October 8, 2015</date></pub-dates></dates><isbn>0092-8674</isbn><electronic-resource-num>10.1016/j.cell.2015.09.029</electronic-resource-num><abstract>We present a first-draft digital reconstruction of the microcircuitry of somatosensory cortex of juvenile rat. The reconstruction uses cellular and synaptic organizing principles to algorithmically reconstruct detailed anatomy and physiology from sparse experimental data. An objective anatomical method defines a neocortical volume of 0.29 ± 0.01 mm3 containing ∼31,000 neurons, and patch-clamp studies identify 55 layer-specific morphological and 207 morpho-electrical neuron subtypes. When digitally reconstructed neurons are positioned in the volume and synapse formation is restricted to biological bouton densities and numbers of synapses per connection, their overlapping arbors form ∼8 million connections with ∼37 million synapses. Simulations reproduce an array of in vitro and in vivo experiments without parameter tuning. Additionally, we find a spectrum of network states with a sharp transition from synchronous to asynchronous activity, modulated by physiological mechanisms. The spectrum of network states, dynamically reconfigured around this transition, supports diverse information processing strategies.&#xD;PaperClip&#xD;Video Abstract</abstract><remote-database-name>ScienceDirect</remote-database-name><language>en</language><urls><web-urls><url>http://www.sciencedirect.com/science/article/pii/S0092867415011915</url></web-urls><pdf-urls><url>/home/alec/Zotero/storage/XGK8BF5Z/Markram et al. - 2015 - Reconstruction and Simulation of Neocortical Micro.pdf</url></pdf-urls></urls><access-date>2019-11-30 14:44:16</access-date></record><record><database name="MyLibrary">MyLibrary</database><source-app name="Zotero">Zotero</source-app><ref-type name="Web Page">12</ref-type><contributors><authors><author>Alonso, Leandro M.</author><author>Marder, Eve</author></authors></contributors><titles><title>Visualization of currents in neural models with similar behavior and different conductance densities</title><secondary-title>eLife</secondary-title></titles><periodical><full-title>eLife</full-title></periodical><dates><year>2019</year><pub-dates><date>2019/01/31</date></pub-dates></dates><language>English</language><urls><web-urls><url>https://link.galegroup.com/apps/doc/A576273302/AONE?sid=lms</url></web-urls><pdf-urls><url>/home/alec/Zotero/storage/WXAPXJEW/Alonso and Marder - 2019 - Visualization of currents in neural models with si.pdf</url><url>/home/alec/Zotero/storage/RL4IVLFG/Alonso and Marder - 2019 - Visualization of currents in neural models with si.pdf</url></pdf-urls><text-urls><url>/home/alec/Zotero/storage/IRENSCIN/i.html</url><url>/home/alec/Zotero/storage/P26P2J96/i.html</url></text-urls></urls><access-date>2019-12-04 21:47:06</access-date></record><record><database name="MyLibrary">MyLibrary</database><source-app name="Zotero">Zotero</source-app><ref-type name="Journal Article">17</ref-type><contributors><authors><author>Pennington, Jeffrey</author><author>Schoenholz, Samuel S.</author><author>Ganguli, Surya</author></authors></contributors><titles><title>Resurrecting the sigmoid in deep learning through dynamical isometry: theory and practice</title><secondary-title>arXiv:1711.04735 [cs, stat]</secondary-title><short-title>Resurrecting the sigmoid in deep learning through dynamical isometry</short-title></titles><periodical><full-title>arXiv:1711.04735 [cs, stat]</full-title></periodical><keywords><keyword>Computer Science - Machine Learning</keyword><keyword>Statistics - Machine Learning</keyword></keywords><dates><year>2017</year><pub-dates><date>2017-11-13</date></pub-dates></dates><abstract>It is well known that the initialization of weights in deep neural networks can have a dramatic impact on learning speed. For example, ensuring the mean squared singular value of a network's input-output Jacobian is $O(1)$ is essential for avoiding the exponential vanishing or explosion of gradients. The stronger condition that all singular values of the Jacobian concentrate near $1$ is a property known as dynamical isometry. For deep linear networks, dynamical isometry can be achieved through orthogonal weight initialization and has been shown to dramatically speed up learning; however, it has remained unclear how to extend these results to the nonlinear setting. We address this question by employing powerful tools from free probability theory to compute analytically the entire singular value distribution of a deep network's input-output Jacobian. We explore the dependence of the singular value distribution on the depth of the network, the weight initialization, and the choice of nonlinearity. Intriguingly, we find that ReLU networks are incapable of dynamical isometry. On the other hand, sigmoidal networks can achieve isometry, but only with orthogonal weight initialization. Moreover, we demonstrate empirically that deep nonlinear networks achieving dynamical isometry learn orders of magnitude faster than networks that do not. Indeed, we show that properly-initialized deep sigmoidal networks consistently outperform deep ReLU networks. Overall, our analysis reveals that controlling the entire distribution of Jacobian singular values is an important design consideration in deep learning.</abstract><remote-database-name>arXiv.org</remote-database-name><urls><web-urls><url>http://arxiv.org/abs/1711.04735</url></web-urls><pdf-urls><url>/home/alec/Zotero/storage/2GR6ZUCU/Pennington et al. - 2017 - Resurrecting the sigmoid in deep learning through .pdf</url></pdf-urls><text-urls><url>/home/alec/Zotero/storage/Y97M4HGR/1711.html</url></text-urls></urls><access-date>2019-12-05 15:07:54</access-date></record><record><database name="MyLibrary">MyLibrary</database><source-app name="Zotero">Zotero</source-app><ref-type name="Journal Article">17</ref-type><contributors><authors><author>Duvenaud, David</author><author>Rippel, Oren</author><author>Adams, Ryan P.</author><author>Ghahramani, Zoubin</author></authors></contributors><titles><title>Avoiding pathologies in very deep networks</title><secondary-title>arXiv:1402.5836 [cs, stat]</secondary-title></titles><periodical><full-title>arXiv:1402.5836 [cs, stat]</full-title></periodical><keywords><keyword>Computer Science - Machine Learning</keyword><keyword>Statistics - Machine Learning</keyword></keywords><dates><year>2016</year><pub-dates><date>2016-07-08</date></pub-dates></dates><abstract>Choosing appropriate architectures and regularization strategies for deep networks is crucial to good predictive performance. To shed light on this problem, we analyze the analogous problem of constructing useful priors on compositions of functions. Specifically, we study the deep Gaussian process, a type of infinitely-wide, deep neural network. We show that in standard architectures, the representational capacity of the network tends to capture fewer degrees of freedom as the number of layers increases, retaining only a single degree of freedom in the limit. We propose an alternate network architecture which does not suffer from this pathology. We also examine deep covariance functions, obtained by composing infinitely many feature transforms. Lastly, we characterize the class of models obtained by performing dropout on Gaussian processes.</abstract><remote-database-name>arXiv.org</remote-database-name><urls><web-urls><url>http://arxiv.org/abs/1402.5836</url></web-urls><pdf-urls><url>/home/alec/Zotero/storage/JXPH7EEM/Duvenaud et al. - 2016 - Avoiding pathologies in very deep networks.pdf</url></pdf-urls><text-urls><url>/home/alec/Zotero/storage/RWRPEQED/1402.html</url></text-urls></urls><access-date>2019-12-05 15:07:23</access-date></record><record><database name="MyLibrary">MyLibrary</database><source-app name="Zotero">Zotero</source-app><ref-type name="Journal Article">17</ref-type><contributors><authors><author>Edelman, Gerald M.</author><author>Gally, Joseph A.</author></authors></contributors><titles><title>Degeneracy and complexity in biological systems</title><secondary-title>Proceedings of the National Academy of Sciences</secondary-title></titles><periodical><full-title>Proceedings of the National Academy of Sciences</full-title><abbr-1>PNAS</abbr-1></periodical><pages>13763-13768</pages><volume>98</volume><number>24</number><issue>24</issue><dates><year>2001</year><pub-dates><date>2001/11/20</date></pub-dates></dates><isbn>0027-8424, 1091-6490</isbn><electronic-resource-num>10.1073/pnas.231499798</electronic-resource-num><abstract>Degeneracy, the ability of elements that are structurally different to perform the same function or yield the same output, is a well known characteristic of the genetic code and immune systems. Here, we point out that degeneracy is a ubiquitous biological property and argue that it is a feature of complexity at genetic, cellular, system, and population levels. Furthermore, it is both necessary for, and an inevitable outcome of, natural selection.</abstract><remote-database-name>www.pnas.org</remote-database-name><language>en</language><urls><web-urls><url>https://www.pnas.org/content/98/24/13763</url></web-urls><pdf-urls><url>/home/alec/Zotero/storage/WPI6WEJZ/Edelman and Gally - 2001 - Degeneracy and complexity in biological systems.pdf</url><url>/home/alec/Zotero/storage/TGTN9DDD/Edelman and Gally - 2001 - Degeneracy and complexity in biological systems.pdf</url></pdf-urls><text-urls><url>http://www.ncbi.nlm.nih.gov/pubmed/11698650</url><url>http://www.ncbi.nlm.nih.gov/pubmed/11698650</url><url>/home/alec/Zotero/storage/GS8AI2BZ/13763.html</url><url>/home/alec/Zotero/storage/XUYEMETI/13763.html</url></text-urls></urls><access-date>2019-12-05 15:02:57</access-date></record><record><database name="MyLibrary">MyLibrary</database><source-app name="Zotero">Zotero</source-app><ref-type name="Journal Article">17</ref-type><contributors><authors><author>Marder, Eve</author></authors></contributors><titles><title>Variability, compensation, and modulation in neurons and circuits</title><secondary-title>Proceedings of the National Academy of Sciences of the United States of America</secondary-title></titles><periodical><full-title>Proceedings of the National Academy of Sciences of the United States of America</full-title><abbr-1>Proc Natl Acad Sci U S A</abbr-1></periodical><pages>15542-15548</pages><volume>108</volume><number>Suppl 3</number><issue>Suppl 3</issue><dates><year>2011</year><pub-dates><date>2011-9-13</date></pub-dates></dates><isbn>0027-8424</isbn><electronic-resource-num>10.1073/pnas.1010674108</electronic-resource-num><abstract>I summarize recent computational and experimental work that addresses the inherent variability in the synaptic and intrinsic conductances in normal healthy brains and shows that multiple solutions (sets of parameters) can produce similar circuit performance. I then discuss a number of issues raised by this observation, such as which parameter variations arise from compensatory mechanisms and which reflect insensitivity to those particular parameters. I ask whether networks with different sets of underlying parameters can nonetheless respond reliably to neuromodulation and other global perturbations. At the computational level, I describe a paradigm shift in which it is becoming increasingly common to develop families of models that reflect the variance in the biological data that the models are intended to illuminate rather than single, highly tuned models. On the experimental side, I discuss the inherent limitations of overreliance on mean data and suggest that it is important to look for compensations and correlations among as many system parameters as possible, and between each system parameter and circuit performance. This second paradigm shift will require moving away from measurements of each system component in isolation but should reveal important previously undescribed principles in the organization of complex systems such as brains.</abstract><remote-database-name>PubMed Central</remote-database-name><urls><web-urls><url>https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3176600/</url></web-urls><pdf-urls><url>/home/alec/Zotero/storage/UJVUKIBL/Marder - 2011 - Variability, compensation, and modulation in neuro.pdf</url></pdf-urls><text-urls><url>https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3176600/</url></text-urls></urls><access-date>2019-12-05 16:24:17</access-date></record><record><database name="MyLibrary">MyLibrary</database><source-app name="Zotero">Zotero</source-app><ref-type name="Journal Article">17</ref-type><contributors><authors><author>Achard, Pablo</author><author>De Schutter, Erik</author></authors></contributors><titles><title>Complex Parameter Landscape for a Complex Neuron Model</title><secondary-title>PLoS Computational Biology</secondary-title></titles><periodical><full-title>PLoS Computational Biology</full-title><abbr-1>PLoS Comput Biol</abbr-1></periodical><volume>2</volume><number>7</number><issue>7</issue><dates><year>2006</year><pub-dates><date>2006-7</date></pub-dates></dates><isbn>1553-734X</isbn><electronic-resource-num>10.1371/journal.pcbi.0020094</electronic-resource-num><abstract>The electrical activity of a neuron is strongly dependent on the ionic channels present in its membrane. Modifying the maximal conductances from these channels can have a dramatic impact on neuron behavior. But the effect of such modifications can also be cancelled out by compensatory mechanisms among different channels. We used an evolution strategy with a fitness function based on phase-plane analysis to obtain 20 very different computational models of the cerebellar Purkinje cell. All these models produced very similar outputs to current injections, including tiny details of the complex firing pattern. These models were not completely isolated in the parameter space, but neither did they belong to a large continuum of good models that would exist if weak compensations between channels were sufficient. The parameter landscape of good models can best be described as a set of loosely connected hyperplanes. Our method is efficient in finding good models in this complex landscape. Unraveling the landscape is an important step towards the understanding of functional homeostasis of neurons., Neurons are believed to be electrical information processors. But how many models of a neuron can have similar input/output behavior? How precisely must the model parameters be tuned? These questions are crucial for models of the cerebellar Purkinje cell, a neuron with a huge dendritic arborization and a complex range of electrical outputs, for which recent experiments have demonstrated that dissimilar sets of ionic channel densities can produce similar activities. The authors have therefore used a detailed model of a Purkinje cell, released its 24 channel density parameters, and let them be optimized through an evolution strategy algorithm. They obtained 20 sets of parameters (20 models) that reproduce very precisely the original electrical waveforms. Therefore, model parameters are not uniquely identifiable. The parameters obtained vary several fold whereas small variations of these can also lead to drastically different results. Therefore, the authors have examined in more details the parameter space to gain better understanding of compensatory mechanisms in such complex models. They demonstrate that the 20 models are neither completely isolated nor fully connected, but rather, they belong to thin hyperplanes of good solutions that grid searches or random searches are likely to miss.</abstract><remote-database-name>PubMed Central</remote-database-name><urls><web-urls><url>https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1513272/</url></web-urls><pdf-urls><url>/home/alec/Zotero/storage/KN433XGX/Achard and De Schutter - 2006 - Complex Parameter Landscape for a Complex Neuron M.pdf</url></pdf-urls><text-urls><url>https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1513272/</url></text-urls></urls><access-date>2019-12-05 16:24:20</access-date></record><record><database name="MyLibrary">MyLibrary</database><source-app name="Zotero">Zotero</source-app><ref-type name="Journal Article">17</ref-type><contributors><authors><author>Banga, Julio R.</author></authors></contributors><titles><title>Optimization in computational systems biology</title><secondary-title>BMC Systems Biology</secondary-title></titles><periodical><full-title>BMC Systems Biology</full-title><abbr-1>BMC Systems Biology</abbr-1></periodical><pages>47</pages><volume>2</volume><number>1</number><issue>1</issue><dates><year>2008</year><pub-dates><date>May 28, 2008</date></pub-dates></dates><isbn>1752-0509</isbn><electronic-resource-num>10.1186/1752-0509-2-47</electronic-resource-num><abstract>Optimization aims to make a system or design as effective or functional as possible. Mathematical optimization methods are widely used in engineering, economics and science. This commentary is focused on applications of mathematical optimization in computational systems biology. Examples are given where optimization methods are used for topics ranging from model building and optimal experimental design to metabolic engineering and synthetic biology. Finally, several perspectives for future research are outlined.</abstract><remote-database-name>BioMed Central</remote-database-name><urls><web-urls><url>https://doi.org/10.1186/1752-0509-2-47</url></web-urls><pdf-urls><url>/home/alec/Zotero/storage/2RXI5UCX/Banga - 2008 - Optimization in computational systems biology.pdf</url></pdf-urls><text-urls><url>/home/alec/Zotero/storage/NLX5AEUQ/1752-0509-2-47.html</url></text-urls></urls><access-date>2019-12-05 16:24:24</access-date></record><record><database name="MyLibrary">MyLibrary</database><source-app name="Zotero">Zotero</source-app><ref-type name="Journal Article">17</ref-type><contributors><authors><author>Cranmer, Kyle</author><author>Brehmer, Johann</author><author>Louppe, Gilles</author></authors></contributors><titles><title>The frontier of simulation-based inference</title><secondary-title>arXiv:1911.01429 [cs, stat]</secondary-title></titles><periodical><full-title>arXiv:1911.01429 [cs, stat]</full-title></periodical><keywords><keyword>Computer Science - Machine Learning</keyword><keyword>Statistics - Machine Learning</keyword><keyword>Statistics - Methodology</keyword></keywords><dates><year>2019</year><pub-dates><date>2019-11-14</date></pub-dates></dates><abstract>Many domains of science have developed complex simulations to describe phenomena of interest. While these simulations provide high-fidelity models, they are poorly suited for inference and lead to challenging inverse problems. We review the rapidly developing field of simulation-based inference and identify the forces giving new momentum to the field. Finally, we describe how the frontier is expanding so that a broad audience can appreciate the profound change these developments may have on science.</abstract><remote-database-name>arXiv.org</remote-database-name><urls><web-urls><url>http://arxiv.org/abs/1911.01429</url></web-urls><pdf-urls><url>/home/alec/Zotero/storage/BURIC92H/Cranmer et al. - 2019 - The frontier of simulation-based inference.pdf</url></pdf-urls><text-urls><url>/home/alec/Zotero/storage/QCXDHAFJ/1911.html</url></text-urls></urls><access-date>2019-12-05 16:36:57</access-date></record><record><database name="MyLibrary">MyLibrary</database><source-app name="Zotero">Zotero</source-app><ref-type name="Journal Article">17</ref-type><contributors><authors><author>Gonçalves, Pedro J.</author><author>Lueckmann, Jan-Matthis</author><author>Deistler, Michael</author><author>Nonnenmacher, Marcel</author><author>Öcal, Kaan</author><author>Bassetto, Giacomo</author><author>Chintaluri, Chaitanya</author><author>Podlaski, William F.</author><author>Haddad, Sara A.</author><author>Vogels, Tim P.</author><author>Greenberg, David S.</author><author>Macke, Jakob H.</author></authors></contributors><titles><title>Training deep neural density estimators to identify mechanistic models of neural dynamics</title><secondary-title>bioRxiv</secondary-title></titles><periodical><full-title>bioRxiv</full-title></periodical><pages>838383</pages><dates><year>2019</year><pub-dates><date>2019-11-12</date></pub-dates></dates><electronic-resource-num>10.1101/838383</electronic-resource-num><abstract>Mechanistic modeling in neuroscience aims to explain neural or behavioral phenomena in terms of underlying causes. A central challenge in building mechanistic models is to identify which models and parameters can achieve an agreement between the model and experimental data. The complexity of models and data characterizing neural systems makes it infeasible to solve model equations analytically or tune parameters manually. To overcome this limitation, we present a machine learning tool that uses density estimators based on deep neural networks---trained using model simulations---to infer data-compatible parameters for a wide range of mechanistic models. Our tool identifies all parameters consistent with data, is scalable both in the number of parameters and data features, and does not require writing new code when the underlying model is changed. It can be used to analyze new data rapidly after training, and can be applied to either raw data or selected data features. We demonstrate our approach for parameter inference on ion channels, receptive fields, and Hodgkin--Huxley models. Finally, we use it to explore the space of parameters which give rise to the same rhythmic activity in a network model of the crustacean stomatogastric ganglion and to search for potential compensation mechanisms. The approach presented here will help close the gap between data-driven and theory-driven models of neural dynamics.</abstract><remote-database-name>www.biorxiv.org</remote-database-name><language>en</language><urls><web-urls><url>https://www.biorxiv.org/content/10.1101/838383v1</url></web-urls><pdf-urls><url>/home/alec/Zotero/storage/73XLLM2J/Gonçalves et al. - 2019 - Training deep neural density estimators to identif.pdf</url></pdf-urls><text-urls><url>/home/alec/Zotero/storage/L8LFGYJY/838383v1.html</url></text-urls></urls><access-date>2019-12-05 16:37:00</access-date></record><record><database name="MyLibrary">MyLibrary</database><source-app name="Zotero">Zotero</source-app><ref-type name="Journal Article">17</ref-type><contributors><authors><author>Coventry, Brandon S.</author><author>Parthasarathy, Aravindakshan</author><author>Sommer, Alexandra L.</author><author>Bartlett, Edward L.</author></authors></contributors><titles><title>Hierarchical winner-take-all particle swarm optimization social network for neural model fitting</title><secondary-title>Journal of Computational Neuroscience</secondary-title></titles><periodical><full-title>Journal of Computational Neuroscience</full-title><abbr-1>J Comput Neurosci</abbr-1></periodical><pages>71-85</pages><volume>42</volume><number>1</number><issue>1</issue><keywords><keyword>Particle swarm optimization</keyword><keyword>Biological neural networks</keyword><keyword>Evolutionary computation</keyword><keyword>Model optimization</keyword></keywords><dates><year>2017</year><pub-dates><date>2017-02-01</date></pub-dates></dates><isbn>1573-6873</isbn><electronic-resource-num>10.1007/s10827-016-0628-2</electronic-resource-num><abstract>Particle swarm optimization (PSO) has gained widespread use as a general mathematical programming paradigm and seen use in a wide variety of optimization and machine learning problems. In this work, we introduce a new variant on the PSO social network and apply this method to the inverse problem of input parameter selection from recorded auditory neuron tuning curves. The topology of a PSO social network is a major contributor to optimization success. Here we propose a new social network which draws influence from winner-take-all coding found in visual cortical neurons. We show that the winner-take-all network performs exceptionally well on optimization problems with greater than 5 dimensions and runs at a lower iteration count as compared to other PSO topologies. Finally we show that this variant of PSO is able to recreate auditory frequency tuning curves and modulation transfer functions, making it a potentially useful tool for computational neuroscience models.</abstract><remote-database-name>Springer Link</remote-database-name><language>en</language><urls><web-urls><url>https://doi.org/10.1007/s10827-016-0628-2</url></web-urls><pdf-urls><url>/home/alec/Zotero/storage/EKEZCZGH/Coventry et al. - 2017 - Hierarchical winner-take-all particle swarm optimi.pdf</url></pdf-urls></urls><access-date>2019-12-05 16:40:01</access-date></record><record><database name="MyLibrary">MyLibrary</database><source-app name="Zotero">Zotero</source-app><ref-type name="Book Section">5</ref-type><contributors><authors><author>Lueckmann, Jan-Matthis</author><author>Goncalves, Pedro J</author><author>Bassetto, Giacomo</author><author>Öcal, Kaan</author><author>Nonnenmacher, Marcel</author><author>Macke, Jakob H</author></authors><secondary-authors><author>Guyon, I.</author><author>Luxburg, U. V.</author><author>Bengio, S.</author><author>Wallach, H.</author><author>Fergus, R.</author><author>Vishwanathan, S.</author><author>Garnett, R.</author></secondary-authors></contributors><titles><title>Flexible statistical inference for mechanistic models of neural dynamics</title><secondary-title>Advances in Neural Information Processing Systems 30</secondary-title></titles><periodical><full-title>Advances in Neural Information Processing Systems 30</full-title></periodical><pages>1289–1299</pages><dates><year>2017</year><pub-dates><date>2017</date></pub-dates></dates><publisher>Curran Associates, Inc.</publisher><remote-database-name>Neural Information Processing Systems</remote-database-name><urls><web-urls><url>http://papers.nips.cc/paper/6728-flexible-statistical-inference-for-mechanistic-models-of-neural-dynamics.pdf</url></web-urls><pdf-urls><url>/home/alec/Zotero/storage/T9ULAUJK/Lueckmann et al. - 2017 - Flexible statistical inference for mechanistic mod.pdf</url></pdf-urls><text-urls><url>/home/alec/Zotero/storage/N5H62EG2/6728-flexible-statistical-inference-for-mechanistic-models-of-neural-dynamics.html</url></text-urls></urls><access-date>2019-12-05 16:55:28</access-date></record><record><database name="MyLibrary">MyLibrary</database><source-app name="Zotero">Zotero</source-app><ref-type name="Journal Article">17</ref-type><contributors><authors><author>Schulz, David J.</author><author>Goaillard, Jean-Marc</author><author>Marder, Eve</author></authors></contributors><titles><title>Variable channel expression in identified single and electrically coupled neurons in different animals</title><secondary-title>Nature Neuroscience</secondary-title></titles><periodical><full-title>Nature Neuroscience</full-title><abbr-1>Nat Neurosci</abbr-1></periodical><pages>356-362</pages><volume>9</volume><number>3</number><issue>3</issue><dates><year>2006</year><pub-dates><date>2006/03</date></pub-dates></dates><isbn>1546-1726</isbn><electronic-resource-num>10.1038/nn1639</electronic-resource-num><abstract>It is often assumed that all neurons of the same cell type have identical intrinsic properties, both within an animal and between animals. We exploited the large size and small number of unambiguously identifiable neurons in the crab stomatogastric ganglion to test this assumption at the level of channel mRNA expression and membrane currents (measured in voltage-clamp experiments). In lateral pyloric (LP) neurons, we saw strong correlations between measured current and the abundance of Shal and BK-KCa mRNAs (encoding the Shal-family voltage-gated potassium channel and large-conductance calcium-activated potassium channel, respectively). We also saw two- to fourfold interanimal variability for three potassium currents and their mRNA expression. Measurements of channel expression in the two electrically coupled pyloric dilator (PD) neurons showed significant interanimal variability, but copy numbers for IH (encoding the hyperpolarization-activated, inward-current channel) and Shal mRNA in the two PD neurons from the same crab were similar, suggesting that the regulation of some currents may be shared in electrically coupled neurons. Note: The PDF version of this article was corrected on 05 February 2006. Please see the PDF for details.</abstract><remote-database-name>www.nature.com</remote-database-name><language>en</language><urls><web-urls><url>https://www.nature.com/articles/nn1639</url></web-urls><pdf-urls><url>/home/alec/Zotero/storage/JZ7NXDXZ/Schulz et al. - 2006 - Variable channel expression in identified single a.pdf</url></pdf-urls><text-urls><url>/home/alec/Zotero/storage/7V66BRKJ/nn1639.html</url></text-urls></urls><access-date>2019-12-05 18:51:06</access-date></record><record><database name="MyLibrary">MyLibrary</database><source-app name="Zotero">Zotero</source-app><ref-type name="Journal Article">17</ref-type><contributors><authors><author>Saxe, Andrew M.</author><author>McClelland, James L.</author><author>Ganguli, Surya</author></authors></contributors><titles><title>Exact solutions to the nonlinear dynamics of learning in deep linear neural networks</title><secondary-title>arXiv:1312.6120 [cond-mat, q-bio, stat]</secondary-title></titles><periodical><full-title>arXiv:1312.6120 [cond-mat, q-bio, stat]</full-title></periodical><keywords><keyword>Computer Science - Machine Learning</keyword><keyword>Statistics - Machine Learning</keyword><keyword>Computer Science - Computer Vision and Pattern Recognition</keyword><keyword>Computer Science - Neural and Evolutionary Computing</keyword><keyword>Condensed Matter - Disordered Systems and Neural Networks</keyword><keyword>Quantitative Biology - Neurons and Cognition</keyword></keywords><dates><year>2014</year><pub-dates><date>2014-02-19</date></pub-dates></dates><abstract>Despite the widespread practical success of deep learning methods, our theoretical understanding of the dynamics of learning in deep neural networks remains quite sparse. We attempt to bridge the gap between the theory and practice of deep learning by systematically analyzing learning dynamics for the restricted case of deep linear neural networks. Despite the linearity of their input-output map, such networks have nonlinear gradient descent dynamics on weights that change with the addition of each new hidden layer. We show that deep linear networks exhibit nonlinear learning phenomena similar to those seen in simulations of nonlinear networks, including long plateaus followed by rapid transitions to lower error solutions, and faster convergence from greedy unsupervised pretraining initial conditions than from random initial conditions. We provide an analytical description of these phenomena by finding new exact solutions to the nonlinear dynamics of deep learning. Our theoretical analysis also reveals the surprising finding that as the depth of a network approaches infinity, learning speed can nevertheless remain finite: for a special class of initial conditions on the weights, very deep networks incur only a finite, depth independent, delay in learning speed relative to shallow networks. We show that, under certain conditions on the training data, unsupervised pretraining can find this special class of initial conditions, while scaled random Gaussian initializations cannot. We further exhibit a new class of random orthogonal initial conditions on weights that, like unsupervised pre-training, enjoys depth independent learning times. We further show that these initial conditions also lead to faithful propagation of gradients even in deep nonlinear networks, as long as they operate in a special regime known as the edge of chaos.</abstract><remote-database-name>arXiv.org</remote-database-name><urls><web-urls><url>http://arxiv.org/abs/1312.6120</url></web-urls><pdf-urls><url>/home/alec/Zotero/storage/5WA97VIS/Saxe et al. - 2014 - Exact solutions to the nonlinear dynamics of learn.pdf</url></pdf-urls><text-urls><url>/home/alec/Zotero/storage/5QPXJYB5/1312.html</url></text-urls></urls><access-date>2019-12-05 19:10:56</access-date></record><record><database name="MyLibrary">MyLibrary</database><source-app name="Zotero">Zotero</source-app><ref-type name="Journal Article">17</ref-type><contributors><authors><author>Orhan, A. Emin</author><author>Pitkow, Xaq</author></authors></contributors><titles><title>Skip Connections Eliminate Singularities</title><secondary-title>arXiv:1701.09175 [cs]</secondary-title></titles><periodical><full-title>arXiv:1701.09175 [cs]</full-title></periodical><keywords><keyword>Computer Science - Machine Learning</keyword><keyword>Computer Science - Neural and Evolutionary Computing</keyword></keywords><dates><year>2018</year><pub-dates><date>2018-03-04</date></pub-dates></dates><abstract>Skip connections made the training of very deep networks possible and have become an indispensable component in a variety of neural architectures. A completely satisfactory explanation for their success remains elusive. Here, we present a novel explanation for the benefits of skip connections in training very deep networks. The difficulty of training deep networks is partly due to the singularities caused by the non-identifiability of the model. Several such singularities have been identified in previous works: (i) overlap singularities caused by the permutation symmetry of nodes in a given layer, (ii) elimination singularities corresponding to the elimination, i.e. consistent deactivation, of nodes, (iii) singularities generated by the linear dependence of the nodes. These singularities cause degenerate manifolds in the loss landscape that slow down learning. We argue that skip connections eliminate these singularities by breaking the permutation symmetry of nodes, by reducing the possibility of node elimination and by making the nodes less linearly dependent. Moreover, for typical initializations, skip connections move the network away from the "ghosts" of these singularities and sculpt the landscape around them to alleviate the learning slow-down. These hypotheses are supported by evidence from simplified models, as well as from experiments with deep networks trained on real-world datasets.</abstract><remote-database-name>arXiv.org</remote-database-name><urls><web-urls><url>http://arxiv.org/abs/1701.09175</url></web-urls><pdf-urls><url>/home/alec/Zotero/storage/Z87RGAUY/Orhan and Pitkow - 2018 - Skip Connections Eliminate Singularities.pdf</url></pdf-urls><text-urls><url>/home/alec/Zotero/storage/BVUP2X5E/1701.html</url></text-urls></urls><access-date>2019-12-05 19:31:31</access-date></record><record><database name="MyLibrary">MyLibrary</database><source-app name="Zotero">Zotero</source-app><ref-type name="Journal Article">17</ref-type><contributors><authors><author>Rathour, Rahul K.</author><author>Narayanan, Rishikesh</author></authors></contributors><titles><title>Degeneracy in hippocampal physiology and plasticity</title><secondary-title>Hippocampus</secondary-title></titles><periodical><full-title>Hippocampus</full-title></periodical><pages>980-1022</pages><volume>29</volume><number>10</number><issue>10</issue><keywords><keyword>compensation</keyword><keyword>learning</keyword><keyword>causality</keyword><keyword>Degeneracy</keyword><keyword>encoding</keyword><keyword>holism</keyword><keyword>homeostasis</keyword><keyword>memory</keyword><keyword>reductionism</keyword><keyword>structure-function relationships</keyword><keyword>variability</keyword></keywords><dates><year>2019</year><pub-dates><date>2019</date></pub-dates></dates><isbn>1098-1063</isbn><electronic-resource-num>10.1002/hipo.23139</electronic-resource-num><abstract>Degeneracy, defined as the ability of structurally disparate elements to perform analogous function, has largely been assessed from the perspective of maintaining robustness of physiology or plasticity. How does the framework of degeneracy assimilate into an encoding system where the ability to change is an essential ingredient for storing new incoming information? Could degeneracy maintain the balance between the apparently contradictory goals of the need to change for encoding and the need to resist change towards maintaining homeostasis? In this review, we explore these fundamental questions with the mammalian hippocampus as an example encoding system. We systematically catalog lines of evidence, spanning multiple scales of analysis that point to the expression of degeneracy in hippocampal physiology and plasticity. We assess the potential of degeneracy as a framework to achieve the conjoint goals of encoding and homeostasis without cross-interferences. We postulate that biological complexity, involving interactions among the numerous parameters spanning different scales of analysis, could establish disparate routes towards accomplishing these conjoint goals. These disparate routes then provide several degrees of freedom to the encoding-homeostasis system in accomplishing its tasks in an input- and state-dependent manner. Finally, the expression of degeneracy spanning multiple scales offers an ideal reconciliation to several outstanding controversies, through the recognition that the seemingly contradictory disparate observations are merely alternate routes that the system might recruit towards accomplishment of its goals.</abstract><remote-database-name>Wiley Online Library</remote-database-name><language>en</language><urls><web-urls><url>https://onlinelibrary.wiley.com/doi/abs/10.1002/hipo.23139</url></web-urls><pdf-urls><url>/home/alec/Zotero/storage/EBKT8Q9T/Rathour and Narayanan - 2019 - Degeneracy in hippocampal physiology and plasticit.pdf</url></pdf-urls><text-urls><url>/home/alec/Zotero/storage/V2XG4V78/hipo.html</url></text-urls></urls><access-date>2019-12-05 20:19:01</access-date></record><record><database name="MyLibrary">MyLibrary</database><source-app name="Zotero">Zotero</source-app><ref-type name="Conference Proceedings">10</ref-type><contributors><authors><author>Bai, Runbo</author><author>Liu, Fusheng</author><author>Qiu, Xiumei</author></authors></contributors><titles><title>Using the Perturbed System to Analyze the Sensitivity of Influential Factors with Neural Networks</title><secondary-title>2009 International Conference on Information Engineering and Computer Science</secondary-title></titles><periodical><full-title>2009 International Conference on Information Engineering and Computer Science</full-title></periodical><pages>1-5</pages><keywords><keyword>Accuracy</keyword><keyword>Artificial neural networks</keyword><keyword>Civil engineering</keyword><keyword>Cost function</keyword><keyword>Educational institutions</keyword><keyword>factor sensitivity analysis</keyword><keyword>input perturbation ratio</keyword><keyword>Input variables</keyword><keyword>neural nets</keyword><keyword>neural network design</keyword><keyword>neural networks</keyword><keyword>Neural networks</keyword><keyword>partial derivatives</keyword><keyword>perturbation method</keyword><keyword>Perturbation methods</keyword><keyword>perturbation techniques</keyword><keyword>perturbed system</keyword><keyword>prediction accuracy</keyword><keyword>Predictive models</keyword><keyword>sensitivity</keyword><keyword>Sensitivity analysis</keyword><keyword>sensitivity definition</keyword></keywords><dates><year>2009</year><pub-dates><date>December 2009</date></pub-dates></dates><electronic-resource-num>10.1109/ICIECS.2009.5362791</electronic-resource-num><abstract>The 'perturbation' method is an effective and widely used method in the factor sensitivity analysis in neural network design, on which two major problems: the sensitivity definition and the input perturbation ratio, are investigated in this study. Four models are considered in the investigation. Through comparison and analysis, results show that the definition derived from the partial derivatives is relatively more rational than others and, the optimum range of the input perturbation ratio could be [-20%, 20%] for a general case. Additionally, the effect of quality of model on the prediction accuracy is discussed, and their correlation is revealed.</abstract><remote-database-name>IEEE Xplore</remote-database-name><urls><pdf-urls><url>/home/alec/Zotero/storage/VM9BTGY7/Bai et al. - 2009 - Using the Perturbed System to Analyze the Sensitiv.pdf</url></pdf-urls><text-urls><url>/home/alec/Zotero/storage/3X4DM78I/5362791.html</url></text-urls></urls><custom3>2009 International Conference on Information Engineering and Computer Science</custom3></record><record><database name="MyLibrary">MyLibrary</database><source-app name="Zotero">Zotero</source-app><ref-type name="Journal Article">17</ref-type><contributors><authors><author>Novak, Roman</author><author>Bahri, Yasaman</author><author>Abolafia, Daniel A.</author><author>Pennington, Jeffrey</author><author>Sohl-Dickstein, Jascha</author></authors></contributors><titles><title>Sensitivity and Generalization in Neural Networks: an Empirical Study</title><secondary-title>arXiv:1802.08760 [cs, stat]</secondary-title><short-title>Sensitivity and Generalization in Neural Networks</short-title></titles><periodical><full-title>arXiv:1802.08760 [cs, stat]</full-title></periodical><keywords><keyword>Computer Science - Artificial Intelligence</keyword><keyword>Computer Science - Machine Learning</keyword><keyword>Computer Science - Neural and Evolutionary Computing</keyword><keyword>Statistics - Machine Learning</keyword></keywords><dates><year>2018</year><pub-dates><date>2018-06-18</date></pub-dates></dates><abstract>In practice it is often found that large over-parameterized neural networks generalize better than their smaller counterparts, an observation that appears to conflict with classical notions of function complexity, which typically favor smaller models. In this work, we investigate this tension between complexity and generalization through an extensive empirical exploration of two natural metrics of complexity related to sensitivity to input perturbations. Our experiments survey thousands of models with various fully-connected architectures, optimizers, and other hyper-parameters, as well as four different image classification datasets. We find that trained neural networks are more robust to input perturbations in the vicinity of the training data manifold, as measured by the norm of the input-output Jacobian of the network, and that it correlates well with generalization. We further establish that factors associated with poor generalization $-$ such as full-batch training or using random labels $-$ correspond to lower robustness, while factors associated with good generalization $-$ such as data augmentation and ReLU non-linearities $-$ give rise to more robust functions. Finally, we demonstrate how the input-output Jacobian norm can be predictive of generalization at the level of individual test points.</abstract><remote-database-name>arXiv.org</remote-database-name><urls><web-urls><url>http://arxiv.org/abs/1802.08760</url></web-urls><pdf-urls><url>/home/alec/Zotero/storage/TTUGFDSL/Novak et al. - 2018 - Sensitivity and Generalization in Neural Networks.pdf</url></pdf-urls><text-urls><url>/home/alec/Zotero/storage/A892AF4V/1802.html</url></text-urls></urls><access-date>2019-12-06 15:19:11</access-date></record></records></xml>